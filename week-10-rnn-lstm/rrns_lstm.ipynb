{"cells":[{"cell_type":"markdown","metadata":{"id":"eDbvjh4I5zKH"},"source":["# Recurrent Neural Networks and LSTM\n","\n","## Introduction\n","\n","Recurrent Neural Networks are one of the most popular Neural Network architecture types due to their importance in processing sequential and time series data. While time-series data usually refers to real valued data with a temporal dimension such as stock market prices and sensor outputs, sequential data is a much broader data type which also includes audio, text, and video. \n","\n","We begin with an overview of basic RNNs with a simple example. After that we look at LSTMs as perhaps the most popular example of a traditional recurrent neural network. Later in the lecture we start looking at the topic of generative machine learning by seeing how recurrent neural networks can be used to create a type of langauge model. \n","\n","Note that some of the notes below are based on the tutorial here:\n","https://r2rt.com/recurrent-neural-networks-in-tensorflow-i.html\n","\n","Transformer Architectures and the idea of attention are also really important in processing sequential data. We won't cover them this week, but we will have a look at these in detail in the next lecture. "]},{"cell_type":"markdown","metadata":{"id":"0Y-3Ugl75zKS"},"source":[" ## Recurrent Neural Networks - Overview \n","\n","A recurrent neural network is a neural network in which the output of the hidden state of input $i$ is fed to the hidden state alongside the $i+1^{th}$ input. That way, the output at time step k is not only dependent on the input at time step k, but also on the inputs up to point k. \n","\n","We can illustrate the most basic RNN instance below with a hidden neuron which is connected back to itself. \n","\n","<!-- rnn.png --> \n","<img width=\"300\" src=\"https://drive.google.com/uc?id=1HqHbFVX-l2XBLskh3VF4IMt8AzUOY5iB\"/>\n","\n","\n","We can 'unfold' such a design in time to get a better sense of what is happening. In the figure below we see the RNN with three inputs applied in sequence. The input t=0 will result in a hidden state which is passed to the output at time t=0 but which is also passed into the hidden state at t=1. Thus at t=1 the output of the hidden neuron is dependent not only on the input, but also on the hidden state variable from time t=0. Similarly at t=2 the output of the hidden neuron is dependent on the input at t=2 but also on the output of the hidden state from t=1 which in turn was dependent on the output of the hidden state at t=0. This shows the power of the model, i.e., the hidden state output at t=2 is not only dependent on the input variable at t=2, but on all the inputs that have been seen to date. \n","\n","<!-- rnn2.png --> \n","<img width=\"300\" src=\"https://drive.google.com/uc?id=1Fa10zfSV0fcc3TAO_1hWyaBydO1XzD1h\"/>\n","\n","The example above has just one input and output neuron, thus we only have one target variable and one feature. We can trivially extend this approach to multiple target variables and input features by connecting each hidden neuron to its corresponding neuron in subsequent instances of the network as it is unfolded over time. This is illustrated below. Note that for visual purposes we mark hidden-to-hidden links with a separate color, but that this has no bearing on the implementation. \n","\n","<!-- rnn5.png --> \n","<img width=\"300\" src=\"https://drive.google.com/uc?id=1FQStfBh1RJ4292Whb6rHRUzH1PNEFtO0\"/>\n","\n","This is a vanilla form of recurrent neural network. In practice the architecture adopted is much more complex than this and allows us a greater degree of control in determining what parts of the state variable is saved or passed on to future time steps. But for the moment let's see how this type of model can be put to work. "]},{"cell_type":"markdown","metadata":{"id":"OePxVkoc5zKT"},"source":["## Recurrent Neural Networks - Model Description \n","\n","For our Vanilla RNN, let us consider the most basic form of RNN possible, i.e., a RNN which has a link to a single neuron in an input layer, a single neuron in the output layer, and a state variable that is one neuron wide. Lets also assume that the input to this network, X, is a stream of values $\\in R$ and that the output from the network, Y, is a stream of values $\\in \\{0,1\\}$. We will refer to the output of the hidden RNN unit as its state, S, and this state is passed to the logit calculations of the output layer and also to the logit calculations of the hidden layer itself. The architecture for this network is illustrated below: \n","\n","<!-- rnn_vanilla.png --> \n","<img width=\"500\" src=\"https://drive.google.com/uc?id=1iPbX79FmcUoVf0WynEkeKZtyRP9GhUUO\"/>\n","\n","Some points to note: \n"," * Our input layer is clamped to input values $x \\in X$ in the normal way. \n"," * Our output layer is in this case a hyperbolic tangent function (softmax is not necessary as we only have one output binary class). \n"," * Our hidden unit is recursive in that its output value is fed back and concatenated with the content from the input layer. \n"," * Weights and biases are assumed for the hidden and output layers in the normal way. We will return to the dimensionality of these variables later. \n"," \n","Operationally the hidden or recursive unit behaves like a typical hidden layer in that a logit is first computed and from this an activation is calculated. We will assume that the activation value is a hyperbolic tangent function, but this need not be the case. The activation function for our vanilla RNN could be a logistic function or other suitable activation function. \n","\n","Our visualization above simplifies the temporal nature of the RNN as it shows the output of the RNN unit being fed directly back in to the same unit. For clarity it is useful to think about the network as having to be unrolled out over time to deal with sequences of inputs. We visualizae this below by extending the network out to a sequence of 3 input units at T=0, T=1, and T=2. \n","\n","<!-- rnn_vanilla_unrolled.png --> \n","<img width=\"500\" src=\"https://drive.google.com/uc?id=1iTG9nopDMktmN1-840xahMEcuGU_GzcT\"/>\n","\n","Referring to the above we can see now how the output of the hidden layer at time T is passed as input to the hidden layer at T+1. Specifically the $S_{T}$ is concatenated with the actual input $X_{T+1}$. At time T=1 we don't have a true hidden state from a previous step to concatenate with our input $X_{0}$. In this case we typically use an initial value for S which we set to 0. \n","\n","As noted above, the output of the hidden RNN unit is commonly referred to as the hidden state, $S$. The value of S for a given time point $t$ is simply: \n","\n","$$S_{t} = tanh( (W * (X \\oplus S_{t-1})) + b) $$\n","\n","where $\\oplus$ denotes vector concatenation. Thus we see that for a given point in time that the calculation of the hidden layer is typical of any feedforward layer that we have dealt with to date. The difference is that this value is passed back to the network at time $t+1$ as well as being passed to the logit calculation for the output layer. \n","\n","### Hidden State Size\n","In our figures above we assumed our hidden layer had just one hyperbolic tangent unit. This corresponds to having a hidden state size of 1. Naturally however having only one channel to carry information across time will limit the complexity of temporal features that our model will be able to learn. Therefore, in practice we will want a hidden state that is much wider.\n","\n","Our equation for calculating $S$ above has no such assumptions about the size of the hidden state, and indeed we will see later our implementations of RNNs are built to accommodate much larger hidden states. The approach taken to this is very simple. Rather than having a single non-linear unit, we make use of a so-called RNN cell which can have an arbitrary state size. Internally the cell is implemented as a collection of neurons, i.e., logit and non-linearity calculations, but the specifics of these internal operations are abstracted for us. \n","\n","We can illustrate this approach by looking again at the case where T=1 and assume instead a hidden state size of 4. In this illustration $S_{0}$ is the output state from the $0^{th}$ application of the model, and $S_{1}$ will be fed to the following application. \n","\n","<!-- rnn_vanilla_states.png --> \n","<img width=\"500\" src=\"https://drive.google.com/uc?id=1iRDZbrkY82WAAdAEOQ9FUaYReluw2CrA\"/>\n","\n","By convention we won't illustrate each of the neurons within the RNN cell and will instead only illustrate the RNN cell itself. For our purposes here we will use a square to capture such RNN cells as opposed to the ovals we have used to date. As above we split the cell in half to illustrate the point that a logit is first calculated before a non-linearity. \n","\n","<!-- rnn_vanilla_unrolled_2.png --> \n","<img width=\"500\" src=\"https://drive.google.com/uc?id=1ijarOspm6Qwtue6VS2o4oBrR1zuUjW2A\"/>\n","\n","\n","### Aside: Unrolling the Vanilla RNN for Training \n","\n","Above we unrolled the Vanilla RNN over a number of time steps in order to make its operation more transparent. It turns out however that unrolling the RNN is in fact essential to its use. \n","\n","Lets consider first the case of using our Vanilla RNN model where the model parameters (i.e., the weights and bias for the hidden and output layers) have already been set. In this case it should be possible for us to feed an input sequence one unit at a time through an unrolled network and produce a set of valid outputs. We have to be careful to copy our output from time point $t$ to time point $t+1$ for concatenation with our actual input, but otherwise there should not be a problem. \n","\n","However for training, things are a little bit more complex. At the end of an input sequence we should be able to calculate a total training loss on the sequence. That loss is not just from the error on the final output symbol, but should instead be based on the true versus predicted value for the output Y at each time step. We need to propagate errors back from the last time step through multiple steps. \n","\n","One question is how many steps should we propagate errors backwards for? There are two extreme conditions. In the first extreme condition we don't propagate errors backwards in time at all. In this case we will not be able to learn temporal dependencies in our data - which defeats the whole purpose of what we are trying to achieve. In the other extreme we allow errors on the final symbol to propagate all the way back, so that in theory corrections to weights at the first time step could be influenced by an error in the final symbol. While this might be appealing in that it would allow arbitrarily long dependencies to be identified, in practice it does not work for two reasons. The first is a purely resource based concern. The further back we allow error propagation, the more complex our model and the more resources (time and memory) that are consumed. The second issue however is more problematic and is an instantiation of the vanishing gradient problem. Essentially over a long time step the repeated multiplication of error gradients will result in the gradient approaching either 0 or $\\infty$. \n","\n","The solution then in practice is to limit the propagation of errors to a fixed number of time steps. This is referred to as truncated backpropagation. \n","\n","### Multiple RNN Layers\n","\n","In the example above we used a single layer of LSTM units. In practice though we can use multiple layers where the output of one layer feeds into the next layer. This can be visualized as follows: \n","\n","<!-- rnn_multi_layer.png --> \n","<img width=\"600\" src=\"https://drive.google.com/uc?id=1iLEQqFBbKfFvXfPMnYFtwI2Ss6E61onL\"/>\n","\n","When we are dealing with the output of an RNN, please keep in mind however that it can be used in two different ways. In one circumstance we may only be interested in the final output of the RNN, i.e., what was the output produced for the final chunk of input. This might be relevant if we want a simple classifier based almost entirely on the RNN's own internal processing. In another circumstance we might want to see the output of the RNN for each step. Here we have an output for each individual time step. This is often useful to further classifier steps as they can make a decision not just based on the final output of the RNN, but also on the intermediary decisions that were made by it. Typical implementations, like for example provided by Tensorflow, allow you to parameterise whether an RNN cell outputs only its final result, or its result unfolded in time. "]},{"cell_type":"markdown","metadata":{"id":"pgtI32b_5zKX"},"source":["## Usage Scenario 1: Alarm Tripping\n","\n","To illustrate the model above, we will create a Trip Alarm for Negative Inputs. This alarm will produce as output the value 0 so long as the input is positive. However if ever the input value turns negative the output of the alarm will switch to output 1 and remain at 1 regardless of the inputs. \n","\n","### Step 1 - Data Creation \n","We can easily generate some sample data that corresponds to our usage scenario. For training we will limit all our examples to being constant length (currently 20 units below in the code). We will also set the input data to be in the range -1 to 20. We also generate the Target data straightforwardly by iterating over our dataset and applying a function that checks for the occurrence of negative values, and if found sets the output to 1 from the point at which the negative value is found, until the end of the sequence. We will create a large training data set and a smaller test data set with the same methods. "]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2882,"status":"ok","timestamp":1680018129355,"user":{"displayName":"Robert Ross","userId":"11121896988139609834"},"user_tz":-60},"id":"UKJ74smi5zKa","outputId":"0e0a64ab-75a5-4b4c-fc4d-b84857677aad"},"outputs":[{"name":"stdout","output_type":"stream","text":["[[14. 12. 13. 13. 19.  4. 16. 10.  1. 13.  9. 14. 16.  6.  7. 18.  9.  8.\n","  13. 17.]]\n","[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n"]}],"source":["import numpy as np\n","import tensorflow as tf\n","import matplotlib.pyplot as plt\n","\n","# set this below to \"./\" if you just want to use the current directory to save temporary model files\n","# else set it to whereever makes sense for you\n","my_temp_folder = \"./\"\n","\n","# Set up some variables used in data collection and in training \n","num_training_examples = 10000\n","num_test_examples = 100\n","example_length = 20\n","lower_bound = -1\n","upper_bound = 20 \n","\n","# Create Training and Test input variables \n","X_train = np.random.randint(low=lower_bound,high=upper_bound, \n","                      size=(num_training_examples,example_length)).astype(float)\n","X_test = np.random.randint(low=lower_bound,high=upper_bound, \n","                      size=(num_test_examples,example_length)).astype(float)\n","\n","# Create Training and Test target variable \n","def scan_example(x):\n","    \"\"\"Function Used to Create Individual Target Vectors by Scanning Input Vectors\"\"\"\n","    y = np.zeros(shape=np.shape(x))\n","    for index,val in np.ndenumerate(x):\n","        if val < 0:\n","            y[index[0]:] = 1\n","            break \n","    return y\n","Y_train = np.apply_along_axis(scan_example,1,X_train)\n","Y_test = np.apply_along_axis(scan_example,1,X_test)\n","\n","# For processing with Tensorflor / Keras, we expand the dimensionality of our data by one. \n","# In other words our inputs will be sets of sets of individual values -- rather than sets of arrays\n","X_train = np.expand_dims(X_train, axis=2)\n","X_test = np.expand_dims(X_test, axis=2)\n","Y_train = np.expand_dims(Y_train, axis=2)\n","Y_test = np.expand_dims(Y_test, axis=2)\n","\n","# Print examples from the test data (we add the T to transpose -- this just means the printing is neater. Remove it if you want to see what happens withut it. )\n","print(X_test[0].T)\n","print(Y_test[0].T)"]},{"cell_type":"markdown","metadata":{"id":"epHeBMOj5zKc"},"source":["### Step 2: Model Creation\n","\n","We will build our recurrent model for this 'alarm system' using a Keras wrapper for Tensorflow. "]},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1680018129355,"user":{"displayName":"Robert Ross","userId":"11121896988139609834"},"user_tz":-60},"id":"-Tgo6-135zKc"},"outputs":[],"source":["import tensorflow as tf\n","from tensorflow.keras import layers"]},{"cell_type":"markdown","metadata":{"id":"lfbnNEmE5zKd"},"source":["Next, lets lust define our batch size etc. We will use a state_size of 8. This is already pretty small, but you can still get results if you reduce it to 4. It just might take longer to train. "]},{"cell_type":"code","execution_count":12,"metadata":{"executionInfo":{"elapsed":10,"status":"ok","timestamp":1680018129356,"user":{"displayName":"Robert Ross","userId":"11121896988139609834"},"user_tz":-60},"id":"NyheAmUS5zKd"},"outputs":[],"source":["batch_size = 100\n","state_size = 8\n","num_classes = 1"]},{"cell_type":"markdown","metadata":{"id":"MyBzPpRY5zKd"},"source":["Next, let's define our model. We will be able to build our model based around the 'Sequential' style constructor that Keras uses. Keep in mind that not ever model that people build is sequential. For now though, it is perfect. "]},{"cell_type":"code","execution_count":13,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1680018129356,"user":{"displayName":"Robert Ross","userId":"11121896988139609834"},"user_tz":-60},"id":"WsXKs1n45zKe"},"outputs":[],"source":["model = tf.keras.Sequential()"]},{"cell_type":"markdown","metadata":{"id":"JfM0sXO45zKe"},"source":["Next, let's define our RNN layer. We need to specify the hidden state size. We also ask Tensorflow not just to output values for every step in the sequence -- not just the final step. Finally, to help guide tensorflow we give it some guidence on the input data dimensions it should expect; here 20 is the length of each example and 1 is the dimensionality of each input element, i.e., just a single number. We don't need to specify the batch size, Keras assumes we will have data in batches -- even if they are just batches of length 1. "]},{"cell_type":"code","execution_count":14,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1680018129356,"user":{"displayName":"Robert Ross","userId":"11121896988139609834"},"user_tz":-60},"id":"25_6BBQb5b6M"},"outputs":[],"source":["# Input will be passed to an basic RNN layer - sequences will be generated \n","model.add(layers.SimpleRNN(state_size,return_sequences=True,input_shape=(20,1)))"]},{"cell_type":"markdown","metadata":{"id":"SIBzzumR5zKe"},"source":["Next, we are going to let the full output of the recurrent network be passed to a single output unit. The output unit will be logistic since we are dealing with a single simple classification tasks. "]},{"cell_type":"code","execution_count":15,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1680018129356,"user":{"displayName":"Robert Ross","userId":"11121896988139609834"},"user_tz":-60},"id":"gZ5T0jSP5zKf"},"outputs":[],"source":["# add an output layer consisting of 1 single logistic classifier \n","model.add(tf.keras.layers.Dense(num_classes, activation=tf.nn.sigmoid))"]},{"cell_type":"markdown","metadata":{"id":"AuqlOZnH5zKf"},"source":["Next, we need to compile our model. We use cross entropy since we want our targets are 0s and 1s and our final layer neuron is pretty good at predicting them. We will use the adam optimiser, and ask for an accuracy metric to be calculated. "]},{"cell_type":"code","execution_count":16,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1680018129356,"user":{"displayName":"Robert Ross","userId":"11121896988139609834"},"user_tz":-60},"id":"L4k-9zCB5zKf"},"outputs":[],"source":["model.compile(loss=\"binary_crossentropy\", \n","                   optimizer='adam', \n","                   metrics=['accuracy'])"]},{"cell_type":"markdown","metadata":{"id":"6KSHx7jK5zKf"},"source":["Let's have a look at the built, but not yet trained model. "]},{"cell_type":"code","execution_count":17,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1680018129356,"user":{"displayName":"Robert Ross","userId":"11121896988139609834"},"user_tz":-60},"id":"yzzd84QV5zKg","outputId":"58e71fdc-5d87-45cf-8a67-c944241ca07a"},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," simple_rnn (SimpleRNN)      (None, 20, 8)             80        \n","                                                                 \n"," dense (Dense)               (None, 20, 1)             9         \n","                                                                 \n","=================================================================\n","Total params: 89\n","Trainable params: 89\n","Non-trainable params: 0\n","_________________________________________________________________\n"]}],"source":["model.summary()"]},{"cell_type":"markdown","metadata":{"id":"tvFJY1_yAGSf"},"source":["Note that our output is a vector of 20 elements, i.e., it is a sequence. Our loss function will be calculating a total loss across all 20 elements. Loss will be calculated on an item by item basis. If we have made the correct prediction at some point, we do not portion some of the loss / blame to the unit at that point, i.e., we don't average the losses back out over the 20 elements. "]},{"cell_type":"markdown","metadata":{"id":"EY-CMWis5zKh"},"source":["We can see that in the great scheme of things this is a very small model. We could in practice make it much smaller by reducing the size of the hidden state. If we did this our model would likely still converge to a good model, but it would not do so as frequently. We might have to rerun training to get a good model.  \n","\n","Next, time to train the model. We provide input and output data, and specify a batch size for training as well as the number of epochs. If the model didn't train for you, increase the number of epochs. "]},{"cell_type":"code","execution_count":18,"metadata":{"executionInfo":{"elapsed":143265,"status":"ok","timestamp":1680018272616,"user":{"displayName":"Robert Ross","userId":"11121896988139609834"},"user_tz":-60},"id":"sYvcAE7y5zKh"},"outputs":[],"source":["hist = model.fit(X_train, Y_train, \n","           batch_size=batch_size,\n","           epochs=100, verbose=0) "]},{"cell_type":"markdown","metadata":{"id":"jmXIYmxb5zKh"},"source":["Note that I had verbosity turned off to save on some printing space, but feel free to set verbose=1 to see the actual progress of training. \n","\n","Next, let's predict based on some test data and see what our outputs look like"]},{"cell_type":"code","execution_count":19,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":21,"status":"ok","timestamp":1680018272616,"user":{"displayName":"Robert Ross","userId":"11121896988139609834"},"user_tz":-60},"id":"jZgu_T1n5zKh","outputId":"78575497-4e93-4b92-b3c6-8cc42600b2b0"},"outputs":[{"name":"stdout","output_type":"stream","text":["1/1 [==============================] - 0s 192ms/step\n","[[14. 12. 13. 13. 19.  4. 16. 10.  1. 13.  9. 14. 16.  6.  7. 18.  9.  8.\n","  13. 17.]]\n","[[2.8567560e-04 1.7283663e-05 3.3696328e-05 4.9623883e-05 2.0529176e-05\n","  1.2700185e-04 5.1920022e-05 6.9267662e-05 2.2660912e-04 9.6176671e-05\n","  8.9629240e-05 4.1427898e-05 1.8018713e-05 6.1348219e-05 1.0366167e-04\n","  2.3916826e-05 4.1651005e-05 7.2412309e-05 4.4640958e-05 1.6268647e-05]]\n"]}],"source":["res = model.predict(X_test,batch_size=batch_size)\n","print(X_test[0].T)\n","print(res[0].T)"]},{"cell_type":"markdown","metadata":{"id":"uQbNBLgx5zKi"},"source":["See that in the input data that when there is a -1 that the output switches from values less than 0.5 to values greater than 0.5. In the ideal case this should be a clean separation between 0s and 1s, but typically we need a more powerful model to pull that off. \n","\n"," "]},{"cell_type":"markdown","metadata":{"id":"AtLN5E2cD_Oj"},"source":["## Aside: Recurrent vs Recursive Networks  \n","A Recurrent Neural Network is one in which the hidden state of the network is reapplied as a parameter into the network when the next input comes along. The short hand RNN is often used for Recurrent Neural Networks. A Recursive Neural Network meanwhile is a distinct architectural approach in neural networks where we attempt to apply a model in a recursive way over an analysis. \n","\n","This distinction is made clear with an example. Consider the following example of a Recurrent Neural Network. A single network layer produces an output for a given input. While the hidden state is reapplied into the network for the analysis of each individual input token, there is no recursive structure. \n","\n","<!-- recurrent.png \n","Figure sourced without permission from http://www.slideshare.net/jiessiecao/parsing-natural-scenes-and-natural-language-with-recursive-neural-networks  --> \n","<img width=\"500\" src=\"https://drive.google.com/uc?id=1hEVc5krN2Qo_fzay-ndCcncU-kw8FrGO\"/>\n","\n","On the other hand consider the example below of a Recursive Neural Network. Here a model is applied to the total input to produce an output for the total input. That total output is then applied as input to a new higher-order tier of analysis. The output of that analysis can in turn be applied as the input to another higher-order tier of analysis and so forth. Thus the Recursive Neural Network recursively analyses the total input and can be thought of conceptually as being similar to a parser.  \n","\n","<!-- recursive.png\n","Figure sourced without permission from http://www.slideshare.net/jiessiecao/parsing-natural-scenes-and-natural-language-with-recursive-neural-networks \n"," --> \n","<img width=\"500\" src=\"https://drive.google.com/uc?id=1hFbDl3d8XAudjCvaPrwYA_sasoxZRqYw\"/>\n","\n","It should be noted that it is possible to construct Recursive Recurrent Neural Networks. Here a Recurrent Network is used to generate an output for one tier of the analysis, but that output is then fed into the network again to produce a higher order tier. For our purposes here we will stay focused on recurrent neural network architectures. "]},{"cell_type":"markdown","metadata":{"id":"RRdWix0QD_OI"},"source":["## Long-Short Term Memory\n","\n","The vanilla RNN that we have looked at to this point is very useful at explaining the basics of how RNNs work, but in practice the vanilla RNN is not strong enough for us to perform complex sequential tasks. \n"]},{"cell_type":"markdown","metadata":{"id":"VsIYZvnTKpIp"},"source":["### The Challenge: Travelling Back in Time! \n","\n","Let's say for example we have a case where a change in the output at T=6 should be dependent on the input at T=1. In an ideal case the backpropogation should allow us to learn this sort of dependency -- but in practice it doesn't. The reason for this is essentially that the learning signal tends to be biased in errors coming from the current time point rather than future time points. We can understand this better by looking at the figure below. The dark coloring indicates the strength of signal between inputs and outputs in either the forward or backward propogation stages. We see that there is a strong relationship between X and Y at the first time step, but because extra layers are present as we move from X at T=1 down to Y at T=2,3,4 etc. that it is more difficult to have a strong signal between X at T=1 and Y's that are further away in time. \n","\n","<!-- bit-1.png --> \n","<img width=\"500\" src=\"https://drive.google.com/uc?id=1KsySRmGN6LGkdTcE88sTiuMLhzgsxADF\"/>\n","\n","\n","This means that at T=6 for example, we cost function might be able to identify that something should be changed and start passing pack the error derivative to achieve that change, but by the time the error derivative makes it back to the first hidden layer at T=0, that error derivative signal is in practice too small relative to earlier error derivative time signals to be paid attention to. \n","\n","What we need is some way to have more direct control over what error signal passes through the network. Our error signal will still get smaller as it backprrops through time, but we will learn not to corrupt it with error derivatives that just aren't as relevant to us. \n","\n","This is illustrated in the figure below. Circles indicate that the unit is accepting signal for that neuron, while dashes indicates that the unit is closed to input in that direction. \n","\n","<!-- bit-2.png --> \n","<img width=\"500\" src=\"https://drive.google.com/uc?id=1KtcFZEk-H8rhJVzbRCDc3eFYlt2629aM\"/>\n","\n","The hand-waving idea here is that there is less leakage or corruption of the error derivative as we move through time. In practice it is a bit more complicated than that -- but not that much. \n","\n"]},{"cell_type":"markdown","metadata":{"id":"KxK4v3NnD_Ok"},"source":["### Long Short Term Memory Overview\n","\n","The best known example of an RNN that overcomes this challenge is called the Long Short Term Memory (LSTM) cell. The LSTM cell design is over 20 years old but has proven to be extremely beneficial. It would no longer be considered state-of-the-art for sequence processing tasks -- particularly if you have a lot of computing power to spare -- but for many common sequence processing tasks it is still extensively used. \n","\n","There are many resources available online that provide excellent introductions to the model. The blog post by Christopher Olah is particularly excellent and well known. Rather than repeating that material here, please review Christopher's blog post on LSTM in detail. \n","\n","http://colah.github.io/posts/2015-08-Understanding-LSTMs/\n","\n","In the lecture I step through the model, but I refer you to this blog post as a source of information on the model. \n","\n","### Putting LSTM to work: Classification\n","\n","Despite the fact that the LSTM model is considerably more complex than the basic RNN cell, the actual usage of the model within Keras / Tensorflow is practically just as easy as the basic RNN case. Let's see how our text calssification task from the last session can be coverted over to use RNNs first, but then LSTMs instead. \n"]},{"cell_type":"code","execution_count":20,"metadata":{"executionInfo":{"elapsed":20,"status":"ok","timestamp":1680018272618,"user":{"displayName":"Robert Ross","userId":"11121896988139609834"},"user_tz":-60},"id":"M30DUgoCK13S"},"outputs":[],"source":["import numpy as np\n","from tensorflow.keras.preprocessing import sequence\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, Embedding, Conv1D, GlobalMaxPooling1D, GlobalAveragePooling1D, LSTM, SimpleRNN\n","from tensorflow.keras.datasets import imdb\n","import tensorflow as tf"]},{"cell_type":"markdown","metadata":{"id":"CBwO6_Zux_-K"},"source":["Let's set up some paraemters and load up the dataset. "]},{"cell_type":"code","execution_count":21,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5267,"status":"ok","timestamp":1680018277865,"user":{"displayName":"Robert Ross","userId":"11121896988139609834"},"user_tz":-60},"id":"AtrSQ69HyAWZ","outputId":"aa76c162-d71f-40f8-ab68-ac73ea973d0f"},"outputs":[{"name":"stdout","output_type":"stream","text":["Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n","17464789/17464789 [==============================] - 5s 0us/step\n","Train shape: (25000, 400)\n","Test shape: (25000, 400)\n"]}],"source":["max_features = 5000\n","maxlen = 400\n","embedding_dims = 16\n","epochs = 5\n","\n","(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n","x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n","x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n","print('Train shape:', x_train.shape)\n","print('Test shape:', x_test.shape)"]},{"cell_type":"markdown","metadata":{"id":"08xsz-EkyHxd"},"source":["Let's define the same basic model we used the last time, and then set up the training on it. "]},{"cell_type":"code","execution_count":22,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":27,"status":"ok","timestamp":1680018277865,"user":{"displayName":"Robert Ross","userId":"11121896988139609834"},"user_tz":-60},"id":"EG5C1F7PyIDo","outputId":"280e89b7-3c75-4f9d-fad2-177c0ea7ba4c"},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: \"sequential_1\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," embedding (Embedding)       (None, 400, 16)           80000     \n","                                                                 \n"," flatten (Flatten)           (None, 6400)              0         \n","                                                                 \n"," dense_1 (Dense)             (None, 2)                 12802     \n","                                                                 \n"," dense_2 (Dense)             (None, 1)                 3         \n","                                                                 \n","=================================================================\n","Total params: 92,805\n","Trainable params: 92,805\n","Non-trainable params: 0\n","_________________________________________________________________\n"]}],"source":["model = Sequential()\n","\n","model.add(Embedding(max_features,\n","                    embedding_dims,\n","                    input_length=maxlen))\n","model.add(Flatten())\n","model.add(Dense(2, activation='tanh'))\n","model.add(Dense(1,activation=\"sigmoid\"))\n","model.summary()"]},{"cell_type":"code","execution_count":23,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":101357,"status":"ok","timestamp":1680018379204,"user":{"displayName":"Robert Ross","userId":"11121896988139609834"},"user_tz":-60},"id":"CtpTne4UzWXO","outputId":"d511e64a-03f0-4ae4-9cc5-8e0a5a92d19f"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/5\n","782/782 [==============================] - 5s 5ms/step - loss: 0.4732 - accuracy: 0.7727 - val_loss: 0.3141 - val_accuracy: 0.8795\n","Epoch 2/5\n","782/782 [==============================] - 3s 4ms/step - loss: 0.2392 - accuracy: 0.9164 - val_loss: 0.2950 - val_accuracy: 0.8810\n","Epoch 3/5\n","782/782 [==============================] - 3s 4ms/step - loss: 0.1507 - accuracy: 0.9554 - val_loss: 0.3200 - val_accuracy: 0.8748\n","Epoch 4/5\n","782/782 [==============================] - 3s 4ms/step - loss: 0.0892 - accuracy: 0.9795 - val_loss: 0.3491 - val_accuracy: 0.8689\n","Epoch 5/5\n","782/782 [==============================] - 3s 4ms/step - loss: 0.0490 - accuracy: 0.9917 - val_loss: 0.3863 - val_accuracy: 0.8672\n"]},{"data":{"text/plain":["<keras.callbacks.History at 0x23ac8b5c0d0>"]},"execution_count":23,"metadata":{},"output_type":"execute_result"}],"source":["model.compile(optimizer='adam',\n","              loss=tf.keras.losses.BinaryCrossentropy(),\n","              metrics=['accuracy'])\n","\n","model.fit(x_train, y_train,\n","          batch_size=32,\n","          epochs=epochs,\n","          validation_data=(x_test, y_test))"]},{"cell_type":"markdown","metadata":{"id":"wiuzWMnmz-Kq"},"source":["Now, let's set up a model that uses the basic RNN. "]},{"cell_type":"code","execution_count":24,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sgOEoG1l0EtT","outputId":"0af69614-3e52-4c8d-c566-6a46d8710bc0"},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: \"sequential_2\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," embedding_1 (Embedding)     (None, None, 16)          80000     \n","                                                                 \n"," simple_rnn_1 (SimpleRNN)    (None, 16)                528       \n","                                                                 \n"," dense_3 (Dense)             (None, 2)                 34        \n","                                                                 \n"," dense_4 (Dense)             (None, 1)                 3         \n","                                                                 \n","=================================================================\n","Total params: 80,565\n","Trainable params: 80,565\n","Non-trainable params: 0\n","_________________________________________________________________\n","Epoch 1/5\n","782/782 [==============================] - 52s 64ms/step - loss: 0.5709 - accuracy: 0.7010 - val_loss: 0.4438 - val_accuracy: 0.8139\n","Epoch 2/5\n","782/782 [==============================] - 51s 65ms/step - loss: 0.4453 - accuracy: 0.8028 - val_loss: 0.4250 - val_accuracy: 0.8256\n","Epoch 3/5\n","782/782 [==============================] - 50s 65ms/step - loss: 0.3495 - accuracy: 0.8618 - val_loss: 0.4280 - val_accuracy: 0.8155\n","Epoch 4/5\n","782/782 [==============================] - 49s 63ms/step - loss: 0.2527 - accuracy: 0.9113 - val_loss: 0.4321 - val_accuracy: 0.8207\n","Epoch 5/5\n","782/782 [==============================] - 50s 64ms/step - loss: 0.1869 - accuracy: 0.9394 - val_loss: 0.4920 - val_accuracy: 0.8124\n"]},{"data":{"text/plain":["<keras.callbacks.History at 0x23acb5a9c70>"]},"execution_count":24,"metadata":{},"output_type":"execute_result"}],"source":["model = Sequential()\n","model.add(Embedding(max_features, embedding_dims))\n","model.add(SimpleRNN(embedding_dims))\n","model.add(Dense(2,activation=\"tanh\"))\n","model.add(Dense(1,activation=\"sigmoid\"))\n","model.summary()\n","\n","model.compile(optimizer='adam',\n","              loss=tf.keras.losses.BinaryCrossentropy(),\n","              metrics=['accuracy'])\n","\n","model.fit(x_train, y_train,\n","          batch_size=32,\n","          epochs=epochs,\n","          validation_data=(x_test, y_test))"]},{"cell_type":"code","execution_count":25,"metadata":{"id":"7gx4hbK95zKe"},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: \"sequential_3\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," embedding_2 (Embedding)     (None, None, 16)          80000     \n","                                                                 \n"," lstm (LSTM)                 (None, 16)                2112      \n","                                                                 \n"," dense_5 (Dense)             (None, 2)                 34        \n","                                                                 \n"," dense_6 (Dense)             (None, 1)                 3         \n","                                                                 \n","=================================================================\n","Total params: 82,149\n","Trainable params: 82,149\n","Non-trainable params: 0\n","_________________________________________________________________\n","Epoch 1/5\n","782/782 [==============================] - 101s 126ms/step - loss: 0.4482 - accuracy: 0.7895 - val_loss: 0.3477 - val_accuracy: 0.8612\n","Epoch 2/5\n","782/782 [==============================] - 100s 128ms/step - loss: 0.2977 - accuracy: 0.8864 - val_loss: 0.3171 - val_accuracy: 0.8718\n","Epoch 3/5\n","782/782 [==============================] - 101s 129ms/step - loss: 0.2505 - accuracy: 0.9085 - val_loss: 0.3211 - val_accuracy: 0.8759\n","Epoch 4/5\n","782/782 [==============================] - 100s 128ms/step - loss: 0.2259 - accuracy: 0.9185 - val_loss: 0.3243 - val_accuracy: 0.8680\n","Epoch 5/5\n","782/782 [==============================] - 100s 128ms/step - loss: 0.2019 - accuracy: 0.9303 - val_loss: 0.3369 - val_accuracy: 0.8743\n"]},{"data":{"text/plain":["<keras.callbacks.History at 0x23acab79310>"]},"execution_count":25,"metadata":{},"output_type":"execute_result"}],"source":["model = Sequential()\n","model.add(Embedding(max_features, embedding_dims))\n","model.add(LSTM(embedding_dims))\n","model.add(Dense(2,activation=\"tanh\"))\n","model.add(Dense(1,activation=\"sigmoid\"))\n","model.summary()\n","\n","model.compile(optimizer='adam',\n","              loss=tf.keras.losses.BinaryCrossentropy(),\n","              metrics=['accuracy'])\n","\n","model.fit(x_train, y_train,\n","          batch_size=32,\n","          epochs=epochs,\n","          validation_data=(x_test, y_test))"]},{"cell_type":"markdown","metadata":{"id":"K4GOULik5zKi"},"source":["## RNNs for Language Modeling\n","\n","Above we have seen examples for classification tasks based on both vanilla RNNs and LSTMs. As well as being used for these basic classification type scenarios, an important application of RNNs is in the area of generative AI where the model is trained to produce content rather than simply make a classification judgement. In practice the generation process is also usually dependent on some classifiers or regression models being trained, but the notable thing is really in terms of how they are used. \n","\n","This example will be based in part on the Peen Treebank tutorial from the TensorFlow website. However this has been significantly simplified in comparison to the original tutorial. A suggestion is to first work through and make sure you understand the example below. Then move on to the full Penn Treebank example. There are a number of important optimizations in that full tutorial - including variable learning rate - that are worth understanding. \n","\n","\n","### Introduction to Language Modeling\n","\n","Language Modeling is useful to illustrate RNN use as it is a straightforward task that has well understood non-neural implementations, but yet is extremely important in a range of real-world applications and is now commonly implemented with a neural architecture. \n","\n","A language model attempts to learn the properties of a language such that when given for example n tokens, the model can accurately predict the $n+1^{th}$ token. For example, given the single token \"Merry\" the following word is highly likely to be \"Christmas\". Here just a 1 word context window is enough to help make a good prediction. Normally some more context is required. Given the string \"turned the\", we would have many reasonable candidates for the next word, but likely none that dominate. However given some more context, such as \"she jumped into the car and turned the\", a language model might reasonable expect the word \"keys\" or \"key\" to be predicted with relatively high probability, while words like \"wheel\" \"engine\", or \"ignition\" might also be given a relatively high probability. \n","\n","Linguistic context clues can of course be much further back in the text. Consider the example: \"He sat with the ring in his pocket, perspiring, anxious, waiting for the perfect moment that would never come. Finally he slid his hand from his pocket, and asked Marie if she would\". Perhaps \"marry\" would be a suitable next word given this longer context. But looking back only two, three, or even four words would make this unlikely. \n","\n","Until recently a language model was built statistically using ngram methods. An algorithm (the estimator) would parse a large body of text and build up tables of all 1-grams, 2-grams, 3-grams and perhaps 4-grams and 5-grams, and use some relatively straightforward probability calculations to give probabilities for individual words given a context.\n","\n","This statistical form of language modeling is an important tool in Natural Language Processing tasks. Python NLTK (natural langauge toolkit) has a simple implementation of an NGram Language Modeler that you could use to estimate your own language models from texts. Let's use it to parse some simple texts and then use it to make predictions. \n","\n","Note that we will be making use of the non neural tool nltk (Natural Langauge Toolkit) to help set up this example. \n","\n","Note that you might need to upgrade the version of nltk installed on your notebook. To install the upgrade, just remove the comment below, run the block, and restart the runtime. "]},{"cell_type":"code","execution_count":26,"metadata":{"id":"SzePWsXe5zKi"},"outputs":[{"name":"stderr","output_type":"stream","text":["\n","[notice] A new release of pip is available: 23.0.1 -> 23.1\n","[notice] To update, run: python.exe -m pip install --upgrade pip\n"]},{"name":"stdout","output_type":"stream","text":["Collecting nltk==3.5\n","  Downloading nltk-3.5.zip (1.4 MB)\n","     ---------------------------------------- 0.0/1.4 MB ? eta -:--:--\n","     ------ --------------------------------- 0.2/1.4 MB 6.9 MB/s eta 0:00:01\n","     ------------- -------------------------- 0.5/1.4 MB 6.0 MB/s eta 0:00:01\n","     -------------------- ------------------- 0.7/1.4 MB 5.7 MB/s eta 0:00:01\n","     -------------------------- ------------- 0.9/1.4 MB 5.4 MB/s eta 0:00:01\n","     ------------------------------------ --- 1.3/1.4 MB 5.5 MB/s eta 0:00:01\n","     ---------------------------------------- 1.4/1.4 MB 5.1 MB/s eta 0:00:00\n","  Preparing metadata (setup.py): started\n","  Preparing metadata (setup.py): finished with status 'done'\n","Requirement already satisfied: click in d:\\projects\\repos\\tu060\\venv_dl\\lib\\site-packages (from nltk==3.5) (8.1.3)\n","Requirement already satisfied: joblib in d:\\projects\\repos\\tu060\\venv_dl\\lib\\site-packages (from nltk==3.5) (1.2.0)\n","Collecting regex\n","  Downloading regex-2023.3.23-cp38-cp38-win_amd64.whl (267 kB)\n","     ---------------------------------------- 0.0/267.9 kB ? eta -:--:--\n","     -------------------------------------  266.2/267.9 kB 5.4 MB/s eta 0:00:01\n","     -------------------------------------- 267.9/267.9 kB 4.1 MB/s eta 0:00:00\n","Requirement already satisfied: tqdm in d:\\projects\\repos\\tu060\\venv_dl\\lib\\site-packages (from nltk==3.5) (4.65.0)\n","Requirement already satisfied: colorama in d:\\projects\\repos\\tu060\\venv_dl\\lib\\site-packages (from click->nltk==3.5) (0.4.6)\n","Building wheels for collected packages: nltk\n","  Building wheel for nltk (setup.py): started\n","  Building wheel for nltk (setup.py): finished with status 'done'\n","  Created wheel for nltk: filename=nltk-3.5-py3-none-any.whl size=1434687 sha256=b15baddb56f46a7350c8e76617161fca90de7ded03f0572c6f5da9b823e60a5c\n","  Stored in directory: c:\\users\\newlo\\appdata\\local\\pip\\cache\\wheels\\ff\\d5\\7b\\f1fb4e1e1603b2f01c2424dd60fbcc50c12ef918bafc44b155\n","Successfully built nltk\n","Installing collected packages: regex, nltk\n","Successfully installed nltk-3.5 regex-2023.3.23\n"]},{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package punkt to\n","[nltk_data]     C:\\Users\\newlo\\AppData\\Roaming\\nltk_data...\n","[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"]},{"data":{"text/plain":["True"]},"execution_count":26,"metadata":{},"output_type":"execute_result"}],"source":["!pip install nltk==3.5\n","\n","import nltk\n","from nltk.util import ngrams\n","from nltk.lm import MLE\n","from nltk import word_tokenize\n","# we need to download a special component that is used by the tokenizer below -- don't worry about it. \n","nltk.download('punkt')"]},{"cell_type":"markdown","metadata":{"id":"9er7KBeG5zKi"},"source":["Language modeling is all about processing text. So let's go ahead and define some sentences that we will be working with. I'm writing them explicitly here so that you can look at them, but normally we would be importing sentence content from some corpus of sentences defined in a text file or some other external record. "]},{"cell_type":"code","execution_count":27,"metadata":{"id":"gAjZEDRN5zKj"},"outputs":[],"source":["docs = [\"The Lord of the Rings\", \n","        \"The Fellowship of the Ring\",\n","        \"The Two Towers\",\n","        \"The return of the King\",\n","       \"In the Second Age of Middle-earth, the lords of Elves, Dwarves, and Men are given Rings of Power. Unbeknownst to them, the Dark Lord Sauron forges the One Ring in Mount Doom, installing into it a great part of his power to dominate the other Rings, so he might conquer Middle-earth. A final alliance of men and elves battles Sauron's forces in Mordor, where Prince Isildur of Gondor severs Sauron's finger, and the Ring with it, thereby destroying his physical form. With Sauron's first defeat, the Third Age of Middle-earth begins. Unfortunately, the Ring's influence corrupts Isildur, who takes it for himself. Isildur is later killed by Orcs, and the Ring is lost for 2,500 years, until it is found by Gollum, who owns it for five centuries. The Ring is then found by a hobbit named Bilbo Baggins, who turns invisible when he puts it on, but is unaware of its history.\",\n","        \"Sixty years later, Bilbo celebrates his 111th birthday in the Shire, reuniting with his old friend, Gandalf the Grey. Bilbo reveals that he intends to leave the Shire for one last adventure, and he leaves his inheritance, including the Ring, to his nephew, Frodo. Although Bilbo has begun to become corrupted by the Ring and tries to keep it for himself, Gandalf intervenes. Gandalf, suspicious of the Ring, tells Frodo to keep it secret and to keep it safe. Gandalf then investigates the Ring, discovers its true nature, and returns to warn Frodo. Gandalf also learns that Gollum was captured and tortured by Orcs, and that Gollum uttered two words during his interrogation: Shire and Baggins. Gandalf instructs Frodo to leave the Shire, accompanied by his friend Samwise Gamgee. Gandalf rides to Isengard to meet with fellow wizard Saruman the White, but learns that he has joined forces with Sauron, who has dispatched his nine undead Nazgûl servants to find Frodo. After a brief battle, Saruman imprisons Gandalf.\",\n","       \"Frodo and Sam are joined by fellow hobbits Merry and Pippin, and they evade the Nazgûl, arriving in Bree, where they are meant to meet Gandalf. However, Gandalf never arrives, and they are instead aided by a ranger named Strider, a friend of Gandalf's, who promises to escort them to Rivendell. The hobbits are ambushed by the Nazgûl on Weathertop, and their leader, the Witch-King, stabs Frodo with a cursed Morgul blade. Arwen, an elf and Strider's betrothed, locates Aragorn and the Hobbits and aids Frodo, rescuing him and incapacitating the Nazgûl. She takes him to Rivendell, where he is healed. Frodo meets Gandalf, who escaped Isengard with help from Gwaihir, a giant eagle. That night, Aragorn reunites with Arwen in Rivendell, where they confirm their love and commitment for each other. Arwen's father, Lord Elrond, holds a council that decides the Ring must be destroyed in Mount Doom. While the members argue, Frodo volunteers to take the Ring, accompanied by Gandalf, Sam, Merry, Pippin, elf Legolas, dwarf Gimli, Boromir of Gondor, and Strider, who is revealed to be Aragorn, Isildur's heir and the rightful King of Gondor. Bilbo gives Frodo his sword, Sting. The Fellowship of the Ring sets off, but Saruman's magic forces them to travel through the Mines of Moria, much to Gandalf's displeasure.\",\n","       \"The Fellowship discovers that the dwarves within Moria have been slain, and they are attacked by Orcs and a cave troll. They defeat them, but are confronted by Durin's Bane, a Balrog residing within the mines. Gandalf casts the Balrog into a vast chasm, but it drags Gandalf down into the darkness with it. The rest of the Fellowship, now led by Aragorn, reaches Lothlórien, home to elves Galadriel and Celeborn. Galadriel privately informs Frodo that only he can complete the quest, and that one of his friends will try to take the Ring. Meanwhile, Saruman creates an army of Uruk-hai to track down and kill the Fellowship.\",\n","       \"The Fellowship leaves Lothlórien by river to Parth Galen. Frodo wanders off and is confronted by Boromir, who tries to take the Ring in desperation. Afraid of the Ring corrupting his friends, Frodo decides to travel to Mordor alone. The Fellowship is then ambushed by the Uruk-hai. Merry and Pippin are taken captive, and Boromir is mortally wounded by the Uruk chieftain, Lurtz. Aragorn arrives and slays Lurtz, and watches Boromir die peacefully. Sam follows Frodo, accompanying him to keep his promise to Gandalf to protect Frodo, while Aragorn, Legolas, and Gimli go to rescue Merry and Pippin.\",\n","       \"Awakening from a dream of Gandalf the Grey battling the Balrog, Frodo Baggins and his friend Samwise Gamgee find themselves lost in the Emyn Muil near Mordor and soon become aware that they are being stalked by Gollum, the former owner of the One Ring. After capturing him, a sympathetic Frodo decides to use Gollum as a guide to Mordor, despite Sam's objections.\",\n","       \"Meanwhile, Aragorn, Legolas and Gimli pursue the Uruk-hai to save their companions Merry and Pippin. The Uruk-hai are ambushed by a group of Rohirrim, while the two Hobbits escape into Fangorn Forest and encounter Treebeard, an Ent. Aragorn's group later meets the Rohirrim and their leader Éomer, who reveals that they have been exiled by their king Théoden who is being manipulated by Saruman and his servant Gríma Wormtongue into turning a blind eye to Saruman's forces running rampant in Rohan. While searching for the Hobbits in Fangorn, Aragorn's group encounters Gandalf, who, after succumbing to his injuries while killing the Balrog in Moria, has been resurrected as Gandalf the White to help save Middle-earth.\",\n","       \"Aragorn's group travels to Rohan's capital city Edoras, where Gandalf releases Théoden from Saruman's influence and Wormtongue is banished. After learning of Saruman's plans to wipe out Rohan with his Uruk-hai army, Théoden decides to evacuate his citizens to Helm's Deep, an ancient fortress that has provided refuge to Rohan's people in times past, while Gandalf departs to acquire the aid of Éomer's army. Aragorn establishes a friendship with Théoden's niece, Éowyn, who quickly becomes infatuated with him. That night, Aragorn experiences a dream of him meeting with Arwen who then tells him that he must depart with Frodo and reassures him when he doesn't feel confident enough that his path is set for him. When the refugees come under attack by Warg-riding Orcs sent by Saruman, Aragorn falls off a cliff and is presumed dead. However, he is awoken by his horse Brego and rides to Helm's Deep. That same night, Arwen is told by her father Elrond that Aragorn will not be returning. He tells her that if she remains in Middle-Earth, only death and destruction await her, even if Aragorn were to survive and become King of Gondor, and if Sauron is defeated again, she will die alone. This convinces her to reluctantly depart for Valinor. The defenders at Helm's Deep are joined by a detachment of Elves from Lothlórien. The Uruk-hai army arrives at Helm's Deep that night and a night-long battle ensues. The Uruk-hai breach the outer wall using gunpowder-like explosives and force the remaining defenders to retreat into the inner castle.\",\n","       \"Merry and Pippin, having convinced Treebeard that they were allies, are brought to an Ent Council in Fangorn where the Ents decide not to assist in the war. Pippin then tells Treebeard to take them in the direction of Isengard, where they witness the devastation caused to the forest by Saruman's war efforts. An enraged Treebeard summons the Ents and they storm Isengard, drowning the orcs by breaking their river dam and stranding Saruman in Orthanc.\",\n","       \"At Helm's Deep, Aragorn convinces a despairing Théoden to ride out and meet the Uruks in one last charge. Gandalf and Éomer's horsemen arrive at sunrise, turning the tide of the battle. The Uruk-hai flee into Fangorn forest, which has moved closer to the battle at the urging of Treebeard, where they are destroyed. Gandalf warns that Sauron's retaliation will be terrible and swift.\",\n","       \"Meanwhile, Gollum leads Frodo and Sam through the Dead Marshes to the Black Gate but convinces them to enter Mordor by an alternative route. Frodo and Sam are captured by the Rangers of Ithilien led by Faramir, brother of the late Boromir. Frodo helps Faramir catch Gollum to save him from being killed and Faramir learns of the One Ring and takes his captives with him to Gondor to win his father's respect. While passing through the besieged Gondorian city of Osgiliath, Sam reveals that Boromir was driven mad by the Ring and tried to take it. An attacking Nazgûl nearly captures Frodo, who momentarily attacks Sam before coming to his senses, forcing Sam to remind him that they are fighting for the good still left in Middle-earth. Faramir is impressed by Frodo and releases them along with Gollum. While leading the hobbits once more, Gollum decides to take revenge on Frodo and reclaim the Ring by leading the group to Her upon arriving at Cirith Ungol.\"\n","       \"Two Hobbits, Sméagol and Déagol, are fishing when Déagol discovers the One Ring in the river. Sméagol is ensnared by the Ring, and kills his friend for it. He retreats into the Misty Mountains as the Ring twists his body and mind, until he becomes the creature Gollum.\",\n","       \"Centuries later, during the War of the Ring, Gandalf leads Aragorn, Legolas, Gimli, and King Théoden to Isengard, where they reunite with Merry and Pippin. Gandalf retrieves the defeated Saruman's palantír. Pippin later looks into the seeing-stone and is telepathically attacked by Sauron. From Pippin's description of his visions, Gandalf surmises that Sauron will attack Gondor's capital Minas Tirith. He rides there to warn Gondor's steward Denethor, taking Pippin with him.\",\n","       \"Gollum leads Frodo Baggins and Samwise Gamgee to Minas Morgul, where they watch the Witch-king, leader of the nine Nazgûl, lead an army of Orcs towards Gondor. The hobbits begin climbing a stair carved in the cliff face that will take them into Mordor via a secret way, unaware that Gollum plans to kill them and take the Ring. The Witch-king and his forces strike and overwhelm Osgiliath, forcing Faramir and his garrison to retreat to Minas Tirith.\",\n","       \"Gollum disposes of the Hobbits' food, blaming Sam. Believing that Sam desires the Ring, Frodo tells him to go home before he and Gollum continue to the tunnel leading to Mordor. Gollum tricks him into venturing into the lair of the giant spider Shelob. Frodo narrowly escapes and confronts Gollum, telling him that he must destroy the Ring for both their sakes. Gollum attacks Frodo but falls down a chasm. Frodo continues on, but Shelob discovers, paralyses, and binds him. However, Sam returns and injures Shelob, driving her away. Sam hides as Orcs appear and take Frodo with them. The Orcs start a fight because of ownership of Frodo's mithril vest, allowing Sam to escape with Frodo and continue their journey.\",\n","       \"Aragorn learns from Elrond that Arwen's strength is fading, having refused to leave and wishing to stay in Middle Earth, reunite with Aragorn, start a family, and willingly become mortal, after seeing a vision of her son with Aragorn. Arwen convinces a reluctant Elrond to accept her choice of staying in Middle Earth and to arm Aragorn with Andúril, a sword recreated by Rivendell blacksmiths on Elrond's orders from the shards of Isildur's sword, Narsil, so he can reclaim his birthright while gaining reinforcements from the Dead Men of Dunharrow. Joined by Legolas and Gimli, Aragorn travels to the Paths of the Dead, recruiting the Army of the Dead by pledging to release them from the curse Isildur put on them.\",\n","       \"Faramir is gravely wounded after a futile effort to retake Osgiliath; believing his son to be dead, Denethor falls into madness. Gandalf is left to defend the city against the Orc army, led by Gothmog. As Gothmog's army forces its way into the city, Denethor attempts to kill himself and Faramir on a pyre. Pippin alerts Gandalf and they save Faramir, but a burning Denethor leaps to his death from the top of Minas Tirith just before Théoden and his nephew, Éomer, arrive with the Rohirrim. During the ensuing battle, they are overwhelmed by the Oliphaunt-riding Haradrim, while the Witch-King mortally wounds Théoden. Though Théoden's niece Éowyn destroys the Witch-king with Merry's help, Théoden succumbs to his wounds. Aragorn arrives with the Army of the Dead, who overcome Sauron's forces and win the battle; Aragorn then frees them from the curse.\",\n","       \"Aragorn decides to march upon the Black Gate as a distraction so Frodo and Sam can reach Mount Doom. Aragorn's army draws out Sauron's remaining forces and empties Mordor, allowing Frodo and Sam to reach the volcano, but Gollum attacks them just as they reach Mount Doom. As he stands on the ledge over the volcanic fire, Frodo succumbs to the Ring and claims it as his own, putting it on his finger. Despite Frodo being invisible, Gollum manages to find and attack him, biting his finger off to reclaim the Ring. Frodo fights back, and as they struggle over the Ring, both fall off the ledge. Gollum falls into the lava with the Ring and dies. Frodo clings to the side of the ledge, and Sam rescues him as the Ring disintegrates in the lava. As Frodo and Sam escape, Sauron is destroyed along with his forces and the Nine as Mordor crumbles.\",\n","       \"Gandalf flies in with eagles to rescue the Hobbits, who awaken in Minas Tirith and are reunited with the surviving Fellowship. Aragorn is crowned King of Gondor and takes Arwen as his queen. The Hobbits return home to the Shire, where Sam marries Rosie Cotton. A few years later, Frodo departs Middle-earth for the Undying Lands with his uncle Bilbo, Gandalf, and the Elves. He leaves Sam the Red Book of Westmarch, which details their adventures. Sam returns to the Shire, where he embraces Rosie and their children.\"]\n"]},{"cell_type":"markdown","metadata":{"id":"jfstsEqA5zKj"},"source":["Next, to make the text easier to process, we split each sentence into an array of individual words. I print out the first for illustration. "]},{"cell_type":"code","execution_count":28,"metadata":{"id":"y3-v7W0i5zKj"},"outputs":[{"name":"stdout","output_type":"stream","text":["['The', 'Lord', 'of', 'the', 'Rings']\n"]}],"source":["texts = []\n","for s in docs:\n","    texts.append(word_tokenize(s))\n","print(texts[0])"]},{"cell_type":"markdown","metadata":{"id":"HrNmoH3F5zKj"},"source":["Traditional language modeling was all about counting statistics. The most important statistics are the number of times individual words occur, but also the number of time particular sequences of words occurred. The sequences here would be small, typically sequences of length 2, 3, 4 or more. These sequences are referred to as n-grams, where 2-grams refer to sequences of 2 words, 3-grams refer to sequences of 3 words and so forth. We use 1-grams to denote sequences of length 1 words -- which are of course just individual words. NLTK has lots of tools for pulling out these sequences for us. For example, lets ask NLTK to produce all 1-grams, 2-grams, and 3-grams from the first sentence. "]},{"cell_type":"code","execution_count":29,"metadata":{"id":"demwzl7P5zKj"},"outputs":[{"name":"stdout","output_type":"stream","text":["[('The',), ('Lord',), ('of',), ('the',), ('Rings',)]\n","[('The', 'Lord'), ('Lord', 'of'), ('of', 'the'), ('the', 'Rings')]\n","[('The', 'Lord', 'of'), ('Lord', 'of', 'the'), ('of', 'the', 'Rings')]\n"]}],"source":["print(list(ngrams(texts[0], n=1)))\n","print(list(ngrams(texts[0], n=2)))\n","print(list(ngrams(texts[0], n=3)))"]},{"cell_type":"markdown","metadata":{"id":"4g-D75t85zKk"},"source":["These are raw n-grams from the raw text. Often in practice we want to let words at the start and the end of sentences appear in more n-grams than they would in this pure sense. What we want to do is pad the sentences just like we discussed image padding for convolutional neural networks. NLTK can take care of the padding for us. It also does a little bit of extra work for us that isn't really important since our real focus will be on neural implementations. The key point though is that 'train' and 'vocab' below corespond to sets of n-grams and lists of pure words that we have been thinking about so far. "]},{"cell_type":"code","execution_count":30,"metadata":{"id":"aCUeNzvA5zKk"},"outputs":[],"source":["from nltk.lm.preprocessing import padded_everygram_pipeline\n","train, vocab = padded_everygram_pipeline(3, texts)"]},{"cell_type":"markdown","metadata":{"id":"D03G-Xvx5zKk"},"source":["So far all we have are the lists of n-grams and the vocab list for the complete text. Now we need to do the actual counting of our n-grams in such a way that we can easily generate new sentences and ask questions like 'what are the odds of this sentence having been seen? We do all of this through a model fitting function. How this works in a traditional (rather than neural) system is beyond our scope. "]},{"cell_type":"code","execution_count":31,"metadata":{"id":"gVONDQwh5zKk"},"outputs":[],"source":["model = MLE(3) \n","model.fit(train, vocab)"]},{"cell_type":"markdown","metadata":{"id":"UleJ8T0Y5zKk"},"source":["We can do many things with our trained model, but at its core it is just a tool for collecting probabilities. \n","\n","How many times is the word 'the', 'are', and 'Balrog' found in the corpus? "]},{"cell_type":"code","execution_count":32,"metadata":{"id":"KWj_q29U5zKl"},"outputs":[{"name":"stdout","output_type":"stream","text":["145\n","18\n","4\n"]}],"source":["print(model.counts['the'])\n","print(model.counts['are'])\n","print(model.counts['Balrog'])"]},{"cell_type":"markdown","metadata":{"id":"hUZMtgu15zKl"},"source":["let's look at the same phrases, but expressed now in terms of likelihoods over the whole corpus"]},{"cell_type":"code","execution_count":33,"metadata":{"id":"Mssa8cqB5zKl"},"outputs":[{"name":"stdout","output_type":"stream","text":["0.05238439306358381\n","0.006502890173410405\n","0.001445086705202312\n"]}],"source":["print(model.score('the'))\n","print(model.score('are'))\n","print(model.score('Balrog'))"]},{"cell_type":"markdown","metadata":{"id":"oIAiyFId5zKm"},"source":["And how many times is the phrase 'they are' seen? "]},{"cell_type":"code","execution_count":34,"metadata":{"id":"a8ZTzfgs5zKm"},"outputs":[{"name":"stdout","output_type":"stream","text":["7\n"]}],"source":["print(model.counts[['they']]['are'])"]},{"cell_type":"markdown","metadata":{"id":"Ge0jFGtu5zKm"},"source":["And what is the likelihood of seeing 'are' after 'they' rather than seeing 'are' after any other context? "]},{"cell_type":"code","execution_count":35,"metadata":{"id":"E7XPdVN45zKm"},"outputs":[{"name":"stdout","output_type":"stream","text":["0.3888888888888889\n"]}],"source":["print(model.score('are','they'.split()))"]},{"cell_type":"markdown","metadata":{"id":"umrRipwq5zKn"},"source":["So far we have just looed at statistics, but as mentioned the really interesting thing about langauge models is that they can also be used to generate text. Even with traditional (non-neural) approaches to langauge modelling it is possible to build a text generator. \n","\n","Using NLTK we can ask for a sentence of a specific length and seed the generator -- re-running with the same seed will result in the same sentence: \n"]},{"cell_type":"code","execution_count":36,"metadata":{"id":"ELWyOwlB5zKn"},"outputs":[{"name":"stdout","output_type":"stream","text":["vast chasm , but it drags Gandalf down into the lair of the One Ring\n"]}],"source":["word_list = model.generate(15, random_seed=2)\n","print(' '.join(word for word in word_list))"]},{"cell_type":"markdown","metadata":{"id":"8cBfMba65zKn"},"source":["Try re-running this with different random seeds -- this just results in different sentences. Then try re-running based on a model that is trained without 3-grams and then without 1-grams. You should see that the model is less fluid. Why do you think that is?\n","\n","### Metrics for Text Generation: Perplexity\n","Language Modeling is not very well served by traditional evaluation metrics such as Precision and Recall. Instead we usually evaluate Language Model performance with a measure called perplexity. Perplexity is essentially a very simple measure with a very grand name. Perplexity is a length normalized inverse probability of a sentence given a specific language model. In other words, given a language model $M$ and a test sentence \"W=w_{0},w_{1},..,w_{n}$, the Perplexity of W given by:\n","\n","$$ PP_{m}(W) = \\sqrt[n]{\\frac{1}{P(w_{0},w_{1},...,w_{n})}} $$\n","\n","The intuition of Perplexity is that it gives us an estimate of how much deviation there is between our language model and our test sentence W. If a given sentence W has a higher perplexity with model A than model B, this means that model A is a worse predictor of our sentence. Hence model B is interpreted as being better as it gave a higher probability of our test sentence. \n","\n","Therefore when building language models we tend to see smaller Perplexity measures as better and we attempt to reduce the average Perplexity over our data as we train our model. \n","\n","Fortunately for the sake of our calculations there is a way to establish a direct relationship between Perplexity and the Cross Entropy loss function we usually use for evaluating our trained models. If we recall, the Cross Entropy is defined as:\n","\n","$$ H(W) = - \\frac{1}{N} ln(P(W)) * P(W) $$\n","\n","In the case of a language that is assumed to not change over time, it is possible to simplify the definition of entropy. This is a significant simplification, but one that is commonly accepted: \n","\n","$$ H(W) = - \\frac{1}{N}ln(P(W)) $$\n","\n","and the perplxity is given by:\n","\n","$$ PP_{m}(W) = 2^{H(W)} $$\n","\n","It is worth keeping in mind that this definition is independent of how exactly we operationalise the Cross Entropy function. Therefore even if we use a definition of cross entropy that was used in our loss functions, we can still use it to calculate perplexity as per the equation above. \n"]},{"cell_type":"markdown","metadata":{"id":"q2a7u--V5zKn"},"source":["## Neural Language Modeling \n","\n","Turning from statistical to neural approaches, how might we attempt to achieve a language model with a neural network? One potential model for this design would be to have an input which somehow captures the individual words in some window, and then have an output that captures the relative likelihood of all words in our dictionary. Such a naive model might look like this:\n","\n","<!-- lmff.png --> \n","<img width=\"500\" src=\"https://drive.google.com/uc?id=1bvTpnJvBN1eXyXXPBSbxwp07rP7ATc-p\"/>\n","\n","(Figure sourced without permission from https://media.licdn.com/mpr/mpr/shrinknp_800_800/AAEAAQAAAAAAAAh8AAAAJGQ5MDRmM2EwLTJkOTMtNDc3ZC05MDAwLWVlYmVmMWE2NmM4Yw.png )\n","\n","where 3 words are provided as context and a softmax over all words in our dictionary is used in the output layer to decide which work is most likely given an individual context of 3 words. \n","\n","This obvious approach can be found to provide some interesting results, and a variant on this model was explored by Bengio et al as far back as 2003. While this feedforward based design can produce useful results given enough training data and training time, the feedforward approach is limited in that it can't take deeper context into account. A recurrent neural networks on the other hand is not limited to taking a fixed history into account. Instead a much deeper history can be explored by allowing the state of the hidden layer to depend not only on an input that represents a word just seen, but also have the hidden state depend on a hidden state further back in time. \n","\n","To illustrate, let's download a dataset that we can use for training our language model. "]},{"cell_type":"code","execution_count":37,"metadata":{"id":"asZGWDRv5zKn"},"outputs":[],"source":["import tensorflow as tf\n","\n","path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')\n","text = open(path_to_file, 'rb').read().decode(encoding='utf-8')"]},{"cell_type":"markdown","metadata":{"id":"B_sJRQHOTJpJ"},"source":["Uncomment the code below if you would like to see some of the text. "]},{"cell_type":"code","execution_count":38,"metadata":{"id":"kl3M26Nx5zKn"},"outputs":[],"source":["#text = \"\"\n","#for doc in docs:\n","#    text = text + doc + \"\\n\""]},{"cell_type":"markdown","metadata":{"id":"qZr3UeIB5zKo"},"source":["Check some characteristics of the text "]},{"cell_type":"code","execution_count":39,"metadata":{"id":"nNGlSVi15zKo"},"outputs":[{"name":"stdout","output_type":"stream","text":["First Citizen:\n","Before we proceed any further, hear me speak.\n"]}],"source":["# Take a look at the first 60 characters in text\n","print(text[:60])"]},{"cell_type":"markdown","metadata":{"id":"g_O4hjCh5zKo"},"source":["We have downloaded the full shakespeare text file. Rather than always working with 100% of the text file, it can be useful just to use a portion of it. The content below picks out a certain proportion of the text to work with. Change the first line to 1.0 if you want to work with all the text. "]},{"cell_type":"code","execution_count":40,"metadata":{"id":"LjYIkq7p5zKo"},"outputs":[],"source":["extract = 0.1\n","text = text[:int(extract*len(text))]"]},{"cell_type":"markdown","metadata":{"id":"ZelWqkmt5zKo"},"source":["Now, let's start the actual modelling. \n","\n","In order to limit the computational complexity of our model we are going to generate based on characters rather than words. The reason for this is that the size of our vocabulary is much smaller if we focus on the English characterset than if we focus on the set of words in English. This in turn reduces the number of parameters needed. The downside to this is that when modelling is not very good, the output of our model can really look like a lot of 'junk'!\n","\n","To move forward, we need to set up some indexes to map back and forward between characters and some number indexces that we will use to encode the text being fed into the model. "]},{"cell_type":"code","execution_count":41,"metadata":{"id":"Fara3oHk5zKp"},"outputs":[],"source":["# exract an ordered vocabulary - this will be letters, some numbers etc. \n","vocab = sorted(set(text))\n","\n","# Create mappings between vocab and numeric indices\n","char2idx = {u:i for i, u in enumerate(vocab)}\n","idx2char = np.array(vocab)\n","\n","# Map all the training text over to a numeric representation\n","text_as_int = np.array([char2idx[c] for c in text])"]},{"cell_type":"markdown","metadata":{"id":"XVGpMUXf5zKp"},"source":["At this stage we now have a representation of the text where each word is encoded as a numeric index against our dictionary. \n","\n","Next we create batches from the complete number encoded dataset. We will be splitting the dataset into sequences of length swq_length. We won't have overlap between these extracts. Let's set up the two variables we need to govern this. "]},{"cell_type":"code","execution_count":42,"metadata":{"id":"rZ1B1mbV5zKp"},"outputs":[],"source":["# The maximum length sentence we want for a single input in characters\n","seq_length = 100\n","examples_per_epoch = len(text)//(seq_length+1)"]},{"cell_type":"markdown","metadata":{"id":"NdP8G1Jk5zKp"},"source":["Let's create a TensorFlow dataset object and ask it to split the dataset into our batches of our specified length. Note that in Tensorflow, Pytorch etc dataset objects and other bits of framework for processing datasets are provided to you to simplify the management of the dataset and in some cases speed up the processing pipeline which takes care of loading and preparing data beofre a backpropagation pass. "]},{"cell_type":"code","execution_count":43,"metadata":{"id":"m8Oj_wv95zKp"},"outputs":[],"source":["# Create training examples / targets\n","char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n","sequences = char_dataset.batch(seq_length+1, drop_remainder=True)"]},{"cell_type":"markdown","metadata":{"id":"e29IaB4F5zKp"},"source":["Now comes the important parts. Let's start thinking about the training. What we will do is learn to predict one character at a time based on the current character. In other words, given character 'i' we will be predicting character 'i+1'. However what is more than that, we will predict character 'i+1' in the context of having maybe already done this task for X other characters. We will have a state that moves from one character prediction task to the next. \n","\n","This way our prediction task pays a lot of attention to the character that is next on our processing list, but it also pays attention to what we have done previously. \n","\n","To facilitate this we need to be able to split our dataset sequences on the fly into appropriate encodings for input and target data. We do that with the function below which is applied as a mapping to our input data. "]},{"cell_type":"code","execution_count":44,"metadata":{"id":"0XaWTjbg5zKp"},"outputs":[],"source":["def split_input_target(chunk):\n","    input_text = chunk[:-1]\n","    target_text = chunk[1:]\n","    return input_text, target_text\n","\n","dataset = sequences.map(split_input_target)"]},{"cell_type":"markdown","metadata":{"id":"UVYNifPz5zKq"},"source":["It is worth visualizing this to make sure we are clear on what is going on:"]},{"cell_type":"code","execution_count":45,"metadata":{"id":"rZuTMinC5zKq"},"outputs":[{"name":"stdout","output_type":"stream","text":["Input data:  'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'\n","Target data: 'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n","Step    0\n","  input: 16 ('F')\n","  expected output: 43 ('i')\n","Step    1\n","  input: 43 ('i')\n","  expected output: 52 ('r')\n","Step    2\n","  input: 52 ('r')\n","  expected output: 53 ('s')\n","Step    3\n","  input: 53 ('s')\n","  expected output: 54 ('t')\n","Step    4\n","  input: 54 ('t')\n","  expected output: 1 (' ')\n"]}],"source":["for input_example, target_example in  dataset.take(1):\n","    print ('Input data: ', repr(''.join(idx2char[input_example.numpy()])))\n","    print ('Target data:', repr(''.join(idx2char[target_example.numpy()])))\n","    \n","for i, (input_idx, target_idx) in enumerate(zip(input_example[:5], target_example[:5])):\n","    print(\"Step {:4d}\".format(i))\n","    print(\"  input: {} ({:s})\".format(input_idx, repr(idx2char[input_idx])))\n","    print(\"  expected output: {} ({:s})\".format(target_idx, repr(idx2char[target_idx])))"]},{"cell_type":"markdown","metadata":{"id":"sWj1ghIW5zKq"},"source":["To finalise the construction of our dataset, we need to set a batch size and buffer that tensorflow will use to work with the dataset. "]},{"cell_type":"code","execution_count":46,"metadata":{"id":"7irx_t6Y5zKq"},"outputs":[],"source":["BATCH_SIZE = 64\n","BUFFER_SIZE = 10000\n","\n","dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)"]},{"cell_type":"markdown","metadata":{"id":"gUWNIo2t5zKq"},"source":["Rather than mapping words directly into our RNN we will map them through our embedding layer. We will assume an embedding of dimension 256. Remember that while this is quite a wide vector, it is a much much smaller dimensionality than if we used a bag of words or similar.  "]},{"cell_type":"code","execution_count":47,"metadata":{"id":"-svhzR3W5zKq"},"outputs":[],"source":["embedding_dim = 256"]},{"cell_type":"markdown","metadata":{"id":"hWht4pxy5zKr"},"source":["We will be using an RNN for our langauge model. For this, we need to define the state size for our recurrent layer. Remember that for a basic RNN as seen last week, this is the size of the vector that is output by the RNN layer. This same vector is then propogated forward to the following layer, but also copied back around and concatenated with the input at the next point in time. "]},{"cell_type":"code","execution_count":48,"metadata":{"id":"9qyWY4AG5zKr"},"outputs":[],"source":["rnn_units = 2048"]},{"cell_type":"markdown","metadata":{"id":"_ZLCx8EI5zKr"},"source":["Now we can construct our model. As noted, the first real layer of our network will be an embedding layer which captures the meaning of our words in a real valued space. "]},{"cell_type":"code","execution_count":49,"metadata":{"id":"aMzC1L145zKr"},"outputs":[],"source":["model = tf.keras.Sequential()\n","model.add(tf.keras.layers.Embedding(len(vocab), embedding_dim,\n","                          batch_input_shape=[BATCH_SIZE, None]))"]},{"cell_type":"markdown","metadata":{"id":"S3EwKPMg5zKr"},"source":["Next we define our recurrent layer. This is the bit that will remember state from one classification task to the next. "]},{"cell_type":"code","execution_count":50,"metadata":{"id":"2abPUa475zKr"},"outputs":[],"source":["model.add(layers.LSTM(rnn_units,return_sequences=True))"]},{"cell_type":"markdown","metadata":{"id":"BvCgWH1w5zKr"},"source":["Next we define our output layer. We have a final layer where each neuron corresponds to an element in our vocab, i.e., a letter or other character that was found in the complete training data set. Note that this is going to use a linear activation function by default. This is ok, as our cost function is set up to deal with linear activations, i.e., logits, rather than the actual outputs of a softmax function. "]},{"cell_type":"code","execution_count":51,"metadata":{"id":"yq3bC_Go5zKs"},"outputs":[],"source":["model.add(tf.keras.layers.Dense(len(vocab)))"]},{"cell_type":"markdown","metadata":{"id":"M0hgMvzI5zKs"},"source":["Our model is built, now let's have a look at it. There should be no surprises here, but it is reassuring to look at. "]},{"cell_type":"code","execution_count":52,"metadata":{"id":"AUeQF0Me5zKs"},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: \"sequential_4\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," embedding_3 (Embedding)     (64, None, 256)           15616     \n","                                                                 \n"," lstm_1 (LSTM)               (64, None, 2048)          18882560  \n","                                                                 \n"," dense_7 (Dense)             (64, None, 61)            124989    \n","                                                                 \n","=================================================================\n","Total params: 19,023,165\n","Trainable params: 19,023,165\n","Non-trainable params: 0\n","_________________________________________________________________\n"]}],"source":["model.summary()"]},{"cell_type":"markdown","metadata":{"id":"7N5lBcY15zKs"},"source":["Next we need to define our loss function. We will just go with a cross entropy as we might expect for a classification type task. We also compile our model and assume the adam optimiser as always. "]},{"cell_type":"code","execution_count":53,"metadata":{"id":"apYNmCuQ5zKs"},"outputs":[],"source":["def loss(labels, logits):\n","    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n","\n","model.compile(optimizer='adam', loss=loss)"]},{"cell_type":"markdown","metadata":{"id":"fOjZk09i5zKt"},"source":["In this example that we are borrowing from, the model is saved to a checkpoint as we go along. We will see that the main reason for doing this here is so that we can later run single examples through the model rather than having to run whole batches. \n","\n","For the moment we just need to setup where our temporary checkpoint is going to be saved and create a callback object that will be supplied to the keras fit function. You can find out more about saving your model as you go along here:\n","https://www.tensorflow.org/tutorials/keras/save_and_load"]},{"cell_type":"code","execution_count":54,"metadata":{"id":"uBOM1lDK5zKt"},"outputs":[{"name":"stdout","output_type":"stream","text":["directory not used yet.\n"]}],"source":["import os\n","# Directory where the checkpoints will be saved\n","checkpoint_dir = my_temp_folder+'training_checkpoints'\n","import shutil\n","try:\n","    shutil.rmtree(checkpoint_dir)\n","except:\n","    print(\"directory not used yet.\")\n","\n","# Name of the checkpoint files\n","checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_\")\n","\n","checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n","    filepath=checkpoint_prefix,\n","    monitor='loss',\n","    save_weights_only=True, \n","    save_best_only=True)"]},{"cell_type":"markdown","metadata":{"id":"kkWSLRyY5zKt"},"source":["Everything is setup now. We can kick off training. "]},{"cell_type":"code","execution_count":55,"metadata":{"id":"UKw0Oqm15zKt"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/20\n","17/17 [==============================] - 153s 9s/step - loss: 3.9539\n","Epoch 2/20\n","17/17 [==============================] - 187s 11s/step - loss: 3.2842\n","Epoch 3/20\n","17/17 [==============================] - 128s 7s/step - loss: 3.1751\n","Epoch 4/20\n","17/17 [==============================] - 131s 8s/step - loss: 2.9582\n","Epoch 5/20\n","17/17 [==============================] - 125s 7s/step - loss: 2.7069\n","Epoch 6/20\n","17/17 [==============================] - 125s 7s/step - loss: 2.5202\n","Epoch 7/20\n","17/17 [==============================] - 121s 7s/step - loss: 2.4020\n","Epoch 8/20\n","17/17 [==============================] - 122s 7s/step - loss: 2.3191\n","Epoch 9/20\n","17/17 [==============================] - 120s 7s/step - loss: 2.2389\n","Epoch 10/20\n","17/17 [==============================] - 121s 7s/step - loss: 2.1676\n","Epoch 11/20\n","17/17 [==============================] - 122s 7s/step - loss: 2.1067\n","Epoch 12/20\n","17/17 [==============================] - 120s 7s/step - loss: 2.0513\n","Epoch 13/20\n","17/17 [==============================] - 121s 7s/step - loss: 1.9974\n","Epoch 14/20\n","17/17 [==============================] - 119s 7s/step - loss: 1.9443\n","Epoch 15/20\n","17/17 [==============================] - 117s 7s/step - loss: 1.8968\n","Epoch 16/20\n","17/17 [==============================] - 122s 7s/step - loss: 1.8510\n","Epoch 17/20\n","17/17 [==============================] - 124s 7s/step - loss: 1.8073\n","Epoch 18/20\n","17/17 [==============================] - 127s 7s/step - loss: 1.7612\n","Epoch 19/20\n","17/17 [==============================] - 120s 7s/step - loss: 1.7168\n","Epoch 20/20\n","17/17 [==============================] - 119s 7s/step - loss: 1.6775\n","training complete.\n"]}],"source":["history = model.fit(dataset, epochs=20,callbacks=[checkpoint_callback],verbose=1)\n","print(\"training complete.\")"]},{"cell_type":"markdown","metadata":{"id":"yvKkJZzr5zKt"},"source":["Training is now complete - and in principle we could directly use the model now to pass through a complete batch of input data and get the outputs. In practice though it can be handy to reload the model assume a batch input size of 1 only. This makes it easier for us to run one example at a time through the network. \n","\n","Therefore we first recreate our model -- but this time fixing the input layer batch size to one: "]},{"cell_type":"code","execution_count":59,"metadata":{"id":"sd89gm8v5zKt"},"outputs":[],"source":["model = tf.keras.Sequential()\n","model.add(tf.keras.layers.Embedding(len(vocab), embedding_dim,\n","                          batch_input_shape=[1, None]))\n","model.add(layers.SimpleRNN(rnn_units,return_sequences=True))\n","model.add(tf.keras.layers.Dense(len(vocab)))"]},{"cell_type":"markdown","metadata":{"id":"P1cFj1n05zKu"},"source":["We then need to load in our saved weights to this new model definition and complete construction manually by calling the build function. The build function needs to be called in this case as normally we would get the fit function to do this type of job for us. We won't be calling fit though, as we already have the training complete. "]},{"cell_type":"code","execution_count":61,"metadata":{"id":"qWV5pTC95zKu"},"outputs":[{"ename":"ValueError","evalue":"Received incompatible tensor with shape (8192,) when attempting to restore variable with shape (2048,) and name layer_with_weights-1/cell/bias/.ATTRIBUTES/VARIABLE_VALUE.","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[1;32mIn[61], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model\u001b[39m.\u001b[39;49mload_weights(tf\u001b[39m.\u001b[39;49mtrain\u001b[39m.\u001b[39;49mlatest_checkpoint(checkpoint_dir))\n\u001b[0;32m      2\u001b[0m model\u001b[39m.\u001b[39mbuild(tf\u001b[39m.\u001b[39mTensorShape([\u001b[39m1\u001b[39m, \u001b[39mNone\u001b[39;00m]))\n","File \u001b[1;32md:\\Projects\\repos\\tu060\\venv_dl\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n","File \u001b[1;32md:\\Projects\\repos\\tu060\\venv_dl\\lib\\site-packages\\tensorflow\\python\\training\\saving\\saveable_object_util.py:139\u001b[0m, in \u001b[0;36mResourceVariableSaveable.restore\u001b[1;34m(self, restored_tensors, restored_shapes)\u001b[0m\n\u001b[0;32m    136\u001b[0m   assigned_variable \u001b[39m=\u001b[39m resource_variable_ops\u001b[39m.\u001b[39mshape_safe_assign_variable_handle(\n\u001b[0;32m    137\u001b[0m       \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandle_op, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_var_shape, restored_tensor)\n\u001b[0;32m    138\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mValueError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m--> 139\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    140\u001b[0m       \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mReceived incompatible tensor with shape \u001b[39m\u001b[39m{\u001b[39;00mrestored_tensor\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    141\u001b[0m       \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mwhen attempting to restore variable with shape \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_var_shape\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    142\u001b[0m       \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mand name \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mname\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n\u001b[0;32m    143\u001b[0m \u001b[39mreturn\u001b[39;00m assigned_variable\n","\u001b[1;31mValueError\u001b[0m: Received incompatible tensor with shape (8192,) when attempting to restore variable with shape (2048,) and name layer_with_weights-1/cell/bias/.ATTRIBUTES/VARIABLE_VALUE."]}],"source":["model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n","model.build(tf.TensorShape([1, None]))"]},{"cell_type":"markdown","metadata":{"id":"0kkiEz2A5zKu"},"source":["The whole point of building the language model was so we could generate some new text. So first, let's define a function that can be used to generate such text. The function takes that trained language model as input and a chunk of text that is used to 'prime' the generator. In other words this parameter will be the first few characters of our generated text. \n","\n","Note that in generative models like this we don't tend to just generate the single most likely string. Instead it is useful to add a little bit of noise or chance into our production / generative processes. "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"d377hZ7E5zKu"},"outputs":[],"source":["def generate_text(model, start_string):\n","\n","    # Number of characters to generate\n","    num_generate = 100\n","\n","    # Converting our start string to numbers (vectorizing)\n","    input_eval = [char2idx[s] for s in start_string]\n","    input_eval = tf.expand_dims(input_eval, 0)\n","\n","    # Empty string to store our results\n","    text_generated = []\n","\n","    # Low temperatures results in more predictable text.\n","    # Higher temperatures results in more surprising text.\n","    # Experiment to find the best setting.\n","    temperature = 1.0\n","\n","    # Here batch size == 1\n","    model.reset_states()\n","    text_ids_gen = []\n","    for i in range(num_generate):\n","        predictions = model(input_eval)\n","        # remove the batch dimension\n","        predictions = tf.squeeze(predictions, 0)\n","\n","        # using a categorical distribution to predict the character returned by the model\n","        predictions = predictions / temperature\n","        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n","\n","        # We pass the predicted character as the next input to the model\n","        # along with the previous hidden state\n","        input_eval = tf.expand_dims([predicted_id], 0)\n","\n","        text_ids_gen.append(predicted_id)\n","        text_generated.append(idx2char[predicted_id])\n","\n","    return (start_string + ''.join(text_generated))"]},{"cell_type":"markdown","metadata":{"id":"dgecw5uG5zKu"},"source":["Put the trained model to use. "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"np2Yx6Yd5zKv"},"outputs":[],"source":["print(generate_text(model, start_string=u\"The \"))"]},{"cell_type":"markdown","metadata":{"id":"Iae5NRzg5zKv"},"source":["```\n","The rtave jun y s n, the:\n","Thu, d dy l madar l thes,\n","we!\n","AULI:\n","THers t,\n","Anoe mat'st'd h n chengheananok n\n","```\n","\n","\n","OK, so that looks a little bit like English, but it didn't work out that well in practice. \n","\n","One of the problems is that our model is good at learning the likelihood of characters given the recent data stream, but it isn't that good at learning to pay attention to further back information in complex ways. This problem is related to the challenge of backpropogation in general, but addressing it is an issue for another week. \n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"cOBdrkwZLOWw"},"source":["## Suggested Tasks\n","\n"," 1. Extend the example above to use a combination of filters of length 2, 3 and 4 as suggested on the source article http://www.wildml.com/2015/11/understanding-convolutional-neural-networks-for-nlp/\n"," 2. Use the pre-trained embeddings layers in place of an on-the-fly embeddings layer"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["A5y9xtQxD_Ob","AtLN5E2cD_Oj","VsIYZvnTKpIp"],"provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.0"}},"nbformat":4,"nbformat_minor":0}
