{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Addiction Simulator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Virtual Environment\n",
    "1. make sure you have a python virtual environment setup\n",
    "2. python -m pip install --upgrade pip setuptools virtualenv\n",
    "3. python -m venv venv (this creates a virtual environment called venv)\n",
    "4. add /venv/ to .gitignore.\n",
    "5. activate virtual environment with \\venv\\Scripts\\activate.bat on windows or source kivy_venv/bin/activate on mac+linux"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install Packages\n",
    "When inside python virtual enironment install the following packages:\n",
    "1. pip install numpy\n",
    "2. pip install matplotlib\n",
    "3. pip install kivy\n",
    "4. pip install torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#basic\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Kivy packages for UI\n",
    "from kivy.app import App\n",
    "from kivy.uix.widget import Widget\n",
    "from kivy.uix.button import Button\n",
    "from kivy.graphics import Color, Ellipse, Line\n",
    "from kivy.config import Config\n",
    "from kivy.properties import NumericProperty, ReferenceListProperty, ObjectProperty\n",
    "from kivy.vector import Vector\n",
    "from kivy.clock import Clock\n",
    "\n",
    "#pytorch for gpu processing of ML model\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.autograd as autograd\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Q-Learning Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):  \n",
    "    def __init__(self, input_size, nb_action):\n",
    "        super(Network, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.nb_action = nb_action\n",
    "        self.fc1 = nn.Linear(input_size, 30)\n",
    "        self.fc2 = nn.Linear(30, nb_action)\n",
    "    \n",
    "    def forward(self, state):\n",
    "        x = F.relu(self.fc1(state))\n",
    "        q_values = self.fc2(x)\n",
    "        return q_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experience Replay Model\n",
    "This model is used for training our DQN model. It stores the transitions that the agent observes, allowing us to reuse this data later. By sampling from it randomly, the transitions that build up a batch are decorrelated. It has been shown that this greatly stabilizes and improves the DQN training procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory(): \n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "    \n",
    "    def push(self, event):\n",
    "        self.memory.append(event)\n",
    "        if len(self.memory) > self.capacity:\n",
    "            del self.memory[0]\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        samples = zip(*random.sample(self.memory, batch_size))\n",
    "        return map(lambda x: Variable(torch.cat(x, 0)), samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DQN Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dqn():\n",
    "    def __init__(self, input_size, nb_action, gamma):\n",
    "        self.gamma = gamma\n",
    "        self.reward_window = []\n",
    "        self.model = Network(input_size, nb_action)\n",
    "        self.memory = ReplayMemory(100000)\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr = 0.001)\n",
    "        self.last_state = torch.Tensor(input_size).unsqueeze(0)\n",
    "        self.last_action = 0\n",
    "        self.last_reward = 0\n",
    "    \n",
    "    def select_action(self, state):\n",
    "        probs = F.softmax(self.model(Variable(state, volatile = True))*100) # T=100\n",
    "        action = probs.multinomial(num_samples=1)\n",
    "        return action.data[0,0]\n",
    "    \n",
    "    def learn(self, batch_state, batch_next_state, batch_reward, batch_action):\n",
    "        outputs = self.model(batch_state).gather(1, batch_action.unsqueeze(1)).squeeze(1)\n",
    "        next_outputs = self.model(batch_next_state).detach().max(1)[0]\n",
    "        target = self.gamma*next_outputs + batch_reward\n",
    "        td_loss = F.smooth_l1_loss(outputs, target)\n",
    "        self.optimizer.zero_grad()\n",
    "        td_loss.backward(retain_graph = True)\n",
    "        self.optimizer.step()\n",
    "    \n",
    "    def update(self, reward, new_signal):\n",
    "        new_state = torch.Tensor(new_signal).float().unsqueeze(0)\n",
    "        self.memory.push((self.last_state, new_state, torch.LongTensor([int(self.last_action)]), torch.Tensor([self.last_reward])))\n",
    "        action = self.select_action(new_state)\n",
    "        if len(self.memory.memory) > 100:\n",
    "            batch_state, batch_next_state, batch_action, batch_reward = self.memory.sample(100)\n",
    "            self.learn(batch_state, batch_next_state, batch_reward, batch_action)\n",
    "        self.last_action = action\n",
    "        self.last_state = new_state\n",
    "        self.last_reward = reward\n",
    "        self.reward_window.append(reward)\n",
    "        if len(self.reward_window) > 1000:\n",
    "            del self.reward_window[0]\n",
    "        return action\n",
    "    \n",
    "    def score(self):\n",
    "        return sum(self.reward_window)/(len(self.reward_window)+1.)\n",
    "    \n",
    "    def save(self):\n",
    "        torch.save({'state_dict': self.model.state_dict(),\n",
    "                    'optimizer' : self.optimizer.state_dict(),\n",
    "                   }, 'last_brain.pth')\n",
    "    def load(self):\n",
    "        if os.path.isfile('last_brain.pth'):\n",
    "            print(\"=> loading checkpoint... \")\n",
    "            checkpoint = torch.load('last_brain.pth')\n",
    "            self.model.load_state_dict(checkpoint['state_dict'])\n",
    "            self.optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "            print(\"done !\")\n",
    "        else:\n",
    "            print(\"no checkpoint found...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Settings & Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding this line if we don't want the right click to put a red point\n",
    "Config.set('input', 'mouse', 'mouse,multitouch_on_demand')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Introducing last_x and last_y, used to keep the last point \n",
    "# in memory when we draw the sand on the map\n",
    "last_x = 0\n",
    "last_y = 0\n",
    "n_points = 0 # the total number of points in the last drawing\n",
    "length = 0 # the length of the last drawing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting our AI, which we call \"brain\", and that contains \n",
    "# our neural network that represents our Q-function\n",
    "brain = Dqn(5,3,0.9) # 5 sensors, 3 actions, gama = 0.9\n",
    "\n",
    "# actions are:\n",
    "# action = 0 => no rotation, \n",
    "# action = 1 => rotate 20 degres, \n",
    "# action = 2 => rotate -20 degres\n",
    "action2rotation = [0,20,-20]\n",
    "\n",
    "# initializing the last reward\n",
    "last_reward = 0\n",
    "# initializing the mean score curve (sliding window of the rewards) with \n",
    "# respect to time\n",
    "scores = []\n",
    "\n",
    "first_update = True # using this trick to initialize the map only once\n",
    "\n",
    "last_distance = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the game class (to understand \"ObjectProperty\", see kivy tutorials: kivy https://kivy.org/docs/tutorials/pong.html)\n",
    "class Game(Widget):\n",
    "\n",
    "    car = ObjectProperty(None) # getting the car object from our kivy file\n",
    "    ball1 = ObjectProperty(None) # getting the sensor 1 object from our kivy file\n",
    "    ball2 = ObjectProperty(None) # getting the sensor 2 object from our kivy file\n",
    "    ball3 = ObjectProperty(None) # getting the sensor 3 object from our kivy file\n",
    "\n",
    "    def initialiseMap(self):\n",
    "        global sand # sand is an array that has as many cells as our graphic interface has pixels. Each cell has a one if there is sand, 0 otherwise.\n",
    "        global goal_x # x-coordinate of the goal (where the car has to go, that is the airport or the downtown)\n",
    "        global goal_y # y-coordinate of the goal (where the car has to go, that is the airport or the downtown)\n",
    "        global first_update\n",
    "        sand = np.zeros((longueur,largeur)) # initializing the sand array with only zeros\n",
    "        goal_x = 20 # the goal to reach is at the upper left of the map (the x-coordinate is 20 and not 0 because the car gets bad reward if it touches the wall)\n",
    "        goal_y = largeur - 20 # the goal to reach is at the upper left of the map (y-coordinate)\n",
    "        first_update = False # trick to initialize the map only once\n",
    "    def serve_car(self): # starting the car when we launch the application\n",
    "        self.car.center = self.center # the car will start at the center of the map\n",
    "        self.car.velocity = Vector(6, 0) # the car will start to go horizontally to the right with a speed of 6\n",
    "\n",
    "    def update(self, dt): # the big update function that updates everything that needs to be updated at each discrete time t when reaching a new state (getting new signals from the sensors)\n",
    "\n",
    "        global brain # specifying the global variables (the brain of the car, that is our AI)\n",
    "        global last_reward # specifying the global variables (the last reward)\n",
    "        global scores # specifying the global variables (the means of the rewards)\n",
    "        global last_distance # specifying the global variables (the last distance from the car to the goal)\n",
    "        global goal_x # specifying the global variables (x-coordinate of the goal)\n",
    "        global goal_y # specifying the global variables (y-coordinate of the goal)\n",
    "        global longueur # specifying the global variables (width of the map)\n",
    "        global largeur # specifying the global variables (height of the map)\n",
    "\n",
    "        longueur = self.width # width of the map (horizontal edge)\n",
    "        largeur = self.height # height of the map (vertical edge)\n",
    "        if first_update: # trick to initialise the map only once\n",
    "            self.initialiseMap()\n",
    "\n",
    "        xx = goal_x - self.car.x # difference of x-coordinates between the goal and the car\n",
    "        yy = goal_y - self.car.y # difference of y-coordinates between the goal and the car\n",
    "        orientation = Vector(*self.car.velocity).angle((xx,yy))/180. # direction of the car with respect to the goal (if the car is heading perfectly towards the goal, then orientation = 0)\n",
    "        last_signal = [self.car.signal1, self.car.signal2, self.car.signal3, orientation, -orientation] # our input state vector, composed of the three signals received by the three sensors, plus the orientation and -orientation\n",
    "        action = brain.update(last_reward, last_signal) # playing the action from our ai (the object brain of the dqn class)\n",
    "        scores.append(brain.score()) # appending the score (mean of the last 100 rewards to the reward window)\n",
    "        rotation = action2rotation[action] # converting the action played (0, 1 or 2) into the rotation angle (0°, 20° or -20°)\n",
    "        self.car.move(rotation) # moving the car according to this last rotation angle\n",
    "        distance = np.sqrt((self.car.x - goal_x)**2 + (self.car.y - goal_y)**2) # getting the new distance between the car and the goal right after the car moved\n",
    "        self.ball1.pos = self.car.sensor1 # updating the position of the first sensor (ball1) right after the car moved\n",
    "        self.ball2.pos = self.car.sensor2 # updating the position of the second sensor (ball2) right after the car moved\n",
    "        self.ball3.pos = self.car.sensor3 # updating the position of the third sensor (ball3) right after the car moved\n",
    "\n",
    "        if sand[int(self.car.x),int(self.car.y)] > 0: # if the car is on the sand\n",
    "            self.car.velocity = Vector(1, 0).rotate(self.car.angle) # it is slowed down (speed = 1)\n",
    "            last_reward = -1 # and reward = -1\n",
    "        else: # otherwise\n",
    "            self.car.velocity = Vector(6, 0).rotate(self.car.angle) # it goes to a normal speed (speed = 6)\n",
    "            last_reward = -0.2 # and it gets bad reward (-0.2)\n",
    "            if distance < last_distance: # however if it getting close to the goal\n",
    "                last_reward = 0.1 # it still gets slightly positive reward 0.1\n",
    "\n",
    "        if self.car.x < 10: # if the car is in the left edge of the frame\n",
    "            self.car.x = 10 # it is not slowed down\n",
    "            last_reward = -1 # but it gets bad reward -1\n",
    "        if self.car.x > self.width-10: # if the car is in the right edge of the frame\n",
    "            self.car.x = self.width-10 # it is not slowed down\n",
    "            last_reward = -1 # but it gets bad reward -1\n",
    "        if self.car.y < 10: # if the car is in the bottom edge of the frame\n",
    "            self.car.y = 10 # it is not slowed down\n",
    "            last_reward = -1 # but it gets bad reward -1\n",
    "        if self.car.y > self.height-10: # if the car is in the upper edge of the frame\n",
    "            self.car.y = self.height-10 # it is not slowed down\n",
    "            last_reward = -1 # but it gets bad reward -1\n",
    "\n",
    "        if distance < 100: # when the car reaches its goal\n",
    "            goal_x = self.width - goal_x # the goal becomes the bottom right corner of the map (the downtown), and vice versa (updating of the x-coordinate of the goal)\n",
    "            goal_y = self.height - goal_y # the goal becomes the bottom right corner of the map (the downtown), and vice versa (updating of the y-coordinate of the goal)\n",
    "\n",
    "        # Updating the last distance from the car to the goal\n",
    "        last_distance = distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent Body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the car class (to understand \"NumericProperty\" and \n",
    "# \"ReferenceListProperty\", \n",
    "# see kivy tutorials: https://kivy.org/docs/tutorials/pong.html)\n",
    "class Car(Widget):\n",
    "\n",
    "    angle = NumericProperty(0) # initializing the angle of the car (angle between the x-axis of the map and the axis of the car)\n",
    "    rotation = NumericProperty(0) # initializing the last rotation of the car (after playing the action, the car does a rotation of 0, 20 or -20 degrees)\n",
    "    velocity_x = NumericProperty(0) # initializing the x-coordinate of the velocity vector\n",
    "    velocity_y = NumericProperty(0) # initializing the y-coordinate of the velocity vector\n",
    "    velocity = ReferenceListProperty(velocity_x, velocity_y) # velocity vector\n",
    "    sensor1_x = NumericProperty(0) # initializing the x-coordinate of the first sensor (the one that looks forward)\n",
    "    sensor1_y = NumericProperty(0) # initializing the y-coordinate of the first sensor (the one that looks forward)\n",
    "    sensor1 = ReferenceListProperty(sensor1_x, sensor1_y) # first sensor vector\n",
    "    sensor2_x = NumericProperty(0) # initializing the x-coordinate of the second sensor (the one that looks 30 degrees to the left)\n",
    "    sensor2_y = NumericProperty(0) # initializing the y-coordinate of the second sensor (the one that looks 30 degrees to the left)\n",
    "    sensor2 = ReferenceListProperty(sensor2_x, sensor2_y) # second sensor vector\n",
    "    sensor3_x = NumericProperty(0) # initializing the x-coordinate of the third sensor (the one that looks 30 degrees to the right)\n",
    "    sensor3_y = NumericProperty(0) # initializing the y-coordinate of the third sensor (the one that looks 30 degrees to the right)\n",
    "    sensor3 = ReferenceListProperty(sensor3_x, sensor3_y) # third sensor vector\n",
    "    signal1 = NumericProperty(0) # initializing the signal received by sensor 1\n",
    "    signal2 = NumericProperty(0) # initializing the signal received by sensor 2\n",
    "    signal3 = NumericProperty(0) # initializing the signal received by sensor 3\n",
    "\n",
    "    def move(self, rotation):\n",
    "        self.pos = Vector(*self.velocity) + self.pos # updating the position of the car according to its last position and velocity\n",
    "        self.rotation = rotation # getting the rotation of the car\n",
    "        self.angle = self.angle + self.rotation # updating the angle\n",
    "        self.sensor1 = Vector(30, 0).rotate(self.angle) + self.pos # updating the position of sensor 1\n",
    "        self.sensor2 = Vector(30, 0).rotate((self.angle+30)%360) + self.pos # updating the position of sensor 2\n",
    "        self.sensor3 = Vector(30, 0).rotate((self.angle-30)%360) + self.pos # updating the position of sensor 3\n",
    "        self.signal1 = int(np.sum(sand[int(self.sensor1_x)-10:int(self.sensor1_x)+10, int(self.sensor1_y)-10:int(self.sensor1_y)+10]))/400. # getting the signal received by sensor 1 (density of sand around sensor 1)\n",
    "        self.signal2 = int(np.sum(sand[int(self.sensor2_x)-10:int(self.sensor2_x)+10, int(self.sensor2_y)-10:int(self.sensor2_y)+10]))/400. # getting the signal received by sensor 2 (density of sand around sensor 2)\n",
    "        self.signal3 = int(np.sum(sand[int(self.sensor3_x)-10:int(self.sensor3_x)+10, int(self.sensor3_y)-10:int(self.sensor3_y)+10]))/400. # getting the signal received by sensor 3 (density of sand around sensor 3)\n",
    "        if self.sensor1_x > longueur-10 or self.sensor1_x<10 or self.sensor1_y>largeur-10 or self.sensor1_y<10: # if sensor 1 is out of the map (the car is facing one edge of the map)\n",
    "            self.signal1 = 1. # sensor 1 detects full sand\n",
    "        if self.sensor2_x > longueur-10 or self.sensor2_x<10 or self.sensor2_y>largeur-10 or self.sensor2_y<10: # if sensor 2 is out of the map (the car is facing one edge of the map)\n",
    "            self.signal2 = 1. # sensor 2 detects full sand\n",
    "        if self.sensor3_x > longueur-10 or self.sensor3_x<10 or self.sensor3_y>largeur-10 or self.sensor3_y<10: # if sensor 3 is out of the map (the car is facing one edge of the map)\n",
    "            self.signal3 = 1. # sensor 3 detects full sand\n",
    "\n",
    "class Ball1(Widget): # sensor 1 (see kivy tutorials: kivy https://kivy.org/docs/tutorials/pong.html)\n",
    "    pass\n",
    "class Ball2(Widget): # sensor 2 (see kivy tutorials: kivy https://kivy.org/docs/tutorials/pong.html)\n",
    "    pass\n",
    "class Ball3(Widget): # sensor 3 (see kivy tutorials: kivy https://kivy.org/docs/tutorials/pong.html)\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent Training Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Painting for graphic interface (see kivy tutorials: https://kivy.org/docs/tutorials/firstwidget.html)\n",
    "class MyPaintWidget(Widget):\n",
    "\n",
    "    def on_touch_down(self, touch): # putting some sand when we do a left click\n",
    "        global length,n_points,last_x,last_y\n",
    "        with self.canvas:\n",
    "            Color(0.8,0.7,0)\n",
    "            d=10.\n",
    "            touch.ud['line'] = Line(points = (touch.x, touch.y), width = 10)\n",
    "            last_x = int(touch.x)\n",
    "            last_y = int(touch.y)\n",
    "            n_points = 0\n",
    "            length = 0\n",
    "            sand[int(touch.x),int(touch.y)] = 1\n",
    "\n",
    "    def on_touch_move(self, touch): # putting some sand when we move the mouse while pressing left\n",
    "        global length,n_points,last_x,last_y\n",
    "        if touch.button=='left':\n",
    "            touch.ud['line'].points += [touch.x, touch.y]\n",
    "            x = int(touch.x)\n",
    "            y = int(touch.y)\n",
    "            length += np.sqrt(max((x - last_x)**2 + (y - last_y)**2, 2))\n",
    "            n_points += 1.\n",
    "            density = n_points/(length)\n",
    "            touch.ud['line'].width = int(20*density + 1)\n",
    "            sand[int(touch.x) - 10 : int(touch.x) + 10, int(touch.y) - 10 : int(touch.y) + 10] = 1\n",
    "            last_x = x\n",
    "            last_y = y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UI Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CarApp(App):\n",
    "    def build(self): # building the app\n",
    "        parent = Game()\n",
    "        parent.serve_car()\n",
    "        Clock.schedule_interval(parent.update, 1.0 / 60.0)\n",
    "        self.painter = MyPaintWidget()\n",
    "        clearbtn = Button(text='clear')\n",
    "        savebtn = Button(text='save',pos=(parent.width,0))\n",
    "        loadbtn = Button(text='load',pos=(2*parent.width,0))\n",
    "        clearbtn.bind(on_release=self.clear_canvas)\n",
    "        savebtn.bind(on_release=self.save)\n",
    "        loadbtn.bind(on_release=self.load)\n",
    "        parent.add_widget(self.painter)\n",
    "        parent.add_widget(clearbtn)\n",
    "        parent.add_widget(savebtn)\n",
    "        parent.add_widget(loadbtn)\n",
    "        return parent\n",
    "\n",
    "    def clear_canvas(self, obj): # clear button\n",
    "        global sand\n",
    "        self.painter.canvas.clear()\n",
    "        sand = np.zeros((longueur,largeur))\n",
    "\n",
    "    def save(self, obj): # save button\n",
    "        print(\"saving brain...\")\n",
    "        brain.save()\n",
    "        plt.title(\"Scores\")\n",
    "        plt.plot(scores)\n",
    "        plt.show()\n",
    "\n",
    "    def load(self, obj): # load button\n",
    "        print(\"loading last saved brain...\")\n",
    "        brain.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "CarApp().run()#closing application will register as a crash, ignore."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "63fc9bad62e87c8f4520331cf897eeae3cca7d7504ec2bc20d709f20b08251da"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
