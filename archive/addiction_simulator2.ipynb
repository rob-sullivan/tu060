{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Addiction Simulator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Virtual Environment\n",
    "1. make sure python3 is installed on your system.\n",
    "2. make sure you have a python virtual environment setup: \n",
    "python -m pip install --upgrade pip setuptools virtualenv\n",
    "3. create a python environment: \n",
    "python -m venv venv (this creates a virtual environment called venv)\n",
    "4. if applicable add /venv/ to your .gitignore.\n",
    "5. activate the virtual environment: \n",
    "either \\venv\\Scripts\\activate.bat on windows \n",
    "or source venv/bin/activate on mac+linux\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#basic\n",
    "from os import system, name\n",
    "import random\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#pytorch for gpu processing of ML model\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.autograd as autograd\n",
    "from torch.autograd import Variable\n",
    "\n",
    "#rich library for Terminal UI\n",
    "from rich.jupyter import print\n",
    "#from rich import print\n",
    "from rich.prompt import IntPrompt\n",
    "\n",
    "\n",
    "#hide pytorch warnings (should eventually be resolved)\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent's Brain (Deep Q-Learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):  \n",
    "    def __init__(self, input_size, nb_action):\n",
    "        #ref: https://discuss.pytorch.org/t/super-model-in-init/97426\n",
    "        #super(Network, self).__init__()\n",
    "        super().__init__() #pytorch's NN model\n",
    "        self.input_size = input_size\n",
    "        self.nb_action = nb_action\n",
    "        self.fc1 = nn.Linear(input_size, 30)#arbitrarily chose 30 hidden layers\n",
    "        self.fc2 = nn.Linear(30, nb_action)\n",
    "    \n",
    "    #base pytorch NN model runs and we override the\n",
    "    #forward function with our own relu activation function\n",
    "    def forward(self, state):\n",
    "        x = F.relu(self.fc1(state))\n",
    "        q_values = self.fc2(x)\n",
    "        return q_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experience Replay Model\n",
    "This model is used for training our DQN model. It stores the transitions that the agent observes, allowing us to reuse this data later. By sampling from it randomly, the transitions that build up a batch are decorrelated. It has been shown that this greatly stabilizes and improves the DQN training procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory(): \n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "    \n",
    "    def push(self, event):\n",
    "        self.memory.append(event)\n",
    "        if len(self.memory) > self.capacity:\n",
    "            del self.memory[0]\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        samples = zip(*random.sample(self.memory, batch_size))\n",
    "        return map(lambda x: Variable(torch.cat(x, 0)), samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DQN Ensemble\n",
    "Comprised of a neural network model and a memory model. \n",
    "* The NN takes in observation of sensor data (brain chemicals) and chooses actions based on the relu activation function. \n",
    "* The agent will sample some of the sensor data and store in long term memory to be reused later for training. \n",
    "* We also use the Adam Optimisation algorithm. This is an extension to stocastic gradient desent to update weights of the neural network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dqn():\n",
    "    def __init__(self, input_size, nb_action, gamma):\n",
    "        self.gamma = gamma\n",
    "        self.reward_window = []\n",
    "        self.model = Network(input_size, nb_action)\n",
    "        self.memory = ReplayMemory(100000)\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr = 0.001)\n",
    "        self.last_state = torch.Tensor(input_size).unsqueeze(0)\n",
    "        self.last_action = 0\n",
    "        self.last_reward = 0\n",
    "    \n",
    "    # select action for x duration\n",
    "    def select_action(self, state):\n",
    "        #softmax converts numbers into probabilities\n",
    "        #Q values are the output of the neural network\n",
    "        # Temperature value = 100. closer to zero the less sure the NN will be to taking the action\n",
    "        probs = F.softmax(self.model(Variable(state, volatile = True))*100) # T=100\n",
    "        #viz q value for each action, (T value by user choice)\n",
    "        #pie chart 0/1 #seperate action\n",
    "        action = probs.multinomial(num_samples=1)\n",
    "        return action.data[0,0]\n",
    "    \n",
    "    #to train our AI\n",
    "    #forward propagation then backproagation\n",
    "    # get our output, target, compare our output to the target to compute the loss error\n",
    "    # backproagate loss error into the nn and use stochastic gradient descent we update the weights according to how much they contributed to the loss error\n",
    "    def learn(self, batch_state, batch_next_state, batch_reward, batch_action):\n",
    "        outputs = self.model(batch_state).gather(1, batch_action.unsqueeze(1)).squeeze(1)\n",
    "        next_outputs = self.model(batch_next_state).detach().max(1)[0]\n",
    "        target = self.gamma*next_outputs + batch_reward\n",
    "        td_loss = F.smooth_l1_loss(outputs, target)\n",
    "        self.optimizer.zero_grad()\n",
    "        td_loss.backward(retain_graph = True)\n",
    "        self.optimizer.step()\n",
    "    \n",
    "    #When ai reaches a new state we update everything\n",
    "    #update action, last action becomes the new action but also the last state becomes the new state and last reward becomes the new state\n",
    "    # we then get this new transition and update our reward window to track training progress and exploration\n",
    "    def update(self, reward, new_signal):\n",
    "        new_state = torch.Tensor(new_signal).float().unsqueeze(0)\n",
    "        self.memory.push((self.last_state, new_state, torch.LongTensor([int(self.last_action)]), torch.Tensor([self.last_reward])))\n",
    "        action = self.select_action(new_state)\n",
    "        if len(self.memory.memory) > 100:\n",
    "            batch_state, batch_next_state, batch_action, batch_reward = self.memory.sample(100)\n",
    "            self.learn(batch_state, batch_next_state, batch_reward, batch_action)\n",
    "        self.last_action = action\n",
    "        self.last_state = new_state\n",
    "        self.last_reward = reward\n",
    "        self.reward_window.append(reward)\n",
    "        if len(self.reward_window) > 1000:\n",
    "            del self.reward_window[0]\n",
    "        return action\n",
    "    \n",
    "    def score(self):\n",
    "        return sum(self.reward_window)/(len(self.reward_window)+1.)\n",
    "    \n",
    "    def save(self):\n",
    "        torch.save({'state_dict': self.model.state_dict(),\n",
    "                    'optimizer' : self.optimizer.state_dict(),\n",
    "                   }, 'last_brain.pth')\n",
    "    def load(self):\n",
    "        if os.path.isfile('last_brain.pth'):\n",
    "            print(\"=> loading checkpoint... \")\n",
    "            checkpoint = torch.load('last_brain.pth')\n",
    "            self.model.load_state_dict(checkpoint['state_dict'])\n",
    "            self.optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "            print(\"done !\")\n",
    "        else:\n",
    "            print(\"no checkpoint found...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Simulation():    \n",
    "    def __init__(self):\n",
    "        # Agent Brain - a neural network that represents our Q-function\n",
    "        self.agent = Dqn(4,8,0.9) # 4 sensors, 8 actions, gama = 0.9\n",
    "        #Agent's Actions\n",
    "        self.actions = ['Sleep', 'Binge on Internet', 'Work', 'Exercise', 'Socialise', 'Drink Alcohol', 'Smoke', 'Take Cocaine'] #8 actions\n",
    "        \n",
    "        # Mehdi Keramati's definition of an animal ref Mehdi Keramati's Homeostatic RL sim\n",
    "        ## fatigue or lever/action cost\n",
    "        self.fatigue = 1\n",
    "        self.outcome = [0,10,5,12.5,12.5,10,25,50] #Keramati's outcome, e.g cocaine = 50, representing the dose of self-administered drug,\n",
    "        self.outcomeBuffer = 0\n",
    "\n",
    "        ## Homeostatic System\n",
    "        self.initialInState = 0\n",
    "        self.initialSetpoint = 200\n",
    "        self.inStateLowerBound = 0\n",
    "        self.outcomeDegradationRate = 0.007 # e.g dose of cocaine that the animal loses in every time-step\n",
    "        self.outcomeAbsorptionRatio = 0.12 # e.g proportion of the injected cocaine that affects the brain right after infusion\n",
    "\n",
    "        ## Allostatic (Stress) System\n",
    "        self.setpointShiftRate = 0.0018\n",
    "        self.setpointRecoveryRate = 0.00016\n",
    "        self.optimalInStateLowerBound = 100\n",
    "        self.optimalInStateUpperBound = 200\n",
    "\n",
    "        ## Drive Function\n",
    "        self.m = 3 # Parameter of the drive function : m-th root\n",
    "        self.n = 4 # Parameter of the drive function : n-th pawer\n",
    "\n",
    "        ## Goal-directed system\n",
    "        self.updateRewardRate = 0.2  # Learning rate for updating the non-homeostatic reward function\n",
    "        self.updateOutcomeRate = 0.2  # Learning rate for updating the outcome function\n",
    "        \n",
    "        #agent's sensors or observation space\n",
    "        self.current_state = 0 #current state agent is in (external_state)\n",
    "        self.internal_state = float(self.initialInState) #internal variable that moves homeostatic setpoint\n",
    "        self.setpoint_S = float(self.initialSetpoint) #homeostatic setpoint\n",
    "\n",
    "        \n",
    "        # the mean score curve (sliding window of the rewards) with \n",
    "        # respect to time.\n",
    "        self.scores = []\n",
    "\n",
    "        #the agent's environment is elapsing time\n",
    "        self.end_day = 364 # day simulation ends\n",
    "        self.end_hour = 24 # hour simulation ends\n",
    "        self.total_time = (self.end_day*24+self.end_hour) # total time in hours (size of the world) e.g 1 year or 8760hrs\n",
    "        self.total_epochs = (self.total_time*3600)/4 #e.g 20 secs is 5 trials or epochs of time or 1 hour = 3600secs/4secs = 900 epochs, 8760hrs is 7,884,000 epochs\n",
    "\n",
    "        self.current_day = 1 # day agent starts\n",
    "        self.current_hour = 1 # hour agent starts\n",
    "        self.elapsed_time = float(0.0) #amount of time that has passed\n",
    "\n",
    "        # reward agent wants to maximise this will be the homeostatic reward\n",
    "        self.reward_received = 0\n",
    "\n",
    "\n",
    "        self.finish = False #trigger to end simulation\n",
    "        while not self.finish:\n",
    "            self.finish = self.next_time_interval()# begin simulation\n",
    "\n",
    "    def next_time_interval(self):\n",
    "        ## agent input state vector, composed of the five brain signals or observations received from being in the environment\n",
    "        current_state = [self.current_state, self.internal_state, self.setpoint_S, self.elapsed_time]\n",
    "        action_to_take = self.agent.update(self.reward_received, current_state) # playing the action from the ai (dqn class)\n",
    "        self.scores.append(self.agent.score()) # appending the score (mean of the last 100 rewards to the reward window)\n",
    "\n",
    "        #This is the limbic system to say what action to take.\n",
    "        #we can take the agent's NN and call forward to output q values for each state.\n",
    "        suggested_action = self.actions[action_to_take]\n",
    "        sa = int(0 if \"Sleep\" else 1 if suggested_action==\"Binge on Video Games\" else 2 if suggested_action==\"Work\" else 3 if suggested_action==\"Exercise\" else 4 if suggested_action==\"Socialise\" else 5 if suggested_action==\"Drink Alcohol\" else 6 if suggested_action==\"Smoke\" else 7 if suggested_action==\"Take Cocaine\" else -1)\n",
    "        \n",
    "        #give user options\n",
    "        self.clear()\n",
    "        print(\"** Current Day: [bold dark_violet]\" + str(self.current_day) + \"[/bold dark_violet], Current Hour: [bold dark_violet]\" + str(self.current_hour) + \"[/bold dark_violet], Time Left: [bold dark_violet]\" + str(time_left) + \"[/bold dark_violet] hrs ** \\n\")\n",
    "        print(\"- [bold dark_green]Current State: \" + str(self.current_state) + \"[/bold dark_green]\")\n",
    "        print(\"- [bold dark_green]Current Homeostatic Variable: \" + str(self.internal_state) + \"[/bold dark_green]\")\n",
    "        print(\"- [bold dark_green]Current Homeostatic Setpoint: \" + str(self.setpoint_S) + \"[/bold dark_green]\")\n",
    "        print(\"- [bold dark_green]Time Elapsed: \" + str(self.elapsed_time) + \"[/bold dark_green]\")\n",
    "        print(\"0. [bold dark_violet]Sleep[/bold dark_violet]\\n1. [bold dark_violet]Binge on Internet[/bold dark_violet]\\n2. [bold dark_violet]Work[/bold dark_violet]\\n3. [bold dark_violet]Exercise[/bold dark_violet]\\n4. [bold dark_violet]Socialise[/bold dark_violet]\\n5. [bold dark_violet]Drink Alcohol[/bold dark_violet]\\n6. [bold dark_violet]Smoke[/bold dark_violet]\\n\")\n",
    "        \n",
    "        action_taken = 0\n",
    "        action_taken = IntPrompt.ask(\"Choose from 1 to 6\", default=sa)\n",
    "\n",
    "        #update agent brain chemicals after action taken\n",
    "        if(action_taken == 0): #Sleep\n",
    "            #1. update internal sensors or observations of the environment\n",
    "            #self.current_state = next_state\n",
    "\n",
    "            ## Update internal state upon consumption  \n",
    "            interS = self.internal_state + (self.outcome[action_taken] * self.outcomeAbsorptionRatio) - self.outcomeDegradationRate * (self.internal_state - self.inStateLowerBound)\n",
    "            if interS < self.inStateLowerBound:\n",
    "                interS = self.inStateLowerBound    \n",
    "            self.internal_state = interS\n",
    "\n",
    "            ## Update homeostatic setpoint\n",
    "            optInS = self.setpoint_S + self.outcome[action_taken]  * self.setpointShiftRate - self.setpointRecoveryRate\n",
    "            if optInS < self.optimalInStateLowerBound:\n",
    "                optInS = self.optimalInStateLowerBound\n",
    "            if optInS > self.optimalInStateUpperBound:\n",
    "                optInS = self.optimalInStateUpperBound\n",
    "            self.setpoint_S = optInS\n",
    "\n",
    "        elif(action_taken != 0 or action_taken != -1): #Binge on Internet, Work, Exercise, Socialise, Drink Alcohol, Smoke\n",
    "            #1. determine the next state that the agent fell into from taking action\n",
    "            next_state = 0\n",
    "            index = np.random.uniform(0,1)\n",
    "            probSum = 0\n",
    "            for nextS in self.actions:\n",
    "                probSum = probSum + (self.transition_environment[self.current_state][action_taken][nextS])\n",
    "                if index <= probSum:\n",
    "                    next_state = nextS\n",
    "                    break\n",
    "            \n",
    "            #2. get Non-Homeostatic reward e.g energy cost or fatigue of doing an action\n",
    "            if action_taken != 0: #if the agent is not sleeping then it is doing something that costs energy\n",
    "                nonHomeoRew = -self.fatigue\n",
    "            self.reward_received +=  (1.0 - self.updateRewardRate) * self.reward_received + self.updateRewardRate * nonHomeoRew\n",
    "\n",
    "            #3. get Homeostatically-regulated Reward of doing action (drive reduction)\n",
    "            d1 = math.pow(math.fabs(math.pow(self.setpoint_S - self.internal_state, self.n*1.0)),(1.0/self.m))\n",
    "            d2 = math.pow(math.fabs(math.pow(self.setpoint_S - self.internal_state - self.outcome[action_taken], self.n*1.0)),(1.0/self.m))\n",
    "            HomeoRew = d1 - d2\n",
    "            self.reward_received +=  (1.0 - self.updateOutcomeRate) * self.reward_received + self.updateOutcomeRate * HomeoRew\n",
    "            \n",
    "            #4. update estimated next state\n",
    "            # n/a\n",
    "            \n",
    "            #5. update internal sensors or observations of the environment\n",
    "            self.current_state = next_state\n",
    "\n",
    "            ## Update internal state upon consumption\n",
    "            self.outcomeBuffer = self.outcomeBuffer + self.outcome[action_taken]   \n",
    "            interS = self.internal_state + (self.outcomeBuffer * self.outcomeAbsorptionRatio) - self.outcomeDegradationRate * (self.internal_state - self.inStateLowerBound)\n",
    "            if interS < self.inStateLowerBound:\n",
    "                interS = self.inStateLowerBound    \n",
    "            self.internal_state = interS\n",
    "\n",
    "            ## Update homeostatic setpoint\n",
    "            optInS = self.setpoint_S + self.outcome[action_taken]  * self.setpointShiftRate - self.setpointRecoveryRate\n",
    "            if optInS < self.optimalInStateLowerBound:\n",
    "                optInS = self.optimalInStateLowerBound\n",
    "            if optInS > self.optimalInStateUpperBound:\n",
    "                optInS = self.optimalInStateUpperBound\n",
    "            self.setpoint_S = optInS \n",
    "\n",
    "            ## Update outcome buffer\n",
    "            self.outcomeBuffer = self.outcomeBuffer * (1 - self.outcomeAbsorptionRatio)\n",
    "\n",
    "        elif(action_taken == -1):#quit\n",
    "            #print(\"saving brain...\")\n",
    "            #brain.save()\n",
    "            plt.title(\"Scores\")\n",
    "            plt.xlabel(\"Epochs\")\n",
    "            plt.ylabel(\"Reward\")\n",
    "            plt.plot(self.scores)\n",
    "            plt.show()\n",
    "            return True   #end simulation\n",
    "\n",
    "        #check if this is the last round otherwise continue\n",
    "        ## get the time left\n",
    "        self.elapsed_time = self.elapsed_time + (self.current_day * 23 + self.current_hour) #e.g elapsed time in hours 23hrs + 1hr = 24hrs or 1 day has passed\n",
    "        time_left = (self.end_day*24 + self.end_hour) - self.elapsed_time #time left in hours, e.g if 24hrs passed then 8,736hrs\n",
    "\n",
    "        if(time_left <= 0):\n",
    "            return True   #end simulation\n",
    "        else:    \n",
    "            # Updating the last time from the agent to the end time (goal)\n",
    "            self.current_day += 1  #update to next day interval\n",
    "            if(self.current_day > self.end_day):\n",
    "                self.current_day = self.end_day\n",
    "            self.current_hour += 1 #update to next hour interval\n",
    "            if(self.current_hour > self.end_hour):\n",
    "                self.current_hour = 0\n",
    "            return False\n",
    "\n",
    "    def clear(self): \n",
    "        \"\"\"\n",
    "        This function was taken from https://www.geeksforgeeks.org/clear-screen-python/ to\n",
    "        allow the terminal to be cleared when changing menus or showing the user important\n",
    "        messages. It checks what operating system is being used and uses the correct \n",
    "        clearing command.\n",
    "        \"\"\"\n",
    "        # for windows \n",
    "        if name == 'nt': \n",
    "            _ = system('cls') \n",
    "\n",
    "        # for mac and linux(here, os.name is 'posix')\n",
    "        else: \n",
    "            _ = system('clear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">** Current Day: <span style=\"color: #af00d7; text-decoration-color: #af00d7; font-weight: bold\">1</span>, Current Hour: <span style=\"color: #af00d7; text-decoration-color: #af00d7; font-weight: bold\">1</span>, Time Left: <span style=\"color: #af00d7; text-decoration-color: #af00d7; font-weight: bold\">8759.0</span> hrs ** \n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "** Current Day: \u001b[1;38;5;128m1\u001b[0m, Current Hour: \u001b[1;38;5;128m1\u001b[0m, Time Left: \u001b[1;38;5;128m8759.0\u001b[0m hrs ** \n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">- <span style=\"color: #005f00; text-decoration-color: #005f00; font-weight: bold\">Current State: </span><span style=\"color: #005f00; text-decoration-color: #005f00; font-weight: bold\">0</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "- \u001b[1;38;5;22mCurrent State: \u001b[0m\u001b[1;38;5;22m0\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">- <span style=\"color: #005f00; text-decoration-color: #005f00; font-weight: bold\">Current Homeostatic Variable: </span><span style=\"color: #005f00; text-decoration-color: #005f00; font-weight: bold\">0.0</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "- \u001b[1;38;5;22mCurrent Homeostatic Variable: \u001b[0m\u001b[1;38;5;22m0.0\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">- <span style=\"color: #005f00; text-decoration-color: #005f00; font-weight: bold\">Current Homeostatic Setpoint: </span><span style=\"color: #005f00; text-decoration-color: #005f00; font-weight: bold\">200.0</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "- \u001b[1;38;5;22mCurrent Homeostatic Setpoint: \u001b[0m\u001b[1;38;5;22m200.0\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">- <span style=\"color: #005f00; text-decoration-color: #005f00; font-weight: bold\">Time Elapsed: </span><span style=\"color: #005f00; text-decoration-color: #005f00; font-weight: bold\">25.0</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "- \u001b[1;38;5;22mTime Elapsed: \u001b[0m\u001b[1;38;5;22m25.0\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>. <span style=\"color: #af00d7; text-decoration-color: #af00d7; font-weight: bold\">Sleep</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>. <span style=\"color: #af00d7; text-decoration-color: #af00d7; font-weight: bold\">Binge on Internet</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>. <span style=\"color: #af00d7; text-decoration-color: #af00d7; font-weight: bold\">Work</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>. <span style=\"color: #af00d7; text-decoration-color: #af00d7; font-weight: bold\">Exercise</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span>. <span style=\"color: #af00d7; text-decoration-color: #af00d7; font-weight: bold\">Socialise</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span>. <span style=\"color: #af00d7; text-decoration-color: #af00d7; font-weight: bold\">Drink Alcohol</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6</span>. <span style=\"color: #af00d7; text-decoration-color: #af00d7; font-weight: bold\">Smoke</span>\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m0\u001b[0m. \u001b[1;38;5;128mSleep\u001b[0m\n",
       "\u001b[1;36m1\u001b[0m. \u001b[1;38;5;128mBinge on Internet\u001b[0m\n",
       "\u001b[1;36m2\u001b[0m. \u001b[1;38;5;128mWork\u001b[0m\n",
       "\u001b[1;36m3\u001b[0m. \u001b[1;38;5;128mExercise\u001b[0m\n",
       "\u001b[1;36m4\u001b[0m. \u001b[1;38;5;128mSocialise\u001b[0m\n",
       "\u001b[1;36m5\u001b[0m. \u001b[1;38;5;128mDrink Alcohol\u001b[0m\n",
       "\u001b[1;36m6\u001b[0m. \u001b[1;38;5;128mSmoke\u001b[0m\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Choose from 1 to 6 <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">(0)</span>: </pre>\n"
      ],
      "text/plain": [
       "Choose from 1 to 6 \u001b[1;36m(0)\u001b[0m: "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "IndexError",
     "evalue": "only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m__name__\u001b[39m \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m__main__\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m----> 2\u001b[0m     Simulation() \n",
      "Cell \u001b[1;32mIn[9], line 63\u001b[0m, in \u001b[0;36mSimulation.__init__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfinish \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m \u001b[39m#trigger to end simulation\u001b[39;00m\n\u001b[0;32m     62\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfinish:\n\u001b[1;32m---> 63\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfinish \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnext_time_interval()\n",
      "Cell \u001b[1;32mIn[9], line 117\u001b[0m, in \u001b[0;36mSimulation.next_time_interval\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    115\u001b[0m probSum \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m    116\u001b[0m \u001b[39mfor\u001b[39;00m nextS \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mactions:\n\u001b[1;32m--> 117\u001b[0m     probSum \u001b[39m=\u001b[39m probSum \u001b[39m+\u001b[39m (\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransition_environment[\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcurrent_state][action_taken][nextS])\n\u001b[0;32m    118\u001b[0m     \u001b[39mif\u001b[39;00m index \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m probSum:\n\u001b[0;32m    119\u001b[0m         next_state \u001b[39m=\u001b[39m nextS\n",
      "\u001b[1;31mIndexError\u001b[0m: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    Simulation() "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_research",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5794a404577a32ff418895c331e736516539893dd666baf27c4118b4263cef3b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
