{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Simulations\n",
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf #pip install \"tensorflow<2.11\"\n",
    "from tensorflow.keras import layers\n",
    "from collections import deque\n",
    "import random\n",
    "import numpy as np\n",
    "import shap\n",
    "\n",
    "import hrl_gym #simulate addiction\n",
    "import gymnasium as gym\n",
    "\n",
    "import shap"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function Approximator (Neural Network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(tf.keras.Model):  \n",
    "    def __init__(self, input_size, nb_action):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.nb_action = nb_action\n",
    "        self.fc1 = tf.keras.layers.Dense(30, activation=tf.nn.relu, input_shape=(input_size,))\n",
    "        self.fc2 = tf.keras.layers.Dense(nb_action)\n",
    "    \n",
    "    def call(self, state):\n",
    "        x = self.fc1(state)\n",
    "        x = tf.nn.relu(x)\n",
    "        q_values = self.fc2(x)\n",
    "        return q_values"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experience Replay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory(): \n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "    \n",
    "    def push(self, event):\n",
    "        self.memory.append(event)\n",
    "        if len(self.memory) > self.capacity:\n",
    "            del self.memory[0]\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        samples = random.sample(self.memory, batch_size)\n",
    "        states, next_states, actions, rewards, dones = zip(*samples)\n",
    "        states = tf.convert_to_tensor(states)\n",
    "        next_states = tf.convert_to_tensor(next_states)\n",
    "        actions = tf.convert_to_tensor(actions)\n",
    "        rewards = tf.convert_to_tensor(rewards)\n",
    "        dones = tf.convert_to_tensor(dones)\n",
    "        return states, next_states, actions, rewards, dones"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Q-Learning Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dqn():\n",
    "    def __init__(self, input_size, nb_action, gamma, capacity=100000, learning=0.001, temperature=100, sample_rate=100):\n",
    "        self.gamma = gamma\n",
    "        self.reward_window = []\n",
    "        self.model = Network(input_size, nb_action)\n",
    "        self.memory = ReplayMemory(capacity)\n",
    "        self.sample_rate = sample_rate\n",
    "        self.optimizer = tf.keras.optimizers.Adam(learning_rate=learning)\n",
    "        self.last_state = tf.zeros((1, input_size))\n",
    "        self.last_action = 0\n",
    "        self.last_reward = 0\n",
    "        self.temp = temperature\n",
    "\n",
    "        self.df = pd.DataFrame(columns=['outputs'])\n",
    "\n",
    "    def select_action(self, state):\n",
    "        q_value_tensor = self.model(state, training=False)\n",
    "        q_values = q_value_tensor.numpy().squeeze()\n",
    "        probs = tf.nn.softmax(q_value_tensor * self.temp)\n",
    "        action_prob = probs.numpy().squeeze()\n",
    "\n",
    "        action = tf.random.categorical(probs, num_samples=1)\n",
    "        action = action.numpy()[0, 0]\n",
    "\n",
    "        return action, q_values, action_prob\n",
    "\n",
    "    def update(self, reward, new_signal):\n",
    "        new_state = tf.convert_to_tensor([new_signal], dtype=tf.float32)\n",
    "        self.memory.push((self.last_state, new_state, self.last_action, reward, False))\n",
    "\n",
    "        self.reward_window.append(reward)\n",
    "        if len(self.reward_window) > 1000:\n",
    "            del self.reward_window[0]\n",
    "\n",
    "        if len(self.memory.memory) > 128:\n",
    "            states, next_states, actions\n",
    "\n",
    "        # Compute SHAP values to explain the contribution of each input feature to the output of the neural network\n",
    "        explainer = shap.KernelExplainer(self.model.predict_on_batch, shap.sample(states, 100))\n",
    "        shap_values = explainer.shap_values(states, nsamples=10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulation"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
