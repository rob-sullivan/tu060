{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import random"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rgb_to_gray(self, im):\n",
    "    \"\"\"Converts RGB image into Greyscale using formula\n",
    "    Formula: Y = 0.2126R+0.7152G+0.0722B;\n",
    "    R=red,G=green,B=blue;\"\"\"\n",
    "    return np.dot(im, [0.2126, 0.7152, 0.0722])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.Reshaping the input image such that the width of the resulting image equals the resized width, 84, indicated by the resized_shape parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cv2_resize_image(image, resized_shape=(84, 84),method='crop', crop_offset=8):\n",
    "    \"\"\"returns a cropped image with default size of 84 x 84, given \n",
    "    a grayscale input image\"\"\"\n",
    "    height, width = image.shape    \n",
    "    resized_height, resized_width = resized_shape\n",
    "    if method == 'crop':\n",
    "        h = int(round(float(height) * resized_width / width))\n",
    "        resized = cv2.resize(image, \n",
    "                             (resized_width, h),\n",
    "                             interpolation=cv2.INTER_LINEAR)\n",
    "        crop_y_cutoff = h - crop_offset - resized_height\n",
    "        cropped = resized[crop_y_cutoff:crop_y_cutoff+resized_height, :]\n",
    "        return np.asarray(cropped, dtype=np.uint8)    \n",
    "    elif method == 'scale':        \n",
    "        return np.asarray(cv2.resize(image,\n",
    "                                     (resized_width, resized_height),\n",
    "                                     interpolation=cv2.INTER_LINEAR),\n",
    "                                     dtype=np.uint8)\n",
    "    else:\n",
    "        raise ValueError('Unrecognized image resize method.')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Q-Learning Theory\n",
    "* The State Space defines all possible events that could happen. In Atari this is a screen image or set of screen images collected over a time interval.\n",
    "\n",
    "* The Reward Function defines the goal that needs to be solved, mapping states and actions to a value that indicates the desirability of being in that state. In Atari reward is the score recieved after taking actions.\n",
    "\n",
    "* Policy Function is the begavior of the agent and maps states to actions that need to be taken when in those states.\n",
    "\n",
    "* Value Function informs of which state action pairs are good in the long term, or state value discounted amount of reward an agent can expect to collect over time.\n",
    "\n",
    "* An episode consists of one complete pass from a start state, thorugh different states until a terminal state is reached (goal reached or times up).\n",
    "\n",
    "* Exploration is trying something new to learn more about the environment and exploitation is making the best decision based on all information you have. Epsilon greedy is the simplest way to make such a trade-off."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Manual_Q_learning():\n",
    "    \"\"\"This simplest Q-learning algorithm can \n",
    "    only handle discrete states and actions\n",
    "    continuous states, it fails because the \n",
    "    convergence is not guaranteed due to the \n",
    "    existence of infinite states\"\"\"\n",
    "    alpha = 1.0\n",
    "    gamma = 0.8\n",
    "    epsilon = 0.2\n",
    "    num_episodes = 100\n",
    "    R = np.array([\n",
    "        [-1, 0,-1, -1, -1, -1],\n",
    "        [0, -1, 0, -1,  0, -1],\n",
    "        [-1, 0,-1,-50, -1, -1],\n",
    "        [-1,-1, 0, -1, -1, -1],\n",
    "        [-1, 0,-1, -1, -1,100],\n",
    "        [-1,-1,-1, -1, -1, -1]])\n",
    "    # initialise Q\n",
    "    Q = np.zeros((6,6))\n",
    "    #run each episode\n",
    "    for _ in range(num_episodes):\n",
    "        #randomly choose an intial state\n",
    "        s = np.random.choice(5)\n",
    "        while s !=5:\n",
    "            #get all possible actions\n",
    "            actions = [a for a in range(6) if R[s][a] != -1]\n",
    "            #epsilon-greedy\n",
    "            if np.random.binomial(1, epsilon) == 1:\n",
    "                a = random.choice(actions)\n",
    "            else:\n",
    "                a = actions[np.argmax(Q[s][actions])]\n",
    "            next_state = a\n",
    "            #update Q(s,a)\n",
    "            Q[s][a] += alpha * (R[s][a] + gamma * np.max(Q[next_state])- Q[s][a])\n",
    "            #go to next state\n",
    "            s = next_state\n",
    "    return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q-Table:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[  0.  ,  64.  ,   0.  ,   0.  ,   0.  ,   0.  ],\n",
       "       [ 51.2 ,   0.  ,  51.2 ,   0.  ,  80.  ,   0.  ],\n",
       "       [  0.  ,  64.  ,   0.  ,  -9.04,   0.  ,   0.  ],\n",
       "       [  0.  ,   0.  ,  51.2 ,   0.  ,   0.  ,   0.  ],\n",
       "       [  0.  ,  64.  ,   0.  ,   0.  ,   0.  , 100.  ],\n",
       "       [  0.  ,   0.  ,   0.  ,   0.  ,   0.  ,   0.  ]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Q-Table:\")\n",
    "Manual_Q_learning()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This simplest Q-learning algorithm can only handle discrete states and actions continuous states, it fails because the convergence is not guaranteed due to the existence of infinite states. The solution is to use a neural network.\n",
    "\n",
    "The Neural Network Architecture takes the state as an input and the output for each possible action. This allows the ability to compute Q-values for all possible actions given that state as a single forward pass through the network and removing the cost of scaling. For some tasks such as playing the game breakout in atari the direction and velocity of the paddle and ball are important. This information is not available in an 84 x 84 pixel image. Instead we need to take 4 images to compare.\n",
    "\n",
    "The state space now becomes 84 x 84 x 4 or 28,224 states. Convolutional neural networks can be useful here. The Q network can contain x1 input layer, x3 convolutional layers and x1 fully connected layer.\n",
    "The first convolutional layer has 64 8x8 filters with a stride of 4, using RELU. The second has 64 4x4 filters, stride of 2, using RELU.\n",
    "The third layer has 64 3x3 filters, stride of 2, using RELU. The reason for the first layer having a large filter size is to pick up on smaller objects in the game such as the ball or paddle. The rest are sufficient to pick up useful features.\n",
    "\n",
    "algorithms that use the bellman equation as an iterative update are called value iteration algorithms.\n",
    "\n",
    "* Q(s,a) = R(s,a) + gamma * max_a(s') * Q(s',a) (1)\n",
    "\n",
    "The formula above is only suitable for deterministic environment where the next state S' is fixed given the current state s and a. \n",
    "\n",
    "In non-deterministic environments the bellman equation needs to be modified to the below formula:\n",
    "* Q(s,a) = Es'~s[R(s,a) + gamma * max_a(s') * Q(s',a)|s,a] (2)\n",
    "\n",
    "The right hand side of the equation takes the value iteration and the Q network can be trained by minimising the loss function at i_th iteration, using the following formula:\n",
    "* L_i(tetha_i) = E_s,a~P(s,a)[yi - Q(s,a; tetha_i)^2] (3)\n",
    "\n",
    "Q(s,a) represents a Q-network that uses equation 2 to update its parameters. \n",
    "P(s,a) is the probability distribution over sequences and actions. previous parameters from i-l are fixed when optimising the loss function L_i(tetha_i) over theta. We don't optimise this directly but instead minimise the emeperical loss using stocastic gradient descent.\n",
    "\n",
    "This algorithm doesn't need to know the internal workings of the atari game it just needs to use samples from the game. This is called model-free, treating the simulator as a black box.\n",
    "\n",
    "It is off-policy because it learns about the greedy policy argmaxQ)s, a, theta) while following a probability distribution P(s,a) which balances exploration with explotation. P(s,a) can be epislon-greedy.\n",
    "\n",
    "The brain of our AI player is the Q-network controller. At each time step t, she observes the screen image  (recall that st is an  image that stacks the last four frames). Then, her brain analyzes this observation and comes up with an action, . The Atari emulator receives this action and returns the next screen image, , and the reward, . The quadruplet  is stored in the memory and is taken as a sample for training the Q-network by minimizing the empirical loss function via stochastic gradient descent.\n",
    "\n",
    "if we sample directly from memory the data is strongly correlated breaking assumptions that samples for emperical loss function are independent, making training unstable and leadning to bad performance. Using experience replay we can sample data randomly for training.\n",
    "\n",
    "Algorithm for Q-Learning\n",
    "```\n",
    "Initialize replay memory  to capacity ;\n",
    "Initialize the Q-network  with random weights ;\n",
    "Repeat for each episode:\n",
    "    Set time step ;\n",
    "    Receive an initial screen image  and do preprocessing ;\n",
    "    While the terminal state hasn't been reached:\n",
    "        Select an action at via greedy, i.e., select a random action with probability , otherwise select ;\n",
    "        Execute action at in the emulator and observe reward  and image ;\n",
    "        Set  and store transition  into replay memory ;\n",
    "        Randomly sample a batch of transitions  from ;\n",
    "        Set  if  is a terminal state or  if  is a non-terminal state;\n",
    "        Perform a gradient descent step on ;\n",
    "    End while\n",
    "```\n",
    "\n",
    "This algorithm will work for some atari games like breakout, seaquest, pong and Qbert but cannot reach humal-level control. This is thought to be because computing the target uses the current estimate of action-value function, which updates Q(st,a) while also increasing Q(st-1,a), in turn increasing the target. This leads to oscillation or divergence of the policy.\n",
    "To solve this problem a seperate network is used for generating the targets in the Q-learning update. More precisely for every M q-learning updates the network Q is cloned to obtain a target network Q which is used for generating targets in the following M updates to Q. The new algorithm is below.\n",
    "\n",
    "Q-learning algorithm:\n",
    "```\n",
    "Initialize replay memory  to capacity ;\n",
    "Initialize the Q-network  with random weights ;\n",
    "Initialize the target network  with weights ;\n",
    "Repeat for each episode:\n",
    " Set time step ;\n",
    "    Receive an initial screen image  and do preprocessing ;\n",
    "    While the terminal state hasn't been reached:\n",
    "        Select an action at via greedy, i.e., select a random action with probability , otherwise select ;\n",
    "        Execute action at in the emulator and observe reward  and image ;\n",
    "        Set  and store transition  into replay memory ;\n",
    "        Randomly sample a batch of transitions  from ;\n",
    "        Set  if  is a terminal state or  if  is a non-terminal state;\n",
    "        Perform a gradient descent step on ;\n",
    "        Set  for every  steps;\n",
    " End while\n",
    "```\n",
    "\n",
    "This update allows the algorithm to solve Star Gunner, Atlantis, Assault, and Space Invaders and 45 other games.\n",
    "\n",
    "However, 1) it is slow at converging (7 days on one GPU) to reach human-level performance, 2) it fails with sparse reward, Montezuma's revenge requires long term plannin, and 3) large data amount reqired. Double Q-learning, prioritized experience replay, bootstrapped DQN, and dueling network architectures have all been proposed to solve this. In the next section we only implement DQN, not its varients."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Q-Learning Implementation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "class DQN:\n",
    "    def __init__(self, \n",
    "                 input_shape=(84, 84, 4), \n",
    "                 n_outputs=4,\n",
    "                 network_type='cnn',\n",
    "                 scope='q_network'):\n",
    "        self.width = input_shape[0]\n",
    "        self.height = input_shape[1]\n",
    "        self.channel = input_shape[2]\n",
    "        self.n_outputs = n_outputs\n",
    "        self.network_type = network_type\n",
    "        self.scope = scope\n",
    "\n",
    "        # frame images\n",
    "        self.x = tf.placeholder(dtype=tf.float32,\n",
    "                                shape=(None, \n",
    "                                       self.channel, \n",
    "                                       self.width, \n",
    "                                       self.height))\n",
    "        # estimates of Q-value\n",
    "        self.y = tf.placeholder(dtype=tf.float32, shape=(None, ))\n",
    "\n",
    "        #selected actions\n",
    "        self.a = tf.placeholder(dtype=tf.int32, shape=(None,))\n",
    "\n",
    "        with tf.variable_scope(scope):\n",
    "            self.build()\n",
    "            self.build_loss()\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_research",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
