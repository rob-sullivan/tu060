{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Addiction Simulator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Virtual Environment\n",
    "1. make sure python3 is installed on your system.\n",
    "2. make sure you have a python virtual environment setup: \n",
    "python -m pip install --upgrade pip setuptools virtualenv\n",
    "3. create a python environment: \n",
    "python -m venv venv (this creates a virtual environment called venv)\n",
    "4. if applicable add /venv/ to your .gitignore.\n",
    "5. activate the virtual environment: \n",
    "either \\venv\\Scripts\\activate.bat on windows \n",
    "or source venv/bin/activate on mac+linux\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#basic\n",
    "from os import system, name\n",
    "import random\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import datetime\n",
    "\n",
    "#pytorch for gpu processing of ML model\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.autograd as autograd\n",
    "from torch.autograd import Variable\n",
    "\n",
    "#rich library for Terminal UI\n",
    "from rich.jupyter import print\n",
    "#from rich import print\n",
    "from rich.prompt import IntPrompt\n",
    "\n",
    "\n",
    "#hide pytorch warnings (should eventually be resolved)\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent's Brain (Deep Q-Learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):  \n",
    "    def __init__(self, input_size, nb_action):\n",
    "        #ref: https://discuss.pytorch.org/t/super-model-in-init/97426\n",
    "        #super(Network, self).__init__()\n",
    "        super().__init__() #pytorch's NN model\n",
    "        self.input_size = input_size\n",
    "        self.nb_action = nb_action\n",
    "        self.fc1 = nn.Linear(input_size, 30)#arbitrarily chose 30 hidden layers\n",
    "        self.fc2 = nn.Linear(30, nb_action)\n",
    "    \n",
    "    #base pytorch NN model runs and we override the\n",
    "    #forward function with our own relu activation function\n",
    "    def forward(self, state):\n",
    "        x = F.relu(self.fc1(state))\n",
    "        q_values = self.fc2(x)\n",
    "        return q_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experience Replay Model\n",
    "This model is used for training our DQN model. It stores the transitions that the agent observes, allowing us to reuse this data later. By sampling from it randomly, the transitions that build up a batch are decorrelated. It has been shown that this greatly stabilizes and improves the DQN training procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory(): \n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "    \n",
    "    def push(self, event):\n",
    "        self.memory.append(event)\n",
    "        if len(self.memory) > self.capacity:\n",
    "            del self.memory[0]\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        samples = zip(*random.sample(self.memory, batch_size))\n",
    "        return map(lambda x: Variable(torch.cat(x, 0)), samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DQN Ensemble\n",
    "Comprised of a neural network model and a memory model. \n",
    "* The NN takes in observation of sensor data (brain chemicals) and chooses actions based on the relu activation function. \n",
    "* The agent will sample some of the sensor data and store in long term memory to be reused later for training. \n",
    "* We also use the Adam Optimisation algorithm. This is an extension to stocastic gradient desent to update weights of the neural network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dqn():\n",
    "    def __init__(self, input_size, nb_action, gamma):\n",
    "        self.gamma = gamma\n",
    "        self.reward_window = []\n",
    "        self.model = Network(input_size, nb_action)\n",
    "        self.memory = ReplayMemory(100000)\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr = 0.001)\n",
    "        self.last_state = torch.Tensor(input_size).unsqueeze(0)\n",
    "        self.last_action = 0\n",
    "        self.last_reward = 0\n",
    "    \n",
    "    # select action for x duration\n",
    "    def select_action(self, state):\n",
    "        #softmax converts numbers into probabilities\n",
    "        #Q values are the output of the neural network\n",
    "            #view q values\n",
    "        q_value_tensor = self.model(Variable(state, volatile = True)) \n",
    "        q_values = [q_value.detach().numpy() for q_value in q_value_tensor]\n",
    "            #print(q_values)\n",
    "            #viz q value for each action, (T value by user choice)\n",
    "            #pie chart 0/1 #seperate action\n",
    "        # Temperature value = 100. closer to zero the less sure the NN will be to taking the action\n",
    "        probs = F.softmax(self.model(Variable(state, volatile = True))*100) # T=100\n",
    "        action_prob = [prob.detach().numpy() for prob in probs]\n",
    "\n",
    "        action = probs.multinomial(num_samples=1) # action taken\n",
    "        #q_values[0][action] #qualiy of taking action in state\n",
    "        #action_prob[0][action] #probability of taking action\n",
    "\n",
    "        #return the action taken, q values and probabilities of taking action given state.\n",
    "        return action.data[0,0], q_values[0][action], action_prob[0][action]\n",
    "        #return action.data[0,0], q_values, action_prob\n",
    "    \n",
    "    #to train our AI\n",
    "    #forward propagation then backproagation\n",
    "    # get our output, target, compare our output to the target to compute the loss error\n",
    "    # backproagate loss error into the nn and use stochastic gradient descent we update the weights according to how much they contributed to the loss error\n",
    "    def learn(self, batch_state, batch_next_state, batch_reward, batch_action):\n",
    "        outputs = self.model(batch_state).gather(1, batch_action.unsqueeze(1)).squeeze(1)\n",
    "        next_outputs = self.model(batch_next_state).detach().max(1)[0]\n",
    "        target = self.gamma*next_outputs + batch_reward\n",
    "        td_loss = F.smooth_l1_loss(outputs, target)\n",
    "        self.optimizer.zero_grad()\n",
    "        td_loss.backward(retain_graph = True)\n",
    "        self.optimizer.step()\n",
    "    \n",
    "    #When ai reaches a new state we update everything\n",
    "    #update action, last action becomes the new action but also the last state becomes the new state and last reward becomes the new state\n",
    "    # we then get this new transition and update our reward window to track training progress and exploration\n",
    "    def update(self, reward, new_signal):\n",
    "        new_state = torch.Tensor(new_signal).float().unsqueeze(0)\n",
    "        self.memory.push((self.last_state, new_state, torch.LongTensor([int(self.last_action)]), torch.Tensor([self.last_reward])))\n",
    "        action, q, p = self.select_action(new_state)\n",
    "        if len(self.memory.memory) > 100:\n",
    "            batch_state, batch_next_state, batch_action, batch_reward = self.memory.sample(100)\n",
    "            self.learn(batch_state, batch_next_state, batch_reward, batch_action)\n",
    "        self.last_action = action\n",
    "        self.last_state = new_state\n",
    "        self.last_reward = reward\n",
    "        self.reward_window.append(reward)\n",
    "        #if len(self.reward_window) > 1000:\n",
    "        #    del self.reward_window[0]\n",
    "        return action, q, p\n",
    "    \n",
    "    def score(self):\n",
    "        #return sum(self.reward_window)/(len(self.reward_window)+1.0)\n",
    "        return sum(self.reward_window)\n",
    "    \n",
    "    def save(self):\n",
    "        torch.save({'state_dict': self.model.state_dict(),\n",
    "                    'optimizer' : self.optimizer.state_dict(),\n",
    "                   }, 'last_brain.pth')\n",
    "    def load(self):\n",
    "        if os.path.isfile('last_brain.pth'):\n",
    "            print(\"=> loading checkpoint... \")\n",
    "            checkpoint = torch.load('last_brain.pth')\n",
    "            self.model.load_state_dict(checkpoint['state_dict'])\n",
    "            self.optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "            print(\"done !\")\n",
    "        else:\n",
    "            print(\"no checkpoint found...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Simulation():    \n",
    "    def __init__(self):\n",
    "        # Agent Brain - a neural network that represents our Q-function\n",
    "        self.agent = Dqn(2,3,0.9) # 4 sensors, 8 actions, gama = 0.9\n",
    "        #Agent's Actions\n",
    "        #self.actions = ['Sleep', 'Binge on Internet', 'Work', 'Exercise', 'Socialise', 'Drink Alcohol', 'Smoke', 'Take Cocaine'] #8 actions\n",
    "        self.actions = ['Do Nothing', 'In-Active Lever', 'Active Lever']\n",
    "        \n",
    "        # Mehdi Keramati's definition of an animal ref Mehdi Keramati's Homeostatic RL sim of addiction\n",
    "        ## fatigue or lever/action cost\n",
    "        self.fatigue = -1\n",
    "\n",
    "        #Keramati's outcome, e.g cocaine = 50, representing the dose of self-administered drug,\n",
    "        self.outcome = [0,0,50] #all others were scaled from cocaine as their assumed impact on the brain. #[0,10,5,12.5,12.5,10,25,50]#self.outcome = [0,0,0,0,0,0,0,50] #all others were scaled from cocaine as their assumed impact on the brain. #[0,10,5,12.5,12.5,10,25,50]\n",
    "        self.outcomeBuffer = 0\n",
    "        self.epochs_inactive = 4 # python index starts at 0 so (1+4)* 4 = 5*4 = 20sec lever will be active for 4 seconds then disabled for 20secs, on the 20th second it will be active.\n",
    "        self.outcome_to_disable = []\n",
    "\n",
    "        ## Homeostatic System\n",
    "        self.initialInState = 0\n",
    "        self.initialSetpoint = 200\n",
    "        self.inStateLowerBound = 0\n",
    "        self.outcomeDegradationRate = 0.007 # e.g dose of cocaine that the animal loses in every time-step\n",
    "        self.outcomeAbsorptionRatio = 0.12 # e.g proportion of the injected cocaine that affects the brain right after infusion\n",
    "        self.estimatedNonHomeostaticReward = 0.0\n",
    "\n",
    "        ## Allostatic (Stress) System\n",
    "        self.setpointShiftRate = 0.0018\n",
    "        self.setpointRecoveryRate = 0.00016\n",
    "        self.optimalInStateLowerBound = 100\n",
    "        self.optimalInStateUpperBound = 200\n",
    "\n",
    "        ## Drive Function\n",
    "        self.m = 3 # Parameter of the drive function : m-th root\n",
    "        self.n = 4 # Parameter of the drive function : n-th pawer\n",
    "        self.driveReductionReward = 0.0\n",
    "\n",
    "        ## Goal-directed system\n",
    "        self.updateRewardRate = 0.2  # Learning rate for updating the non-homeostatic reward function\n",
    "        self.updateOutcomeRate = 0.2  # Learning rate for updating the outcome function\n",
    "        \n",
    "        #agent's sensors or observation space\n",
    "        self.last_action = 0 #c(external state /exited)\n",
    "        self.internal_state = float(self.initialInState) #internal variable that moves homeostatic setpoint\n",
    "        self.setpoint_S = float(self.initialSetpoint) #homeostatic setpoint\n",
    "\n",
    "        \n",
    "        # the mean score curve (sliding window of the rewards) with \n",
    "        # respect to time.\n",
    "        self.total_reward = []\n",
    "\n",
    "        #Environment: the agent's grid world is elapsing time from 1 to endpoint\n",
    "        self.total_time = 4 # total time in hours (size of the world) e.g 1 year or 8760hrs, or 4 (1hr pretraining and 3 hours seeking cocaine)\n",
    "        self.total_epochs = (self.total_time*3600)/4 #e.g 20 secs is 5 trials or epochs of time or 1 hour = 3600secs/4secs = 900 epochs, 8760hrs is 7,884,000 epochs\n",
    "\n",
    "        self.current_epoch = 0 # e.g 1=4secs\n",
    "\n",
    "        # reward agent wants to maximise this will be the homeostatic reward\n",
    "        self.reward_received = 0.0\n",
    "\n",
    "        #create data table\n",
    "        #self.df = pd.DataFrame(columns=['epoch', 'action', 'q_value_0', 'q_value_1','q_value_2','t_prob_0', 't_prob_1','t_prob_2', 'epochs_inactive', 'internal_variable', 'homeostatic_setpoint', 'reward', 'score'])\n",
    "\n",
    "        self.finish = False #trigger to end simulation\n",
    "        while not self.finish:\n",
    "            self.finish = self.next_time_interval()# begin simulation and repeat until triggered not to\n",
    "        if(self.finish):\n",
    "            #print(\"saving brain...\")\n",
    "            #brain.save()\n",
    "            plt.title(\"Rewards\")\n",
    "            plt.xlabel(\"Epoch\")\n",
    "            plt.ylabel(\"Reward\")\n",
    "            plt.plot(self.total_reward)\n",
    "            plt.show()\n",
    "\n",
    "            plt.title(\"Internal_variable [Dopamine]\")\n",
    "            plt.xlabel(\"Epoch\")\n",
    "            plt.ylabel(\"Internal Variable\")\n",
    "            plt.plot(self.df[\"internal_variable\"])\n",
    "            plt.show()\n",
    "\n",
    "            plt.title(\"Homeostatic Setpoint\")\n",
    "            plt.xlabel(\"Epoch\")\n",
    "            plt.ylabel(\"Homeostatic Setpoint\")\n",
    "            plt.plot(self.df[\"homeostatic_setpoint\"])\n",
    "            plt.show()\n",
    "\n",
    "    def next_time_interval(self):\n",
    "        ## get the time left\n",
    "        epochs_left = int(self.total_epochs - self.current_epoch)\n",
    "        \n",
    "        ## agent input state vector, composed of the five brain signals or observations received from being in the environment\n",
    "        current_state = [self.internal_state, self.setpoint_S] #self.last_action,, self.current_epoch\n",
    "        action_to_take, qValues, transitionProbs = self.agent.update(self.reward_received, current_state) # playing the action from the ai (dqn class)\n",
    "        self.total_reward.append(self.agent.score())\n",
    "        \n",
    "        #we can take the agent's NN and call forward to output via softmax q values for each state.\n",
    "        suggested_action = self.actions[action_to_take]\n",
    "        #sa = int(0 if suggested_action==\"Sleep\" else 1 if suggested_action==\"Binge on Internet\" else 2 if suggested_action==\"Work\" else 3 if suggested_action==\"Exercise\" else 4 if suggested_action==\"Socialise\" else 5 if suggested_action==\"Drink Alcohol\" else 6 if suggested_action==\"Smoke\" else 7 if suggested_action==\"Take Cocaine\" else -1)\n",
    "        sa = int(0 if suggested_action==\"Do Nothing\" else 1 if suggested_action==\"In-Active Lever\" else 2 if suggested_action==\"Active Lever\" else -1)\n",
    "        #give user options\n",
    "        #print(\"0. [bold dark_violet]Sleep[/bold dark_violet]\\n1. [bold dark_violet]Binge on Internet[/bold dark_violet]\\n2. [bold dark_violet]Work[/bold dark_violet]\\n3. [bold dark_violet]Exercise[/bold dark_violet]\\n4. [bold dark_violet]Socialise[/bold dark_violet]\\n5. [bold dark_violet]Drink Alcohol[/bold dark_violet]\\n6. [bold dark_violet]Smoke[/bold dark_violet]\\n\")\n",
    "        #action_taken = 0\n",
    "        #action_taken = IntPrompt.ask(\"Choose from 1 to 6\", default=sa)\n",
    "        action_taken = sa #automatic\n",
    "        self.last_action = action_taken #automatic\n",
    "            \n",
    "        #update agent brain chemicals after action taken\n",
    "        if(action_taken == 0): #Do Nothing/Sleep\n",
    "            #1. update internal sensors or observations of the environment\n",
    "\n",
    "            ## Update internal state upon consumption  \n",
    "            interS = self.internal_state + (self.outcome[action_taken] * self.outcomeAbsorptionRatio) - self.outcomeDegradationRate * (self.internal_state - self.inStateLowerBound)\n",
    "            if interS < self.inStateLowerBound:\n",
    "                interS = self.inStateLowerBound    \n",
    "            self.internal_state = interS\n",
    "\n",
    "            ## Update homeostatic setpoint\n",
    "            optInS = self.setpoint_S + self.outcome[action_taken]  * self.setpointShiftRate - self.setpointRecoveryRate\n",
    "            if optInS < self.optimalInStateLowerBound:\n",
    "                optInS = self.optimalInStateLowerBound\n",
    "            if optInS > self.optimalInStateUpperBound:\n",
    "                optInS = self.optimalInStateUpperBound\n",
    "            self.setpoint_S = optInS\n",
    "\n",
    "        elif((action_taken != 0) or (action_taken != -1)): #In-Active Lever/Active Lever #Binge on Internet, Work, Exercise, Socialise, Drink Alcohol, Smoke\n",
    "            #1. determine the next state that the agent fell into from taking action\n",
    "            #n/a\n",
    "            \n",
    "            #2. get Non-Homeostatic reward e.g energy cost or fatigue of doing an action\n",
    "                ## if the agent is not sleeping then it is doing something that costs energy\n",
    "                #nonHomeoRew = -self.fatigue\n",
    "            self.estimatedNonHomeostaticReward = (1.0 - self.updateRewardRate) * self.estimatedNonHomeostaticReward + self.updateRewardRate * (self.fatigue)\n",
    "            \n",
    "            #3. get Homeostatically-regulated Reward of doing action (drive reduction)\n",
    "            d1 = math.pow(math.fabs(math.pow(self.setpoint_S - self.internal_state, self.n*1.0)),(1.0/self.m))\n",
    "            d2 = math.pow(math.fabs(math.pow(self.setpoint_S - self.internal_state - self.outcome[action_taken], self.n*1.0)),(1.0/self.m))\n",
    "                #HomeoRew = d1 - d2\n",
    "            self.driveReductionReward = (1.0 - self.updateOutcomeRate) * self.driveReductionReward + self.updateOutcomeRate * (d1 - d2)\n",
    "            \n",
    "            #4. Now calculate agent reward\n",
    "            #self.reward_received = values[action] +  transitionProb * ( self.driveReductionReward + self.estimatedNonHomeostaticReward )\n",
    "            #self.reward_received = qValue +  transitionProb * ( self.driveReductionReward + self.estimatedNonHomeostaticReward )\n",
    "            self.reward_received = self.driveReductionReward + self.estimatedNonHomeostaticReward\n",
    "            #5. update estimated next state\n",
    "            # n/a\n",
    "            \n",
    "            #6. update internal sensors or observations of the environment\n",
    "            \n",
    "            ## Update internal state upon consumption\n",
    "            self.outcomeBuffer = self.outcomeBuffer + self.outcome[action_taken]   \n",
    "            interS = self.internal_state + (self.outcomeBuffer * self.outcomeAbsorptionRatio) - self.outcomeDegradationRate * (self.internal_state - self.inStateLowerBound)\n",
    "            if interS < self.inStateLowerBound:\n",
    "                interS = self.inStateLowerBound    \n",
    "            self.internal_state = interS\n",
    "\n",
    "            ## Update homeostatic setpoint\n",
    "            optInS = self.setpoint_S + (self.outcome[action_taken]  * self.setpointShiftRate) - self.setpointRecoveryRate\n",
    "            if optInS < self.optimalInStateLowerBound:\n",
    "                optInS = self.optimalInStateLowerBound\n",
    "            if optInS > self.optimalInStateUpperBound:\n",
    "                optInS = self.optimalInStateUpperBound\n",
    "            self.setpoint_S = optInS \n",
    "\n",
    "            ## Update outcome buffer\n",
    "            self.outcomeBuffer = self.outcomeBuffer * (1 - self.outcomeAbsorptionRatio)\n",
    "\n",
    "        elif(action_taken == -1):#quit\n",
    "            return True   #end simulation\n",
    "        \n",
    "        clear_output(wait=True)\n",
    "        print(\"** Current Time: [bold dark_violet]\" + str(round((1/900)*(self.current_epoch),2)) + \n",
    "              \"[/bold dark_violet] hrs, Epoch Left: [bold dark_violet]\" + str(round((1/900)*(epochs_left),2)) + \n",
    "              \"[/bold dark_violet] hrs, **\\n[bold dark_green]Last Action: \" + str(suggested_action) + \n",
    "              \"[/bold dark_green],\\n[bold dark_green]Current Homeostatic Variable: \" + str(round(self.internal_state, 4)) + \n",
    "              \"[/bold dark_green], [bold dark_green]Current Homeostatic Setpoint: \" + str(round(self.setpoint_S, 4)) +\n",
    "              \"[/bold dark_green],\\n [bold dark_green]Reward Received: \" + str(round(self.reward_received,2)) + \n",
    "              \"[/bold dark_green], [bold dark_green]Total Score: \" + str(round(self.total_reward[-1],2)) + \n",
    "              \"[/bold dark_green],\\n [bold dark_green]Q-Values: \")# + str(qValues[0]) + \n",
    "              #\"[/bold dark_green],\\n [bold dark_green]T-Probs: \" + str(transitionProbs[0]) + \"[/bold dark_green]\")\n",
    "        #self.df.loc[self.current_epoch] = [self.current_epoch , suggested_action, qValues[0][0], qValues[0][1], qValues[0][2], transitionProbs[0][0], transitionProbs[0][1], transitionProbs[0][2], self.epochs_inactive, self.internal_state, self.setpoint_S, self.reward_received, self.total_reward[-1]]\n",
    "    \n",
    "        #check if this is the last round otherwise continue\n",
    "        if(self.current_epoch >= self.total_epochs):\n",
    "            return True   #end simulation\n",
    "        else:    \n",
    "            # Updating the last time from the agent to the end time (goal)\n",
    "            self.current_epoch += 1  #update to next day interval\n",
    "\n",
    "            #set time when active lever outcome 50 goes to zero\n",
    "            if(self.epochs_inactive == 4):#lever is active\n",
    "                self.epochs_inactive = 0 #off for 16 seconds then on the 20th second activate\n",
    "                self.outcome_to_disable = self.outcome #remember what values were\n",
    "                self.outcome = [0] * len(self.outcome)\n",
    "            else:\n",
    "                self.outcome = self.outcome_to_disable #restore outcome values\n",
    "                self.epochs_inactive += 1 #count down by 4secs\n",
    "                \n",
    "            return False\n",
    "\n",
    "    def clear(self): \n",
    "        \"\"\"\n",
    "        This function was taken from https://www.geeksforgeeks.org/clear-screen-python/ to\n",
    "        allow the terminal to be cleared when changing menus or showing the user important\n",
    "        messages. It checks what operating system is being used and uses the correct \n",
    "        clearing command.\n",
    "        \"\"\"\n",
    "        # for windows \n",
    "        if name == 'nt': \n",
    "            _ = system('cls') \n",
    "\n",
    "        # for mac and linux(here, os.name is 'posix')\n",
    "        else: \n",
    "            _ = system('clear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "invalid index to scalar variable.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m__name__\u001b[39m \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m__main__\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m----> 2\u001b[0m     env \u001b[39m=\u001b[39m Simulation() \n",
      "Cell \u001b[1;32mIn[21], line 66\u001b[0m, in \u001b[0;36mSimulation.__init__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfinish \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m \u001b[39m#trigger to end simulation\u001b[39;00m\n\u001b[0;32m     65\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfinish:\n\u001b[1;32m---> 66\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfinish \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnext_time_interval()\u001b[39m# begin simulation and repeat until triggered not to\u001b[39;00m\n\u001b[0;32m     67\u001b[0m \u001b[39mif\u001b[39;00m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfinish):\n\u001b[0;32m     68\u001b[0m     \u001b[39m#print(\"saving brain...\")\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m#brain.save()\u001b[39;00m\n\u001b[0;32m     70\u001b[0m     plt\u001b[39m.\u001b[39mtitle(\u001b[39m\"\u001b[39m\u001b[39mRewards\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[21], line 179\u001b[0m, in \u001b[0;36mSimulation.next_time_interval\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    169\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mTrue\u001b[39;00m   \u001b[39m#end simulation\u001b[39;00m\n\u001b[0;32m    171\u001b[0m clear_output(wait\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m    172\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m** Current Time: [bold dark_violet]\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(\u001b[39mround\u001b[39m((\u001b[39m1\u001b[39m\u001b[39m/\u001b[39m\u001b[39m900\u001b[39m)\u001b[39m*\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcurrent_epoch),\u001b[39m2\u001b[39m)) \u001b[39m+\u001b[39m \n\u001b[0;32m    173\u001b[0m       \u001b[39m\"\u001b[39m\u001b[39m[/bold dark_violet] hrs, Epoch Left: [bold dark_violet]\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(\u001b[39mround\u001b[39m((\u001b[39m1\u001b[39m\u001b[39m/\u001b[39m\u001b[39m900\u001b[39m)\u001b[39m*\u001b[39m(epochs_left),\u001b[39m2\u001b[39m)) \u001b[39m+\u001b[39m \n\u001b[0;32m    174\u001b[0m       \u001b[39m\"\u001b[39m\u001b[39m[/bold dark_violet] hrs, **\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m[bold dark_green]Last Action: \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(suggested_action) \u001b[39m+\u001b[39m \n\u001b[0;32m    175\u001b[0m       \u001b[39m\"\u001b[39m\u001b[39m[/bold dark_green],\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m[bold dark_green]Current Homeostatic Variable: \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(\u001b[39mround\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39minternal_state, \u001b[39m4\u001b[39m)) \u001b[39m+\u001b[39m \n\u001b[0;32m    176\u001b[0m       \u001b[39m\"\u001b[39m\u001b[39m[/bold dark_green], [bold dark_green]Current Homeostatic Setpoint: \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(\u001b[39mround\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msetpoint_S, \u001b[39m4\u001b[39m)) \u001b[39m+\u001b[39m\n\u001b[0;32m    177\u001b[0m       \u001b[39m\"\u001b[39m\u001b[39m[/bold dark_green],\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m [bold dark_green]Reward Received: \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(\u001b[39mround\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreward_received,\u001b[39m2\u001b[39m)) \u001b[39m+\u001b[39m \n\u001b[0;32m    178\u001b[0m       \u001b[39m\"\u001b[39m\u001b[39m[/bold dark_green], [bold dark_green]Total Score: \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(\u001b[39mround\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtotal_reward[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m],\u001b[39m2\u001b[39m)) \u001b[39m+\u001b[39m \n\u001b[1;32m--> 179\u001b[0m       \u001b[39m\"\u001b[39m\u001b[39m[/bold dark_green],\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m [bold dark_green]Q-Values: \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(qValues[\u001b[39m0\u001b[39;49m]) \u001b[39m+\u001b[39m \n\u001b[0;32m    180\u001b[0m       \u001b[39m\"\u001b[39m\u001b[39m[/bold dark_green],\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m [bold dark_green]T-Probs: \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(transitionProbs[\u001b[39m0\u001b[39m]) \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m[/bold dark_green]\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    181\u001b[0m \u001b[39m#self.df.loc[self.current_epoch] = [self.current_epoch , suggested_action, qValues[0][0], qValues[0][1], qValues[0][2], transitionProbs[0][0], transitionProbs[0][1], transitionProbs[0][2], self.epochs_inactive, self.internal_state, self.setpoint_S, self.reward_received, self.total_reward[-1]]\u001b[39;00m\n\u001b[0;32m    182\u001b[0m \n\u001b[0;32m    183\u001b[0m \u001b[39m#check if this is the last round otherwise continue\u001b[39;00m\n\u001b[0;32m    184\u001b[0m \u001b[39mif\u001b[39;00m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcurrent_epoch \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtotal_epochs):\n",
      "\u001b[1;31mIndexError\u001b[0m: invalid index to scalar variable."
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    env = Simulation() "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Dataset\n",
    "## View Synthetic Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epoch</th>\n",
       "      <th>action</th>\n",
       "      <th>q_value_0</th>\n",
       "      <th>q_value_1</th>\n",
       "      <th>q_value_2</th>\n",
       "      <th>t_prob_0</th>\n",
       "      <th>t_prob_1</th>\n",
       "      <th>t_prob_2</th>\n",
       "      <th>epochs_inactive</th>\n",
       "      <th>internal_variable</th>\n",
       "      <th>homeostatic_setpoint</th>\n",
       "      <th>reward</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>500</th>\n",
       "      <td>500</td>\n",
       "      <td>Do Nothing</td>\n",
       "      <td>-0.000017</td>\n",
       "      <td>-24.261391</td>\n",
       "      <td>-17.384188</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>199.91984</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>501</th>\n",
       "      <td>501</td>\n",
       "      <td>Do Nothing</td>\n",
       "      <td>-0.000018</td>\n",
       "      <td>-24.261374</td>\n",
       "      <td>-17.384176</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>199.91968</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>502</th>\n",
       "      <td>502</td>\n",
       "      <td>Do Nothing</td>\n",
       "      <td>-0.000021</td>\n",
       "      <td>-24.261353</td>\n",
       "      <td>-17.384159</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>199.91952</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>503</th>\n",
       "      <td>503</td>\n",
       "      <td>Do Nothing</td>\n",
       "      <td>-0.000025</td>\n",
       "      <td>-24.261337</td>\n",
       "      <td>-17.384146</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>199.91936</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>504</th>\n",
       "      <td>504</td>\n",
       "      <td>Do Nothing</td>\n",
       "      <td>-0.000037</td>\n",
       "      <td>-24.261318</td>\n",
       "      <td>-17.384132</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>199.91920</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>505</th>\n",
       "      <td>505</td>\n",
       "      <td>Do Nothing</td>\n",
       "      <td>-0.000046</td>\n",
       "      <td>-24.261299</td>\n",
       "      <td>-17.384119</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>199.91904</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>506</th>\n",
       "      <td>506</td>\n",
       "      <td>Do Nothing</td>\n",
       "      <td>-0.000058</td>\n",
       "      <td>-24.261284</td>\n",
       "      <td>-17.384106</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>199.91888</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>507</th>\n",
       "      <td>507</td>\n",
       "      <td>Do Nothing</td>\n",
       "      <td>-0.000065</td>\n",
       "      <td>-24.261265</td>\n",
       "      <td>-17.384090</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>199.91872</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>508</th>\n",
       "      <td>508</td>\n",
       "      <td>Do Nothing</td>\n",
       "      <td>-0.000074</td>\n",
       "      <td>-24.261246</td>\n",
       "      <td>-17.384077</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>199.91856</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>509</th>\n",
       "      <td>509</td>\n",
       "      <td>Do Nothing</td>\n",
       "      <td>-0.000076</td>\n",
       "      <td>-24.261227</td>\n",
       "      <td>-17.384066</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>199.91840</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>510</th>\n",
       "      <td>510</td>\n",
       "      <td>Do Nothing</td>\n",
       "      <td>-0.000079</td>\n",
       "      <td>-24.261206</td>\n",
       "      <td>-17.384050</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>199.91824</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>511</th>\n",
       "      <td>511</td>\n",
       "      <td>Do Nothing</td>\n",
       "      <td>-0.000079</td>\n",
       "      <td>-24.261189</td>\n",
       "      <td>-17.384039</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>199.91808</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>512</th>\n",
       "      <td>512</td>\n",
       "      <td>Do Nothing</td>\n",
       "      <td>-0.000078</td>\n",
       "      <td>-24.261166</td>\n",
       "      <td>-17.384026</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>199.91792</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>513</th>\n",
       "      <td>513</td>\n",
       "      <td>Do Nothing</td>\n",
       "      <td>-0.000072</td>\n",
       "      <td>-24.261147</td>\n",
       "      <td>-17.384012</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>199.91776</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>514</th>\n",
       "      <td>514</td>\n",
       "      <td>Do Nothing</td>\n",
       "      <td>-0.000068</td>\n",
       "      <td>-24.261127</td>\n",
       "      <td>-17.383997</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>199.91760</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>515</th>\n",
       "      <td>515</td>\n",
       "      <td>Do Nothing</td>\n",
       "      <td>-0.000063</td>\n",
       "      <td>-24.261108</td>\n",
       "      <td>-17.383982</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>199.91744</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>516</th>\n",
       "      <td>516</td>\n",
       "      <td>Do Nothing</td>\n",
       "      <td>-0.000055</td>\n",
       "      <td>-24.261089</td>\n",
       "      <td>-17.383968</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>199.91728</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>517</th>\n",
       "      <td>517</td>\n",
       "      <td>Do Nothing</td>\n",
       "      <td>-0.000051</td>\n",
       "      <td>-24.261074</td>\n",
       "      <td>-17.383953</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>199.91712</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>518</th>\n",
       "      <td>518</td>\n",
       "      <td>Do Nothing</td>\n",
       "      <td>-0.000048</td>\n",
       "      <td>-24.261051</td>\n",
       "      <td>-17.383940</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>199.91696</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>519</th>\n",
       "      <td>519</td>\n",
       "      <td>Do Nothing</td>\n",
       "      <td>-0.000046</td>\n",
       "      <td>-24.261036</td>\n",
       "      <td>-17.383924</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>199.91680</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>520</th>\n",
       "      <td>520</td>\n",
       "      <td>Do Nothing</td>\n",
       "      <td>-0.000047</td>\n",
       "      <td>-24.261013</td>\n",
       "      <td>-17.383911</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>199.91664</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>521</th>\n",
       "      <td>521</td>\n",
       "      <td>Do Nothing</td>\n",
       "      <td>-0.000048</td>\n",
       "      <td>-24.260994</td>\n",
       "      <td>-17.383900</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>199.91648</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>522</th>\n",
       "      <td>522</td>\n",
       "      <td>Do Nothing</td>\n",
       "      <td>-0.000050</td>\n",
       "      <td>-24.260979</td>\n",
       "      <td>-17.383886</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>199.91632</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>523</th>\n",
       "      <td>523</td>\n",
       "      <td>Do Nothing</td>\n",
       "      <td>-0.000046</td>\n",
       "      <td>-24.260956</td>\n",
       "      <td>-17.383871</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>199.91616</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>524</th>\n",
       "      <td>524</td>\n",
       "      <td>Do Nothing</td>\n",
       "      <td>-0.000048</td>\n",
       "      <td>-24.260939</td>\n",
       "      <td>-17.383860</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>199.91600</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>525</th>\n",
       "      <td>525</td>\n",
       "      <td>Do Nothing</td>\n",
       "      <td>-0.000043</td>\n",
       "      <td>-24.260921</td>\n",
       "      <td>-17.383844</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>199.91584</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>526</th>\n",
       "      <td>526</td>\n",
       "      <td>Do Nothing</td>\n",
       "      <td>-0.000038</td>\n",
       "      <td>-24.260904</td>\n",
       "      <td>-17.383829</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>199.91568</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>527</th>\n",
       "      <td>527</td>\n",
       "      <td>Do Nothing</td>\n",
       "      <td>-0.000030</td>\n",
       "      <td>-24.260883</td>\n",
       "      <td>-17.383816</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>199.91552</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>528</th>\n",
       "      <td>528</td>\n",
       "      <td>Do Nothing</td>\n",
       "      <td>-0.000027</td>\n",
       "      <td>-24.260864</td>\n",
       "      <td>-17.383804</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>199.91536</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>529</th>\n",
       "      <td>529</td>\n",
       "      <td>Do Nothing</td>\n",
       "      <td>-0.000016</td>\n",
       "      <td>-24.260845</td>\n",
       "      <td>-17.383787</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>199.91520</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>530</th>\n",
       "      <td>530</td>\n",
       "      <td>Do Nothing</td>\n",
       "      <td>-0.000012</td>\n",
       "      <td>-24.260828</td>\n",
       "      <td>-17.383776</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>199.91504</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>531</th>\n",
       "      <td>531</td>\n",
       "      <td>Do Nothing</td>\n",
       "      <td>-0.000009</td>\n",
       "      <td>-24.260807</td>\n",
       "      <td>-17.383760</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>199.91488</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>532</th>\n",
       "      <td>532</td>\n",
       "      <td>Do Nothing</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>-24.260796</td>\n",
       "      <td>-17.383745</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>199.91472</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>533</th>\n",
       "      <td>533</td>\n",
       "      <td>Do Nothing</td>\n",
       "      <td>-0.000015</td>\n",
       "      <td>-24.260769</td>\n",
       "      <td>-17.383736</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>199.91456</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>534</th>\n",
       "      <td>534</td>\n",
       "      <td>Do Nothing</td>\n",
       "      <td>-0.000024</td>\n",
       "      <td>-24.260754</td>\n",
       "      <td>-17.383720</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>199.91440</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>535</th>\n",
       "      <td>535</td>\n",
       "      <td>Do Nothing</td>\n",
       "      <td>-0.000036</td>\n",
       "      <td>-24.260735</td>\n",
       "      <td>-17.383703</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>199.91424</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>536</th>\n",
       "      <td>536</td>\n",
       "      <td>Do Nothing</td>\n",
       "      <td>-0.000045</td>\n",
       "      <td>-24.260715</td>\n",
       "      <td>-17.383694</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>199.91408</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>537</th>\n",
       "      <td>537</td>\n",
       "      <td>Do Nothing</td>\n",
       "      <td>-0.000054</td>\n",
       "      <td>-24.260696</td>\n",
       "      <td>-17.383680</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>199.91392</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>538</th>\n",
       "      <td>538</td>\n",
       "      <td>Do Nothing</td>\n",
       "      <td>-0.000060</td>\n",
       "      <td>-24.260681</td>\n",
       "      <td>-17.383665</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>199.91376</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>539</th>\n",
       "      <td>539</td>\n",
       "      <td>Do Nothing</td>\n",
       "      <td>-0.000065</td>\n",
       "      <td>-24.260658</td>\n",
       "      <td>-17.383650</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>199.91360</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>540</th>\n",
       "      <td>540</td>\n",
       "      <td>Do Nothing</td>\n",
       "      <td>-0.000067</td>\n",
       "      <td>-24.260643</td>\n",
       "      <td>-17.383636</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>199.91344</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>541</th>\n",
       "      <td>541</td>\n",
       "      <td>Do Nothing</td>\n",
       "      <td>-0.000071</td>\n",
       "      <td>-24.260616</td>\n",
       "      <td>-17.383623</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>199.91328</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>542</th>\n",
       "      <td>542</td>\n",
       "      <td>Do Nothing</td>\n",
       "      <td>-0.000071</td>\n",
       "      <td>-24.260605</td>\n",
       "      <td>-17.383612</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>199.91312</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>543</th>\n",
       "      <td>543</td>\n",
       "      <td>Do Nothing</td>\n",
       "      <td>-0.000076</td>\n",
       "      <td>-24.260586</td>\n",
       "      <td>-17.383596</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>199.91296</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>544</th>\n",
       "      <td>544</td>\n",
       "      <td>Do Nothing</td>\n",
       "      <td>-0.000075</td>\n",
       "      <td>-24.260571</td>\n",
       "      <td>-17.383583</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>199.91280</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>545</th>\n",
       "      <td>545</td>\n",
       "      <td>Do Nothing</td>\n",
       "      <td>-0.000073</td>\n",
       "      <td>-24.260548</td>\n",
       "      <td>-17.383566</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>199.91264</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>546</th>\n",
       "      <td>546</td>\n",
       "      <td>Do Nothing</td>\n",
       "      <td>-0.000063</td>\n",
       "      <td>-24.260529</td>\n",
       "      <td>-17.383554</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>199.91248</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>547</th>\n",
       "      <td>547</td>\n",
       "      <td>Do Nothing</td>\n",
       "      <td>-0.000058</td>\n",
       "      <td>-24.260509</td>\n",
       "      <td>-17.383539</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>199.91232</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>548</th>\n",
       "      <td>548</td>\n",
       "      <td>Do Nothing</td>\n",
       "      <td>-0.000054</td>\n",
       "      <td>-24.260492</td>\n",
       "      <td>-17.383528</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>199.91216</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>549</th>\n",
       "      <td>549</td>\n",
       "      <td>Do Nothing</td>\n",
       "      <td>-0.000049</td>\n",
       "      <td>-24.260471</td>\n",
       "      <td>-17.383512</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>199.91200</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     epoch      action  q_value_0  q_value_1  q_value_2  t_prob_0  t_prob_1  \\\n",
       "500    500  Do Nothing  -0.000017 -24.261391 -17.384188       1.0       0.0   \n",
       "501    501  Do Nothing  -0.000018 -24.261374 -17.384176       1.0       0.0   \n",
       "502    502  Do Nothing  -0.000021 -24.261353 -17.384159       1.0       0.0   \n",
       "503    503  Do Nothing  -0.000025 -24.261337 -17.384146       1.0       0.0   \n",
       "504    504  Do Nothing  -0.000037 -24.261318 -17.384132       1.0       0.0   \n",
       "505    505  Do Nothing  -0.000046 -24.261299 -17.384119       1.0       0.0   \n",
       "506    506  Do Nothing  -0.000058 -24.261284 -17.384106       1.0       0.0   \n",
       "507    507  Do Nothing  -0.000065 -24.261265 -17.384090       1.0       0.0   \n",
       "508    508  Do Nothing  -0.000074 -24.261246 -17.384077       1.0       0.0   \n",
       "509    509  Do Nothing  -0.000076 -24.261227 -17.384066       1.0       0.0   \n",
       "510    510  Do Nothing  -0.000079 -24.261206 -17.384050       1.0       0.0   \n",
       "511    511  Do Nothing  -0.000079 -24.261189 -17.384039       1.0       0.0   \n",
       "512    512  Do Nothing  -0.000078 -24.261166 -17.384026       1.0       0.0   \n",
       "513    513  Do Nothing  -0.000072 -24.261147 -17.384012       1.0       0.0   \n",
       "514    514  Do Nothing  -0.000068 -24.261127 -17.383997       1.0       0.0   \n",
       "515    515  Do Nothing  -0.000063 -24.261108 -17.383982       1.0       0.0   \n",
       "516    516  Do Nothing  -0.000055 -24.261089 -17.383968       1.0       0.0   \n",
       "517    517  Do Nothing  -0.000051 -24.261074 -17.383953       1.0       0.0   \n",
       "518    518  Do Nothing  -0.000048 -24.261051 -17.383940       1.0       0.0   \n",
       "519    519  Do Nothing  -0.000046 -24.261036 -17.383924       1.0       0.0   \n",
       "520    520  Do Nothing  -0.000047 -24.261013 -17.383911       1.0       0.0   \n",
       "521    521  Do Nothing  -0.000048 -24.260994 -17.383900       1.0       0.0   \n",
       "522    522  Do Nothing  -0.000050 -24.260979 -17.383886       1.0       0.0   \n",
       "523    523  Do Nothing  -0.000046 -24.260956 -17.383871       1.0       0.0   \n",
       "524    524  Do Nothing  -0.000048 -24.260939 -17.383860       1.0       0.0   \n",
       "525    525  Do Nothing  -0.000043 -24.260921 -17.383844       1.0       0.0   \n",
       "526    526  Do Nothing  -0.000038 -24.260904 -17.383829       1.0       0.0   \n",
       "527    527  Do Nothing  -0.000030 -24.260883 -17.383816       1.0       0.0   \n",
       "528    528  Do Nothing  -0.000027 -24.260864 -17.383804       1.0       0.0   \n",
       "529    529  Do Nothing  -0.000016 -24.260845 -17.383787       1.0       0.0   \n",
       "530    530  Do Nothing  -0.000012 -24.260828 -17.383776       1.0       0.0   \n",
       "531    531  Do Nothing  -0.000009 -24.260807 -17.383760       1.0       0.0   \n",
       "532    532  Do Nothing  -0.000008 -24.260796 -17.383745       1.0       0.0   \n",
       "533    533  Do Nothing  -0.000015 -24.260769 -17.383736       1.0       0.0   \n",
       "534    534  Do Nothing  -0.000024 -24.260754 -17.383720       1.0       0.0   \n",
       "535    535  Do Nothing  -0.000036 -24.260735 -17.383703       1.0       0.0   \n",
       "536    536  Do Nothing  -0.000045 -24.260715 -17.383694       1.0       0.0   \n",
       "537    537  Do Nothing  -0.000054 -24.260696 -17.383680       1.0       0.0   \n",
       "538    538  Do Nothing  -0.000060 -24.260681 -17.383665       1.0       0.0   \n",
       "539    539  Do Nothing  -0.000065 -24.260658 -17.383650       1.0       0.0   \n",
       "540    540  Do Nothing  -0.000067 -24.260643 -17.383636       1.0       0.0   \n",
       "541    541  Do Nothing  -0.000071 -24.260616 -17.383623       1.0       0.0   \n",
       "542    542  Do Nothing  -0.000071 -24.260605 -17.383612       1.0       0.0   \n",
       "543    543  Do Nothing  -0.000076 -24.260586 -17.383596       1.0       0.0   \n",
       "544    544  Do Nothing  -0.000075 -24.260571 -17.383583       1.0       0.0   \n",
       "545    545  Do Nothing  -0.000073 -24.260548 -17.383566       1.0       0.0   \n",
       "546    546  Do Nothing  -0.000063 -24.260529 -17.383554       1.0       0.0   \n",
       "547    547  Do Nothing  -0.000058 -24.260509 -17.383539       1.0       0.0   \n",
       "548    548  Do Nothing  -0.000054 -24.260492 -17.383528       1.0       0.0   \n",
       "549    549  Do Nothing  -0.000049 -24.260471 -17.383512       1.0       0.0   \n",
       "\n",
       "     t_prob_2  epochs_inactive  internal_variable  homeostatic_setpoint  \\\n",
       "500       0.0                4                0.0             199.91984   \n",
       "501       0.0                0                0.0             199.91968   \n",
       "502       0.0                1                0.0             199.91952   \n",
       "503       0.0                2                0.0             199.91936   \n",
       "504       0.0                3                0.0             199.91920   \n",
       "505       0.0                4                0.0             199.91904   \n",
       "506       0.0                0                0.0             199.91888   \n",
       "507       0.0                1                0.0             199.91872   \n",
       "508       0.0                2                0.0             199.91856   \n",
       "509       0.0                3                0.0             199.91840   \n",
       "510       0.0                4                0.0             199.91824   \n",
       "511       0.0                0                0.0             199.91808   \n",
       "512       0.0                1                0.0             199.91792   \n",
       "513       0.0                2                0.0             199.91776   \n",
       "514       0.0                3                0.0             199.91760   \n",
       "515       0.0                4                0.0             199.91744   \n",
       "516       0.0                0                0.0             199.91728   \n",
       "517       0.0                1                0.0             199.91712   \n",
       "518       0.0                2                0.0             199.91696   \n",
       "519       0.0                3                0.0             199.91680   \n",
       "520       0.0                4                0.0             199.91664   \n",
       "521       0.0                0                0.0             199.91648   \n",
       "522       0.0                1                0.0             199.91632   \n",
       "523       0.0                2                0.0             199.91616   \n",
       "524       0.0                3                0.0             199.91600   \n",
       "525       0.0                4                0.0             199.91584   \n",
       "526       0.0                0                0.0             199.91568   \n",
       "527       0.0                1                0.0             199.91552   \n",
       "528       0.0                2                0.0             199.91536   \n",
       "529       0.0                3                0.0             199.91520   \n",
       "530       0.0                4                0.0             199.91504   \n",
       "531       0.0                0                0.0             199.91488   \n",
       "532       0.0                1                0.0             199.91472   \n",
       "533       0.0                2                0.0             199.91456   \n",
       "534       0.0                3                0.0             199.91440   \n",
       "535       0.0                4                0.0             199.91424   \n",
       "536       0.0                0                0.0             199.91408   \n",
       "537       0.0                1                0.0             199.91392   \n",
       "538       0.0                2                0.0             199.91376   \n",
       "539       0.0                3                0.0             199.91360   \n",
       "540       0.0                4                0.0             199.91344   \n",
       "541       0.0                0                0.0             199.91328   \n",
       "542       0.0                1                0.0             199.91312   \n",
       "543       0.0                2                0.0             199.91296   \n",
       "544       0.0                3                0.0             199.91280   \n",
       "545       0.0                4                0.0             199.91264   \n",
       "546       0.0                0                0.0             199.91248   \n",
       "547       0.0                1                0.0             199.91232   \n",
       "548       0.0                2                0.0             199.91216   \n",
       "549       0.0                3                0.0             199.91200   \n",
       "\n",
       "     reward  score  \n",
       "500     0.0    0.0  \n",
       "501     0.0    0.0  \n",
       "502     0.0    0.0  \n",
       "503     0.0    0.0  \n",
       "504     0.0    0.0  \n",
       "505     0.0    0.0  \n",
       "506     0.0    0.0  \n",
       "507     0.0    0.0  \n",
       "508     0.0    0.0  \n",
       "509     0.0    0.0  \n",
       "510     0.0    0.0  \n",
       "511     0.0    0.0  \n",
       "512     0.0    0.0  \n",
       "513     0.0    0.0  \n",
       "514     0.0    0.0  \n",
       "515     0.0    0.0  \n",
       "516     0.0    0.0  \n",
       "517     0.0    0.0  \n",
       "518     0.0    0.0  \n",
       "519     0.0    0.0  \n",
       "520     0.0    0.0  \n",
       "521     0.0    0.0  \n",
       "522     0.0    0.0  \n",
       "523     0.0    0.0  \n",
       "524     0.0    0.0  \n",
       "525     0.0    0.0  \n",
       "526     0.0    0.0  \n",
       "527     0.0    0.0  \n",
       "528     0.0    0.0  \n",
       "529     0.0    0.0  \n",
       "530     0.0    0.0  \n",
       "531     0.0    0.0  \n",
       "532     0.0    0.0  \n",
       "533     0.0    0.0  \n",
       "534     0.0    0.0  \n",
       "535     0.0    0.0  \n",
       "536     0.0    0.0  \n",
       "537     0.0    0.0  \n",
       "538     0.0    0.0  \n",
       "539     0.0    0.0  \n",
       "540     0.0    0.0  \n",
       "541     0.0    0.0  \n",
       "542     0.0    0.0  \n",
       "543     0.0    0.0  \n",
       "544     0.0    0.0  \n",
       "545     0.0    0.0  \n",
       "546     0.0    0.0  \n",
       "547     0.0    0.0  \n",
       "548     0.0    0.0  \n",
       "549     0.0    0.0  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#env.df.head(250)\n",
    "#env.df.iloc[100:150] #when hit is taken\n",
    "env.df.iloc[500:550]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = datetime.datetime.now()\n",
    "file_name = str(x.year) + \"_\" + str(x.month) + \"_\" + str(x.day) + \"_\" + str(x.strftime(\"%H\")) + \"_\" + str(x.strftime(\"%M\")) + \"_\" + str(x.strftime(\"%S\")) + \".csv\"\n",
    "env.df.to_csv(file_name, index=False, encoding='utf-8')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_research",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5794a404577a32ff418895c331e736516539893dd666baf27c4118b4263cef3b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
