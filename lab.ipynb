{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Linear Regression Lab\n",
    "The assignments below should help you to play around and 'break' the examples in Linear Regression.ipynb. \n",
    "\n",
    "1. Create your own examples using max daily temperature versus day of year. \n",
    "2. Run your data through modelling and visualisation. \n",
    "3. Make some predictions and compare your predictions to what you might expect according to your data points. \n",
    "\n",
    "If you can do all of that, create yourself a multivariate example based on Squirrel Weight as a function of day of year and age! If you don't like that one, think of something that you think would be a more interesting multivariate regression topic that you can code up quickly. \n",
    "\n",
    "By the end of week 2, you should be able to load up notebooks onto Colab or your own chosen notebook server, plus you should have a good python install available on your machine which you can use with venv and pip to install tensorflow. \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Logistic Regression Lab\n",
    "* Come up with your own classification task that has a clear binary distinction based on two input variables – you will need to use the multivariate code to work through this model. \n",
    "* Can you come up with a pattern that is clearly a pattern, but which provides the training algorithm a little bit more challenge in coming up with a clear classification boundary? \n",
    "* Using python on your own laptop, take my own code for the basic learn regression model and get it working as a simple python (.py) chunk of code rather than a jupyter notebook\n",
    "\n",
    "* Add extra training cases (variables + target) for the diabetes case\n",
    "* Manipulate the parameters associated with the examples\n",
    "* Extend the examples to use multiple input variables\n",
    "* Download and execute the code in the noteboook as python code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Neural Network Representation Lab\n",
    "Create a Jupyter notebook called “week 04 work”. \n",
    "For each of the items below complete the work it its own cell. We will refer to the contents of the cell as a “programme” just because it is a natural way of talking.  The contents of a single cell are not a programme in the normal sense of the word of course. \n",
    "1. Write a programme to print out “Hello network”\n",
    "Write a programme that creates a python array with the integers 0 to 9 in it, and then prints them\n",
    "2. Write a programme that creates a numpy vector with the integers 0 to 9 in it and then try to print the vector\n",
    "3. Write a programme to print out the cube of each element in the array created in item 3. Try doing this with a loop.\n",
    "4. Write a programme to print out the cube of each element in the vector created in item 4. Try doing this with vector notation. \n",
    "5. Write a function name my_fun that takes two arguments as input. The function should assume that the first item passed into the function is a vector and the second item is a scalar. The function should return a new vector which is the vector raised to the power of the scalar. Test the function with your own selection of vectors and scalars. \n",
    "6. Write a function named my_fun2 that takes as input a single numpy vector and returns the sum of the squares of that vector. Test with input [1,1,1,1] and [2,2,2,2]. \n",
    "7. Write a function that takes as input a vector and a scalar. The function should return the sum of the square of the difference between each individual item and the scalar. Test with your own data. \n",
    "8. Write a function that takes a vector as input and returns the log of each item in the vector as another vector. Test with your own data. Look in particular at the results you get for inputs such as 0, 0.5, 1, 1.5. \n",
    "9. Test the function created in point 10 with a sequence of numbers 0.1, 0.2, 0.3, …. 2.0. Use matplotlib to plot the data with a line plot where the input numbers are the X axis and the returned values give the Y axis. \n",
    "10. Using your own intuitions and trial and error, give parameters Theta0 and Theta1 for the line below. Theta0 is c and Theta1 is m in the traditional highschool equation y=mx+c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Neural Network Training Lab\n",
    "1. Questions\n",
    "* For a given input, do we always compute the same output?\n",
    "* What are the parameters of the network?\n",
    "* What are the hyperparameters of the network? \n",
    "* Given a layer of the network where the weights are all = 0, what can we say about the logit values of the following layer? \n",
    "\n",
    "2. Backprop\n",
    "* What is the relationship between Gradient Descent and Backprop? \n",
    "* Do we need to calculate error for the backprop algorithm? \n",
    "* What impact does backprop have on the input layer? \n",
    "* What does it mean if a weight after training has a value close to 0 (relatively speaking)? \n",
    "* Discuss the interaction between the backprop algorithm and network baiases. \n",
    "\n",
    "3. Loss Functions\n",
    "* Investigate a loss function (look at Keras losses as a starting point) (10 mins), in a Keras context (10 mins)\n",
    "* Details on the losses investigated. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Improving Performance Lab\n",
    "* Use CIFAR-10 to investigate Dropout and Early Stopping.\n",
    "* Investigate Leaky RELU\n",
    "* Investigate clipping for RELU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.8.10 (tags/v3.8.10:3d8993a, May  3 2021, 11:48:03) [MSC v.1928 64 bit (AMD64)]\n",
      "2.11.0\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import tensorflow as tf\n",
    "print(sys.version)\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential()\n",
    "model.add(hub_layer)\n",
    "model.add(tf.keras.layers.Dense(256, activation='relu'))\n",
    "\n",
    "model.add(tf.keras.layers.Dense(256, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(1))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "hist = model.fit(train_data.shuffle(10000).batch(512),\n",
    "                    epochs=num_epochs,\n",
    "                    validation_data=validation_data.batch(512),\n",
    "                    verbose=0)\n",
    "\n",
    "# Visualize Costs over Epochs \n",
    "plt.plot(hist.history['loss'],label=\"training loss\")\n",
    "plt.plot(hist.history['val_loss'],label=\"validation loss\")\n",
    "plt.ylabel(\"Cost\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.show()\n",
    "\n",
    "results = model.evaluate(test_data.batch(512), verbose=2)\n",
    "\n",
    "for name, value in zip(model.metrics_names, results):\n",
    "  print(\"%s: %.3f\" % (name, value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.3600 - accuracy: 0.9029\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1809 - accuracy: 0.9477\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.1326 - accuracy: 0.9606\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.1049 - accuracy: 0.9693\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0866 - accuracy: 0.9743\n",
      "Model: \"sequential_18\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_18 (Flatten)        (32, 784)                 0         \n",
      "                                                                 \n",
      " dense_43 (Dense)            (32, 64)                  50240     \n",
      "                                                                 \n",
      " dense_44 (Dense)            (32, 10)                  650       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 50,890\n",
      "Trainable params: 50,890\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "\n",
      "Test accuracy: 96.82000279426575%\n"
     ]
    }
   ],
   "source": [
    "# Get a copy of the mnist dataset container \n",
    "mnist = tf.keras.datasets.mnist \n",
    "\n",
    "# Pull out the training and test data \n",
    "(x_train, y_train),(x_test, y_test) = mnist.load_data() \n",
    "\n",
    "# Normalize the training and test datasets\n",
    "x_train = tf.keras.utils.normalize(x_train, axis=1)\n",
    "x_test = tf.keras.utils.normalize(x_test, axis=1)\n",
    "\n",
    "# Create a simple sequential network object\n",
    "model = tf.keras.models.Sequential()\n",
    "\n",
    "# Add layers to the network for processing the input data \n",
    "model.add(tf.keras.layers.Flatten())\n",
    "model.add(tf.keras.layers.Dense(64, activation=tf.nn.relu))\n",
    "#model.add(tf.keras.layers.Dropout(0.3)),\n",
    "model.add(tf.keras.layers.Dense(10, activation=tf.nn.softmax))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Start the training process\n",
    "model.fit(x=x_train, y=y_train, epochs=5) \n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "# Evaluate the model performance with test data\n",
    "test_loss, test_acc = model.evaluate(x=x_test, y=y_test,verbose=0)\n",
    "\n",
    "# Print out the model accuracy \n",
    "print('\\nTest accuracy: ' + str(test_acc*100) + \"%\" )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "43971d533993891f6333e161f7c74c61830d17fc4458297799e138bc989c60e2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
