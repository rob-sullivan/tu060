{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explaining Deep Q-Learning Experience Replay with SHapley Additive exPlanations\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://gymnasium.farama.org/environments/atari/\n",
    "#pip install gymnasium[atari]\n",
    "#pip install gymnasium[accept-rom-license]\n",
    "#pip install moviepy\n",
    "#https://www.youtube.com/watch?v=hCeJeq8U0lo&t=447s\n",
    "import datetime\n",
    "\n",
    "#test environments\n",
    "import gymnasium as gym\n",
    "\n",
    "#image preprocessing\n",
    "from PIL import Image\n",
    "from gymnasium.core import ObservationWrapper\n",
    "from gymnasium.spaces.box import Box\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.ioff() #prevent plots from being displayed automatically in the notebook\n",
    "\n",
    "#n-step experience replay\n",
    "from collections import namedtuple, deque\n",
    "\n",
    "#deep learning\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from gymnasium.wrappers import RecordVideo #enviornment monitoring\n",
    "import random\n",
    "\n",
    "from sklearn.model_selection import train_test_split #for splitting memory into training and testing\n",
    "import pandas as pd\n",
    "\n",
    "import shap # explaining deep q learning model\n",
    "from shap.plots._image import image as image_plotter\n",
    "from shap.plots import colors\n",
    "import math\n",
    "import datetime #logging experiment time\n",
    "\n",
    "from PIL import Image #converting state test images into shap inputs\n",
    "import matplotlib.gridspec as gridspec #displaying shap graphs with states and q values\n",
    "\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.stats.multicomp as mc\n",
    "from statsmodels.formula.api import ols\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "  device = torch.device('cuda:0')\n",
    "  print('Running on the GPU')\n",
    "else:\n",
    "  device = torch.device('cpu')\n",
    "  print('Running on the CPU')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Convelutional Q-Learning Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Convelutional Neural Network\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, number_actions):\n",
    "        super(CNN, self).__init__() # call nn module init\n",
    "        #define what each layer in CNN is\n",
    "        self.convolution1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=5)\n",
    "        self.convolution2 = nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3)\n",
    "        self.convolution3 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=2)\n",
    "\n",
    "        #pass image through convolution layers and get neurons in a flatten layer to pass into a neural network\n",
    "        self.fc1 = nn.Linear(in_features=self.count_neurons((1, 80, 80)), out_features=40) #1 is number of channels so black and white images, 80 80 is width and height\n",
    "        self.fc2 = nn.Linear(in_features=40, out_features=number_actions)\n",
    "\n",
    "    def count_neurons(self, image_dim):#image_dim for example 80px x 80px in size\n",
    "        \"\"\"Will give us the number of neurons after convolutions are applied\"\"\"\n",
    "        #we need to first create a fake image (1 batch, 80px x 80px in size), * allows image_dim to be passed as a list\n",
    "        fake_image = Variable(torch.rand(1, *image_dim))\n",
    "\n",
    "        #pass image into first layer and max pool result then activate all neurons in max pool layer\n",
    "        x = F.relu(F.max_pool2d(self.convolution1(fake_image), 3, 2)) #kernal size is 3, #stride is 2\n",
    "\n",
    "        #pass image into second layer and max pool result then activate all neurons in max pool layer\n",
    "        x = F.relu(F.max_pool2d(self.convolution2(x), 3, 2)) #kernal size is 3, #stride is 2\n",
    "\n",
    "        #pass image into third layer and max pool result then activate all neurons in max pool layer\n",
    "        x = F.relu(F.max_pool2d(self.convolution3(x), 3, 2)) #kernal size is 3, #stride is 2\n",
    "        \n",
    "        #now we get all pixels in third layer and flatten it. we get the data, view what's inside it then we get all the pixels and put it into 1 dimension\n",
    "        return x.data.view(1, -1).size(1)\n",
    "    \n",
    "    def forward(self, x):       \n",
    "        x = F.relu(F.max_pool2d(self.convolution1(x), 3, 2))\n",
    "        x = F.relu(F.max_pool2d(self.convolution2(x), 3, 2))\n",
    "        x = F.relu(F.max_pool2d(self.convolution3(x), 3, 2))\n",
    "        #propagate data from convolutional layers to hidden layers by first flattening convolutional layers\n",
    "        #flatten third layer by taking all pixels and all channels in third layer and arrange one after another\n",
    "        x = x.view(x.size(0), -1) #RuntimeError: mat1 and mat2 shapes cannot be multiplied (64x49 and 3136x40)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "### Softmax Policy\n",
    "class SoftmaxPolicy(nn.Module):\n",
    "    \"\"\"data from the CNN is passed to softmax to play an action.\n",
    "    Temperature (often denoted as Ï„ or tau) is a hyperparameter that controls the level of randomness or exploration in the action selection process. \n",
    "    - High T values (e.g., > 5): A high temperature encourages a high level of exploration and randomness in action selection. This can be useful when you want the agent to explore a wide range of actions to discover their effects and learn about the environment.\n",
    "    - Moderate T values (e.g., 1 - 5): A moderate temperature strikes a balance between exploration and exploitation. It allows the agent to favor actions with higher Q-values while still exploring other options. \n",
    "    - Low T values (e.g., < 1): A low temperature reduces the randomness in action selection, making the agent more deterministic and focused on exploiting actions with higher Q-values. This can be useful when the agent has learned a relatively good policy and you want to minimize unnecessary exploration.\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, T=10):\n",
    "        super(SoftmaxPolicy, self).__init__()\n",
    "        self.T = T\n",
    "\n",
    "    def forward(self, outputs, number_actions=1):\n",
    "        probs = F.softmax(outputs * self.T, dim=1)\n",
    "        actions = probs.multinomial(num_samples=number_actions)\n",
    "        return actions\n",
    "\n",
    "### Agent (DCQ Learning System )\n",
    "class DCQ():\n",
    "    def __init__(self, CNN, SoftmaxPolicy):\n",
    "        self.cnn = CNN\n",
    "        self.softmax = SoftmaxPolicy\n",
    "\n",
    "    def __call__(self, inputs):#comes from NStepProgress -> np.array([state])\n",
    "        \"\"\"similar to init function but it allows this AI class \n",
    "        instance to be treated like a function, not modifying the initial instance\"\"\"\n",
    "        #receive images from the game by converting image into a numpy array then into a torch tensor, then put a torch tensor into a torch variable with a gradient\n",
    "        input = Variable(torch.from_numpy(np.array(inputs, dtype = np.float32)))\n",
    "        output = self.cnn(input)\n",
    "        actions = self.softmax(output)\n",
    "        return actions.data.numpy()\n",
    "\n",
    "### Experience Replay\n",
    "#### N-Step\n",
    "class NStepProgress:\n",
    "    \"\"\"This class allows the AI to progress on several (n_step) steps\"\"\"\n",
    "    def __init__(self, env, ai, n_step):\n",
    "        self.ai = ai\n",
    "        self.rewards = []\n",
    "        self.env = env\n",
    "        self.n_step = n_step\n",
    "        self.step = namedtuple('Step', ['state', 'action', 'reward', 'done']) #Defining one Step\n",
    "    def __iter__(self):\n",
    "        \"\"\"Repeats but only incrementing parent loop when yield is called\"\"\"\n",
    "        state, info = self.env.reset()\n",
    "        history = deque()\n",
    "        reward = 0.0\n",
    "        while True: #go on forever until parent flag in ReplayMemory.runstep triggered\n",
    "            #select an action\n",
    "            action = self.ai(np.array([state]))[0][0] #agent.update\n",
    "            #get reward and next state\n",
    "            next_state, r, terminated, truncated, info = self.env.step(action)\n",
    "            done = terminated or truncated #if  game has some kind of max_steps or timeout, read 'truncated' with 'terminated'\n",
    "            reward += r #sum reward for every step\n",
    "            #add to stacked frame\n",
    "            history.append(self.step(state=state, action=action, reward=r, done=done))\n",
    "            while len(history) > self.n_step +1: #Always keep it n-steps e.g 10\n",
    "                history.popleft()\n",
    "            if len(history) == self.n_step + 1:#create our stacked tuple when finished\n",
    "                yield tuple(history)\n",
    "            state = next_state\n",
    "            if done: #either terminated or truncated signaling that the game has ended\n",
    "                if len(history) > self.n_step + 1:\n",
    "                    history.popleft()\n",
    "                while len(history) >= 1:\n",
    "                    yield tuple(history)\n",
    "                    history.popleft()\n",
    "                self.rewards.append(reward) #save accumulated reward per done\n",
    "                reward = 0.0\n",
    "                state, info  = self.env.reset()\n",
    "                history.clear()\n",
    "    \n",
    "    def rewards_steps(self):\n",
    "        \"\"\"stores total reward accumulated from start to done trigger\"\"\"\n",
    "        rewards_steps = self.rewards\n",
    "        self.rewards = []\n",
    "        return rewards_steps\n",
    "\n",
    "#### Replay Memory\n",
    "class ReplayMemory:\n",
    "    \"\"\"This class is modified to do n-step learning\"\"\"\n",
    "    def __init__(self, n_steps, capacity = 10000):\n",
    "        self.capacity = capacity # https://github.com/juliuskunze/nevermind/blob/master/nevermind/configurations.py\n",
    "        self.n_steps = n_steps\n",
    "        self.n_steps_iter = iter(n_steps) #creates an object that can be accessed one element at a time using __next__()\n",
    "        self.buffer = deque()\n",
    "\n",
    "    def sample_batch(self, batch_size): # creates an iterator that returns random batches\n",
    "        ofs = 0 #we use an offset to keep track of starting index for each batch\n",
    "        #we get samples from experience replay\n",
    "        vals = list(self.buffer)\n",
    "        #then randomly suffle them\n",
    "        np.random.shuffle(vals)\n",
    "        #now we check to see if we have enough samples in the buffer to make a batch if not we wait.\n",
    "        while (ofs+1)*batch_size <= len(self.buffer):\n",
    "            yield vals[ofs*batch_size:(ofs+1)*batch_size] #we slice from the offset position to the e.g 128 to 256\n",
    "            ofs += 1\n",
    "\n",
    "    def run_steps(self, steps):\n",
    "        \"\"\"Runs environment wait 10 consecutive steps of (state, action, reward, done) then save to buffer\n",
    "        until n sample steps are saved in buffer. Does not iterate until n_steps_iter collects 10 steps\"\"\"\n",
    "\n",
    "        while steps > 0:\n",
    "            entry = next(self.n_steps_iter) # run subtask as many times as it takes to return 10 consecutive steps of (state, action, reward, done)\n",
    "            self.buffer.append(entry) # we put e.g 200 n-step samples for the current episode, e.g 200 samples x 10 steps = 2,000 steps per episode\n",
    "            steps -= 1\n",
    "\n",
    "        while len(self.buffer) > self.capacity: # we accumulate no more than the capacity (e.g 10,000)\n",
    "            self.buffer.popleft()\n",
    "            \n",
    "### N-Step Q-Learning\n",
    "def eligibility_trace(batch, cnn, g=0.99):#batch is a sample of 128 10-steps where each step is ['state', 'action', 'reward', 'done'] so 1,280 transitions from memory\n",
    "    \"\"\"Asynchronous N-Step Q-Learning\n",
    "    learns the cumulative rewards and cumulative targets\n",
    "    on n-steps instead of one step like DQL\"\"\"\n",
    "    gamma = g\n",
    "    inputs = []\n",
    "    targets = []\n",
    "    for series in batch: #series of 10 transitions in our batch\n",
    "        #get the first and last image as the input. Convert fron numpy to torch variable\n",
    "        input = Variable(torch.from_numpy(np.array([series[0].state, series[-1].state], dtype=np.float32)))\n",
    "        output = cnn(input) #this is the prediction from the ai\n",
    "\n",
    "        #if the last transition of the series is not done we get the max q values\n",
    "        cumulative_reward = 0.0 if series[-1].done else output[1].data.max()\n",
    "\n",
    "        #start with the last step and go backwards to the first step\n",
    "        for step in reversed(series[:-1]):#reversed goes backwards \n",
    "            state = series[0].state # first state we need. This is where we started\n",
    "            target = output[0].data # this is the q value of the input state of the first step. This is what we thought we would get being in this state\n",
    "\n",
    "            # what new reward did we actually get\n",
    "            cumulative_reward = step.reward + gamma * cumulative_reward \n",
    "            target[series[0].action] = cumulative_reward # this is the q value we actually got\n",
    "\n",
    "            inputs.append(state) # we append our first state\n",
    "            targets.append(target) #we append the actual target q value for the first state\n",
    "\n",
    "            #output the input and the target after being processed through eligibility_trace\n",
    "            #we now have the first state and the target q values for the first state over 10 steps\n",
    "            return torch.from_numpy(np.array(inputs, dtype=np.float32)), torch.stack(targets)#we stack targets together\n",
    "\n",
    "### Image Preprocessing\n",
    "class ImagePreprocessor(ObservationWrapper):\n",
    "    \"\"\"Custom Image Preprocessor similar to Atari standard in gymnasium\"\"\"\n",
    "    def __init__(self, env, height = 64, width = 64, grayscale = True, crop = lambda img: img):\n",
    "        super(ImagePreprocessor, self).__init__(env)\n",
    "        self.img_size = (height, width)\n",
    "        self.grayscale = grayscale\n",
    "        self.crop = crop\n",
    "        n_colors = 1 if self.grayscale else 3\n",
    "        self.observation_space = Box(0.0, 1.0, [n_colors, height, width])\n",
    "\n",
    "    def observation(self, img):\n",
    "        img = self.crop(img)\n",
    "        img = Image.fromarray(img)\n",
    "        img = img.resize(self.img_size)\n",
    "        if self.grayscale:\n",
    "            img = img.convert('L')  # Convert to grayscale\n",
    "        else:\n",
    "            img = img.convert('RGB')  # Convert to RGB if necessary\n",
    "\n",
    "        #view preprocessed image\n",
    "        #plt.imshow(img)\n",
    "        #plt.show()\n",
    "\n",
    "        img = np.array(img)\n",
    "        #adds a new dimension to the array to represent the single color channel, resulting in the desired shape\n",
    "        img = np.expand_dims(img, axis=2)\n",
    "        \n",
    "        #if img.ndim == 2:  # Add channel dimension if missing\n",
    "        #    img = np.expand_dims(img, axis=2)\n",
    "        img = np.transpose(img, (2, 0, 1))\n",
    "        img = img.astype('float32') / 255\n",
    "\n",
    "        #check dim of new image\n",
    "        #print(\"image was preprocessed to: \" + str(\"greyscale\" if self.grayscale else \"RGB\") + \" with shape \" + str(img.shape))\n",
    "        return img\n",
    "\n",
    "### Moving Average Reward (for evaluation) on n-steps\n",
    "class MA:\n",
    "    def __init__(self, size):\n",
    "        self.list_of_rewards = []\n",
    "        self.size = size\n",
    "    def add(self, rewards):\n",
    "        \"\"\"adds step rewards until nth step then removes oldest rewardm leaving 100 steps of reward saved per episode\"\"\"\n",
    "        if isinstance(rewards, list):\n",
    "            self.list_of_rewards += rewards\n",
    "        else:\n",
    "            self.list_of_rewards.append(rewards)\n",
    "        while len(self.list_of_rewards) > self.size:\n",
    "            del self.list_of_rewards[0]\n",
    "    def average(self):\n",
    "        \"\"\"gets the average reward per nth step\"\"\"\n",
    "        if len(self.list_of_rewards) > 0:\n",
    "            return np.mean(self.list_of_rewards)\n",
    "        else:\n",
    "            return 0.0\n",
    "\n",
    "class CustomMask(Dataset): \n",
    "    \"\"\" Defines a custom dataset mask for SHAP Deep Explainer\"\"\"\n",
    "    #ref: https://blog.paperspace.com/deep-learning-model-interpretability-with-shap/\n",
    "    def __init__(self, data, transforms=None):\n",
    "        self.data = data\n",
    "        self.transforms = transforms\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image = self.data[idx]\n",
    "        if self.transforms!=None:\n",
    "            image = self.transforms(image)\n",
    "        return image"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "#split into different runs as full 20 difficult on limited hardware\n",
    "sims = ['ALE/AirRaid-v5', \n",
    "'ALE/Asterix-v5', \n",
    "'ALE/Asteroids-v5', \n",
    "'ALE/Bowling-v5', \n",
    "'ALE/Breakout-v5',\n",
    "'ALE/DemonAttack-v5',\n",
    "'ALE/Freeway-v5',\n",
    "'ALE/Gravitar-v5']\n",
    "\n",
    "sims = ['ALE/Jamesbond-v5',\n",
    "'ALE/MontezumaRevenge-v5',\n",
    "'ALE/MsPacman-v5',\n",
    "'ALE/Pong-v5',\n",
    "'ALE/PrivateEye-v5',\n",
    "'ALE/Qbert-v5',\n",
    "'ALE/Seaquest-v5'\n",
    "]\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "sims = ['ALE/SpaceInvaders-v5', \n",
    "'ALE/Venture-v5',\n",
    "'ALE/WizardOfWor-v5',\n",
    "'ALE/YarsRevenge-v5',\n",
    "'ALE/Zaxxon-v5']\n",
    "\n",
    "\n",
    "for sim in sims:\n",
    "    sim_name = sim # obs_type=\"ram\", frameskip=4, repeat_action_probability=0.25\n",
    "    filename = sim_name.replace(\"/\", \"_\").lower()\n",
    "    # stella emulator for atari = https://stella-emu.github.io/\n",
    "    # https://stella-emu.github.io/docs/index.html#Remapping:~:text=count%20and%20associated-,frames%20per%20second,-%2C%20bankswitch%20and%20display\n",
    "    # assume 60 fpS\n",
    "\n",
    "    capacity = [1000000, 500000, 100000, 50000, 10000, 5000, 1000, 500]\n",
    "    labels = ['1M', '500k', '100k', '50k', '10k', '5k', '1k', '500']\n",
    "\n",
    "    episodes = 200 #200\n",
    "    n = 10 # n-steps\n",
    "    samples = 200 #step samples to take per episode\n",
    "    bat = 128 + 26 #training batches randomly sampled from experience replay + extra samples to accommodate shap training\n",
    "    rw = 10 # reward window (e.g 100 steps)\n",
    "    shap_test_sample_ratio = 0.20 # e.g 10%\n",
    "\n",
    "    #create table for evaluating capacity against reward\n",
    "    df_rewards = pd.DataFrame(columns=['sim','capacity', 'episodes', 'reward'])\n",
    "\n",
    "\n",
    "    for i, cap in enumerate(capacity):\n",
    "        #get current timestamp\n",
    "        start_datetime = datetime.datetime.now()\n",
    "        start_datetime = start_datetime.strftime(\"%Y-%m-%d %H:%M:%S\")# Format the date and time as a string\n",
    "        print(filename.upper() + \" EXPERIMENT \" + str(i) + \": Date-Time: \" + str(start_datetime) + \", Capacity: \" + str(labels[i]))\n",
    "        \n",
    "        #### Setup Preprocessed Environment\n",
    "        print(\"Setting up environment\")\n",
    "        #env = gym.make(sim)\n",
    "        #We first preprocess the image by applying a greyscale and reducing the size to 80px.\n",
    "        title = sim_name[4:-3]\n",
    "        env = ImagePreprocessor(gym.make(sim_name), width=80, height=80, grayscale=True) #width=210, height=160 # , render_mode='human'\n",
    "        state, info = env.reset(seed=42)\n",
    "        \n",
    "        ##### Create DCQL Agent\n",
    "        print(\"Creating Agent with capacity set to: \" + str(cap))\n",
    "        a = env.action_space.n # actions\n",
    "        g = 0.99 #gamma\n",
    "        c = cap #1000000 #10000 #memory capacity\n",
    "        b = bat #training batches #128\n",
    "        l = 0.001 #learning rate\n",
    "        t = 1.0 #softmax policy temperature rate (tau). T controls the level of randomness or exploration in the action selection process. T is temperature high, meaning other actions are more explored\n",
    "    \n",
    "        \n",
    "        #Build the Agent\n",
    "        cnn = CNN(a)\n",
    "        softmax = SoftmaxPolicy(T=t)\n",
    "\n",
    "        agent = DCQ(CNN=cnn, SoftmaxPolicy=softmax)\n",
    "\n",
    "        # Set up Experience Replay\n",
    "        n_steps = NStepProgress(env=env, ai=agent, n_step=n) #instead of learning every transition we learn every nth transition\n",
    "        memory = ReplayMemory(n_steps=n_steps, capacity=c) #store the last c steps in memory e.g 1millm 500k etc.\n",
    "\n",
    "        #set learning parameters\n",
    "        loss = nn.MSELoss()#calculate mean squared error loss\n",
    "        optimizer = optim.Adam(cnn.parameters(), lr=l) #use adams optimiser with a learning rate of 0.001\n",
    "\n",
    "        ma = MA(rw) #used to get the average of the last n-step rewards\n",
    "\n",
    "        # Initialize an empty list to store the sampled inputs for SHAP Explainer\n",
    "        # set a sampling rate based on episodes as batches could be zero if not enough available at start\n",
    "\n",
    "        #define a random selection size such as 10%\n",
    "        random_select_size = math.ceil(episodes * shap_test_sample_ratio) \n",
    "\n",
    "        # Create an array of random episode numbers\n",
    "        random_episodes = np.random.choice(range(episodes), size=random_select_size, replace=False)\n",
    "\n",
    "        # Initialize an empty list to store the sampled inputs for SHAP Explainer\n",
    "        sampling_episode = []\n",
    "        sampled_inputs = []\n",
    "        sampled_targets = []\n",
    "        episode_sampled = False\n",
    "\n",
    "        #### Simulate the environment\n",
    "        print(\"Simulating environment\")\n",
    "        for episode in range(episodes):\n",
    "            #run the game for 200 runs of 10 steps and push sample transitions into memory\n",
    "            memory.run_steps(steps=samples) # e.g 200 steps sampled per episode\n",
    "            #sample 128 x10['state', 'action', 'reward', 'done'] or 1,280 transitions from memory if there is enough in memory to sample otherwise skip\n",
    "            for batch in memory.sample_batch(b): #b= e.g 128 batches\n",
    "                #agent training\n",
    "\n",
    "                #creates the training set for the agent, \n",
    "                # we get target discounted q values for the first state in the batch over 10 steps\n",
    "                inputs, targets = eligibility_trace(batch, cnn, g)\n",
    "\n",
    "                #We take some training samples for shap.deepxplainer to create heatmap images or we use them for training. Not both!\n",
    "                if episode in random_episodes and not episode_sampled:\n",
    "                    sampling_episode.append(episode) #record when sample was taken\n",
    "                    sampled_inputs.append(inputs)\n",
    "                    sampled_targets.append(targets)\n",
    "                    episode_sampled = True # Set the flag to True\n",
    "                else:\n",
    "                    #we convert them to tensor variables\n",
    "                    inputs, targets = Variable(inputs), Variable(targets)\n",
    "                    #like during eligibility_trace we get predicted q values from the cnn model\n",
    "                    predictions = cnn(inputs)\n",
    "                    loss_error = loss(predictions, targets)\n",
    "                    optimizer.zero_grad()\n",
    "                    loss_error.backward()\n",
    "                    optimizer.step()\n",
    "            \n",
    "            episode_sampled = False # Reset the flag for the next iteration\n",
    "                    \n",
    "            rewards_steps = n_steps.rewards_steps() # accumulated reward per 200 steps\n",
    "            ma.add(rewards_steps) #100 rewards kept\n",
    "\n",
    "            avg_reward = round(ma.average(),2) #average 100 rewards\n",
    "            print(\"Episode: %s, Reward: %s\" % (str(episode), str(avg_reward)))\n",
    "            \n",
    "            #save rewards per given nth step\n",
    "            new_row = [title, labels[i], episode, avg_reward]\n",
    "            df_rewards.loc[len(df_rewards)] = new_row\n",
    "        env.close()\n",
    "\n",
    "        #get end timestamp\n",
    "        end_datetime = datetime.datetime.now()\n",
    "        end_datetime = end_datetime.strftime(\"%Y-%m-%d %H:%M:%S\")# Format the date and time as a string\n",
    "        print(\"END - Date-Time: \" + str(end_datetime))\n",
    "\n",
    "        print(\"Saving SHAP graphs\")\n",
    "        #### Shap graphs\n",
    "        #took 10% of total episodes played as sample experience replay to be used as shap training/test data. So if 200 episodes we took 20 episodes. The agent has not seen these images yet\n",
    "        print(str(len(sampled_inputs)) + \" test images available:\")\n",
    "\n",
    "        print(\"Episodes samples were taken from:\")\n",
    "        print(sampling_episode)\n",
    "        \n",
    "        #convert sample states into PIL images\n",
    "        sampled_inputs_reshaped = []\n",
    "        for tensor in sampled_inputs:\n",
    "            # Convert tensor to numpy array and reshape\n",
    "            image_array = tensor.numpy().squeeze()  # Remove singleton dimensions\n",
    "        \n",
    "            # Convert the numpy array to a PIL Image\n",
    "            pil_image = Image.fromarray((image_array * 255).astype(np.uint8), mode='L')  # Convert to grayscale\n",
    "            sampled_inputs_reshaped.append(pil_image)\n",
    "        \n",
    "        #shap training set\n",
    "        print(\"Shap training set: \")\n",
    "        mask = sampled_inputs_reshaped[::2] #get all even images from index zero as training\n",
    "        print(len(mask))\n",
    "\n",
    "        #  turning mask to pytorch dataset then into tensors\n",
    "        mask = CustomMask(mask, transforms=transforms.ToTensor())\n",
    "        print(\"Masks created:\")\n",
    "        print(len(mask))\n",
    "        \n",
    "        print(\"Moving CNN to GPU and creating a mask loader\")\n",
    "        # Move the model to the same device as the input data\n",
    "        cnn.to(device)\n",
    "\n",
    "        # Convert the model weights to the same data type as the input data\n",
    "        cnn.float()  # Or whatever appropriate data type\n",
    "\n",
    "        #  creating dataloader for mask\n",
    "        mask_loader = DataLoader(mask, batch_size=10)\n",
    "\n",
    "        print(\"Training Deep SHAP Explainer please wait...\")\n",
    "        #  creating explainer for model behaviour\n",
    "        for images in mask_loader:\n",
    "            images = images.to(device)\n",
    "            explainer = shap.DeepExplainer(cnn, images)\n",
    "            break\n",
    "\n",
    "        print(\"Generating SHAP images and saving...\")\n",
    "        #  converting image to tensor\n",
    "        test_images = sampled_inputs_reshaped[1::2] # get all odd values from index 1 to use as test images\n",
    "        test_episodes = sampling_episode[1::2]\n",
    "        for t, test_image in enumerate(test_images):\n",
    "            \n",
    "            image = transforms.ToTensor()(test_image)\n",
    "            image = image.to(device)\n",
    "\n",
    "            #  deriving shap values for image of interest based on model behaviour\n",
    "            shap_values = explainer.shap_values(image.view(-1, 1, 80, 80))\n",
    "\n",
    "            #  preparing for visualization by changing channel arrangement\n",
    "            shap_numpy = [np.swapaxes(np.swapaxes(x, 1, -1), 1, 2) for x in shap_values]\n",
    "            image_numpy = np.swapaxes(np.swapaxes(image.view(-1, 1, 80, 80).cpu().numpy(), 1, -1), 1, 2)\n",
    "            shap_fig = plt.figure()\n",
    "            #  producing shap plots\n",
    "            shap.image_plot(shap_numpy, image_numpy, show=False, labels=env.unwrapped.get_action_meanings())\n",
    "            plt.savefig(\"./../plots/shap/\" + filename +\"_\"+ str(labels[i]) + \"_shap_ep_\"+ str(test_episodes[t]) +\".png\")\n",
    "            plt.close()\n",
    "\n",
    "        print(\"Saving Reward Graphs...\")\n",
    "        #### reward/episode graph\n",
    "        # Save and plot reward\n",
    "        reward_fig = plt.figure()\n",
    "        rewards_data = df_rewards[df_rewards['capacity'] == labels[i]]['reward'].values\n",
    "        plt.title(title.capitalize() + \": \" + str(labels[i]))\n",
    "        plt.xlabel(\"Episodes\")\n",
    "        plt.ylabel(\"Average Reward/\" + str(rw) + \" steps\")\n",
    "        plt.plot(rewards_data)\n",
    "\n",
    "        # Draw a vertical line at the optimal point\n",
    "        plt.axhline(y=round(np.mean(rewards_data),2), color='r', linestyle='--', label='Optimal Point')\n",
    "\n",
    "        # Draw red dots at the sampled experiences SHAP values will appear from\n",
    "        plt.scatter(test_episodes, [rewards_data[exp] for exp in test_episodes],\n",
    "                    marker='x', color='r', label='Experience Sampled', zorder=5)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        reward_fig.savefig(\"./../plots/rewardplots/\" + filename +\"_\"+ str(labels[i]) + \"_reward.png\") #must be before show to save correctly\n",
    "        #plt.show()\n",
    "        plt.close()\n",
    "\n",
    "        print(\"Saving Q-value/action graphs...\")\n",
    "        #### Q-values/Actions Taken/Episode graph\n",
    "\n",
    "        #save and plot q-values from sampled experience\n",
    "        # Iterate through sampled_targets\n",
    "\n",
    "        test_targets = sampled_targets[1::2]\n",
    "        action_labels = env.unwrapped.get_action_meanings()\n",
    "\n",
    "        for s in range(len(test_targets)):\n",
    "            # Create a figure\n",
    "            qvalue_fig = plt.figure(figsize=(10, 5))\n",
    "\n",
    "            # Iterate through the actions\n",
    "            for al in range(len(action_labels)):\n",
    "                # Initialize an empty array to store the y-values for this action\n",
    "                y_values = []\n",
    "\n",
    "                # Extract the y-values for the current action 'i' from all series\n",
    "                for st in range(len(test_targets)):\n",
    "                    y_values.append(np.array(test_targets[st][0][al]))\n",
    "\n",
    "                # Create a line plot for this action using all series' y-values\n",
    "                plt.plot(test_episodes, y_values, label='Action ' + str(action_labels[al]))\n",
    "\n",
    "            # Set the labels and title\n",
    "            plt.xlabel('Episodes')\n",
    "            plt.ylabel('Q-values')\n",
    "\n",
    "            # Draw a vertical line at sample point point\n",
    "            plt.axvline(x=test_episodes[s], color='r', linestyle='--', label='State: '+ str(test_episodes[s]))\n",
    "\n",
    "            # Set x-axis ticks for each value in test_episodes\n",
    "            plt.xticks(test_episodes,  rotation=90)\n",
    "\n",
    "            # Add a legend\n",
    "            plt.legend()\n",
    "\n",
    "            plt.tight_layout()\n",
    "\n",
    "            # Save the line chart to a file\n",
    "            qvalue_fig.savefig(\"./../plots/qvalueplots/\" + filename +\"_\"+ str(labels[i]) + \"_qvalues_ep_\"+ str(test_episodes[s]) +\".png\")\n",
    "            \n",
    "            # Display the line chart\n",
    "            #plt.show()\n",
    "            plt.close()\n",
    "\n",
    "        #### reward graph showing sample points\n",
    "        for s in range(len(test_targets)):\n",
    "        \n",
    "            # Save and plot reward\n",
    "            reward_test_fig = plt.figure(figsize=(5, 5))\n",
    "            plt.title(title.capitalize() + \": \" + str(labels[i]))\n",
    "            plt.xlabel(\"Episodes\")\n",
    "            plt.ylabel(\"Average Reward/\" + str(rw) + \" steps\")\n",
    "            plt.plot(rewards_data)\n",
    "\n",
    "            # Draw a vertical line at sample point point\n",
    "            plt.axvline(x=test_episodes[s], color='r', linestyle='--', label='State: '+ str(test_episodes[s]))\n",
    "\n",
    "            # Draw red dots at the sampled experiences\n",
    "            plt.scatter(test_episodes, [rewards_data[exp] for exp in test_episodes],\n",
    "                            marker='x', color='r', label='Experience Sampled', zorder=5)\n",
    "        \n",
    "            plt.tight_layout()\n",
    "            reward_test_fig.savefig(\"./../plots/rewardtest/\" + filename +\"_\"+ str(labels[i]) + \"_reward_ep_\"+ str(test_episodes[s]) +\".png\") #must be before show to save correctly\n",
    "            #plt.show()\n",
    "            plt.close()\n",
    "\n",
    "        #CREATE SHAP GIF\n",
    "        # List to store frames for the GIF\n",
    "        gif_frames = []\n",
    "\n",
    "        for s in range(len(test_targets)):\n",
    "            # Load images\n",
    "            reward_sample = Image.open('./../plots/rewardtest/' + filename +'_' + str(labels[i]) +'_reward_ep_' + str(test_episodes[s]) +'.png') \n",
    "            q_values = Image.open('./../plots/qvalueplots/' + filename +'_' + str(labels[i]) + '_qvalues_ep_' + str(test_episodes[s]) +'.png')\n",
    "            shap_values = Image.open('./../plots/shap/' + filename + '_' + str(labels[i]) +'_shap_ep_' + str(test_episodes[s]) + '.png')\n",
    "\n",
    "            # Get dimensions of input images\n",
    "            rs_width, rs_height = reward_sample.size  # Assuming the square images have the same dimensions\n",
    "            qv_width, qv_height = q_values.size\n",
    "            s_width, s_height = shap_values.size\n",
    "            new_width = s_width\n",
    "            new_height = (s_height*2)\n",
    "\n",
    "            # Create a new image with the calculated dimensions\n",
    "            new_image = Image.new('RGB', (new_width, (new_height)), (255, 255, 255))\n",
    "\n",
    "\n",
    "            # Paste the square images on top\n",
    "            new_image.paste(reward_sample, (240, 0))\n",
    "            new_image.paste(q_values, (310 + s_width - rs_width - qv_width, 1))\n",
    "\n",
    "            # Paste the landscape image on the bottom\n",
    "            new_image.paste(shap_values, (0, s_height+10))\n",
    "\n",
    "            # Save the new image\n",
    "            new_image.save('./../plots/shapexplainer/'+ filename +'_'+ str(labels[i]) +'_ep_' + str(test_episodes[s]) +'.png')\n",
    "\n",
    "            #pil_image = Image.open('images/thumbnail.webp')\n",
    "            #display(new_image)\n",
    "\n",
    "            #add image to gif array\n",
    "            gif_frames.append(new_image)\n",
    "\n",
    "        # Save the list of frames as a GIF\n",
    "        gif_frames[0].save('./../plots/shapexplainer/'+ filename +'_'+ str(labels[i]) +'_shap.gif', save_all=True, append_images=gif_frames[1:], loop=0, duration=200)\n",
    "        #gif_frames[0].show()\n",
    "            \n",
    "    print(\"Saving rewards to csv\")\n",
    "    x = datetime.datetime.now()\n",
    "    file_name = str(x.year) + \"_\" + str(x.month) + \"_\" + str(x.day) + \"_\" + str(x.strftime(\"%H\")) + \"_\" + str(x.strftime(\"%M\")) + \"_\" + str(x.strftime(\"%S\")) + \"_\" + filename + \".csv\"\n",
    "    df_rewards.to_csv(file_name, index=False, encoding='utf-8')\n",
    "\n",
    "    # Create a box plots using Seaborn for Evaluation\n",
    "    boxplot_fig = plt.figure()\n",
    "    unique_capacities = df_rewards['capacity'].unique()\n",
    "\n",
    "    print(\"Saving boxplot of results\")\n",
    "    #### Agent's final score to determine how it performed\n",
    "    average_accumulated_reward = round(sum(df_rewards['reward'].values),0)\n",
    "    print(\"Average Accumulated Reward: \" + str(average_accumulated_reward))\n",
    "    num_unique_capacities = len(unique_capacities)\n",
    "    p = []\n",
    "    for c in range(num_unique_capacities):\n",
    "        if c<1:\n",
    "            p.append('#FFF380')\n",
    "        else:\n",
    "            p.append('#8FD9F6')\n",
    "\n",
    "    ax = sns.boxplot(x='capacity', y='reward', data=df_rewards, width=0.5,\n",
    "                    palette=p)  # Specify colors for each box\n",
    "\n",
    "    # Adding labels and title to the plot\n",
    "    ax.set(ylabel='Average Reward/' + str(rw) + ' steps', xlabel='Experience Replay Capacity',\n",
    "        title=title.capitalize() + \": \" + str(episodes) + \" episodes\")  # Set labels and title for the axes\n",
    "\n",
    "    # Calculate the average value for the first box plot\n",
    "    average_value = df_rewards[df_rewards['capacity'] == df_rewards['capacity'].unique()[0]]['reward'].mean()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    boxplot_fig.savefig(\"./../plots/boxplot/\" + filename +\".png\") #must be before show to save correctly\n",
    "    # Show the plot\n",
    "    #plt.show()\n",
    "    plt.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_research",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
