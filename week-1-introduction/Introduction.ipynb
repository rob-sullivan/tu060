{"cells":[{"cell_type":"markdown","metadata":{"id":"wO_R2OMZ1uNY"},"source":["# Deep Learning Course - Introduction\n","\n","This is the introduction to the Deep Learning course. \n","\n","This component will cover \n","\n"," * Course Scope \n"," * Prerequisites Information \n"," * Course Structure\n"," * Assignments and Assessment\n"," * Coding \n"," * A little bit of background on Linear Algebra \n","\n","## Coarse Scope\n","\n","Let's get to the point. Modern frameworks make it easy to code up some solutions for Deep Learning. Have a look.... \n","\n","### Tensorflow: Accurate Digit Classification in 14 Lines "]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3699,"status":"ok","timestamp":1674493356674,"user":{"displayName":"Robert Ross","userId":"11121896988139609834"},"user_tz":0},"id":"ZdmixXzT7cX8","outputId":"eb8d5a8a-1785-439b-981f-1fc878c18f0c"},"outputs":[{"name":"stdout","output_type":"stream","text":["3.8.10 (default, Nov 14 2022, 12:59:47) \n","[GCC 9.4.0]\n","2.9.2\n"]}],"source":["import sys\n","import tensorflow as tf\n","print(sys.version)\n","print(tf.__version__)"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":31385,"status":"ok","timestamp":1674493388054,"user":{"displayName":"Robert Ross","userId":"11121896988139609834"},"user_tz":0},"id":"fCwcvZG91uNg","outputId":"58aec292-500f-4701-a49c-6d7795b3ea95"},"outputs":[{"name":"stdout","output_type":"stream","text":["Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n","11490434/11490434 [==============================] - 2s 0us/step\n","Epoch 1/5\n","1875/1875 [==============================] - 7s 2ms/step - loss: 0.5844 - accuracy: 0.8630\n","Epoch 2/5\n","1875/1875 [==============================] - 4s 2ms/step - loss: 0.2739 - accuracy: 0.9206\n","Epoch 3/5\n","1875/1875 [==============================] - 4s 2ms/step - loss: 0.2182 - accuracy: 0.9375\n","Epoch 4/5\n","1875/1875 [==============================] - 4s 2ms/step - loss: 0.1825 - accuracy: 0.9471\n","Epoch 5/5\n","1875/1875 [==============================] - 4s 2ms/step - loss: 0.1564 - accuracy: 0.9543\n","Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," flatten (Flatten)           (32, 784)                 0         \n","                                                                 \n"," dense (Dense)               (32, 64)                  50240     \n","                                                                 \n"," dense_1 (Dense)             (32, 10)                  650       \n","                                                                 \n","=================================================================\n","Total params: 50,890\n","Trainable params: 50,890\n","Non-trainable params: 0\n","_________________________________________________________________\n","None\n","\n","Test accuracy: 95.35999894142151%\n"]}],"source":["# Get a copy of the mnist dataset container \n","mnist = tf.keras.datasets.mnist \n","\n","# Pull out the training and test data \n","(x_train, y_train),(x_test, y_test) = mnist.load_data() \n","\n","# Normalize the training and test datasets\n","x_train = tf.keras.utils.normalize(x_train, axis=1)\n","x_test = tf.keras.utils.normalize(x_test, axis=1)\n","\n","# Create a simple sequential network object\n","model = tf.keras.models.Sequential()\n","\n","# Add layers to the network for processing the input data \n","model.add(tf.keras.layers.Flatten())\n","model.add(tf.keras.layers.Dense(64, activation=tf.nn.sigmoid))\n","model.add(tf.keras.layers.Dense(10, activation=tf.nn.softmax))\n","\n","# Compile the model\n","model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n","\n","# Start the training process\n","model.fit(x=x_train, y=y_train, epochs=5) \n","\n","print(model.summary())\n","\n","# Evaluate the model performance with test data\n","test_loss, test_acc = model.evaluate(x=x_test, y=y_test,verbose=0)\n","\n","# Print out the model accuracy \n","print('\\nTest accuracy: ' + str(test_acc*100) + \"%\" )"]},{"cell_type":"markdown","metadata":{"id":"DwoyTzjR1uNq"},"source":["The code above is training a Deep Neural Network based model to classify handwritten images like the ones below to digits. It is using state of the art optimisers, normalising the data, and gets 97% accuracy in just 5 epochs of training. It is taking advantage of the Keras interface to Tensorflow, and subject to getting your training data organised in the right way, it can be easily adopted to new problems. \n","\n","<img width=\"200\" src=\"https://drive.google.com/uc?id=17oViO2__3QWWoEKzf8LGlcFgMO-bpc_c\"/>\n","\n","Therefore we can say that it is very easy to take some code that we find online to get a basic Deep Learning model training and producing results. This is great, and the work that the developers of Tensorflow and libraries like it have done to allow users to rapidly prototype models is simply amazing. \n","\n","That said, if all we do is copy and paste code -- even with alterations -- we seldom learn and find it very difficult to apply learnings to novel or more challenging situations. For this reason, this coarse is not simply a guide to copying and pasting -- you don't need help to do that -- instead this course tries to give you a good understanding of some of the most important concepts that underpin the Deep Learning approach. Putting it simply, I want to  give you an understanding of why things are the way that they are. \n","\n","Of course theory is useless without practice, so we will keep theory and practice interleaved for you. \n","\n","### Content Covered \n","\n","In this Deep Learning course we will trace through the most important concepts in Deep Learning such as the Backpropagation Algorithm, Convolutional Neural Networks, Recurrent Neural Networks and Reinforcement Learning. \n","\n","The course will place emphasis on introducing concepts that are commonly used in practical Deep Learning research and application. An emphasis on running examples will be made. As such most lecture notes will be provided in the style of Jupyter notebooks that a student can download, edit and run on their own machines. I may also provide lecture slides on a week to seek basis, but the Jupyther notebooks are the main source of information. \n","\n","With the exception of Week 1, notes for each week are provided in advance of the lecture, and students are encouraged to study in advance for the class. The class - lecture and tutorial time - will be used to revise the material that has already been given out and will give you an opportunity to ask questions about it. This is commonly referred to as a flipped classroom delivery. \n","\n","Some content commonly found in other courses will not be covered here. For example the following will not be addressed: \n","\n"," * The history of neural networks \n"," * The biological inspiration of neural networks \n"," * Unsupervised learning of neural networks \n"," * Ethical Issues and Explainability \n","\n","The interested student is directed to the many good external resources such as Geoff Hinton's online course on neural networks for detail on these topics. \n","\n","Speaking of videos, this course is not intended to be self-contained. While detailed notes with working examples will be given from week to week as Jupyter notebooks, links to external resources such as videos from Geoff Hinton and Andrew Ng will also be suggested for additional coverage or for a different perspective on a given question. \n","\n","## Prerequisites / corequisites \n","\n","This course is ideally intended as an Introduction to Deep Learning for students who have already completed undergraduate or Masters level modules on:\n","\n"," * Artificial Intelligence, and, or \n"," * Machine Learning\n","\n","A Machine Learning course is a minimum co-requisite for this course. In other words, while it would be ideal that the Machine Learning module be taken already, it is also acceptable if you are taking Machine Learning in parallel to this module.  \n","\n","In the first few weeks the essential background topics of Linear and Logistic Regression will be covered. While these elements do overlap with the content of many Machine Learning modules, they are so essential here that we must make sure to cover them properly here. No other background Machine Learning concepts will be addressed such as distinctions between learning types; or benchmarking machine learning performance.\n","\n","The module is suitable for someone who has already taken an online module in Deep Learning or otherwise consulted books etc. While no new material will be covered, the course will give you an opportunity to reflect on important topics with the support of fellow students. \n","\n","All assignments must be coded up using Python with related libraries. If you have not already coded in Python, now is a good time to start. Introductory tutorials to Python and the use of the SKLearn (scikit) package should be consulted. In short, make sure that you can load a data set, perform k-fold cross validation of an SVM based classifier, and then run and present metrics such as the F1 score. \n","\n","\n","### Learning Outcomes \n","\n","Our Learning Outcomes cover the following: \n","\n","1. Use linear and logistic regression to build supervised machine learning models. \n","2. Use Deep Learning frameworks to implement Deep Learning methods for classification and or regression tasks.\n","3. Compare and contrast alternative activation types and cost functions in practical regression and classification tasks. \n","4. Apply convolutional networks for image and or text classification tasks. \n","5. Critically evaluate Recurrent Neural Networks against contemporary architectures for language processing tasks. \n","6. Compare stochastic gradient descent to other methods such as the Adam optimizer in training in Deep Neural Networks. \n","7. Evaluate the role of hardware in achieving efficient neural network training and deployment. \n","\n","Knowing what these are in advance isn't particularly useful, but some students do ask for them. Also it is worth noting that we will go beyond these simple few learning outcomes to give you as rounded as possible a view on particular Deep Learning based modelling. \n","\n","## Course Structure \n","\n","This is a 13 class programme. The course structure is subject to change and this content should be considered indicative. The earlier weeks focus in detail on low level issues and particularly in the first few weeks we will be purposfully replicating material that you should find in a statistics or intro to ML course. As we move through the course we will move to more high-level descriptions. This will be reflected in the lecture material style. The first half of the course puts a large emphasis on the Jupyter notebooks and detailed python code, but as we move forward the amount of material covered in the notebooks will ease off as we cover more content at a more abstract level in slide format instead.\n","\n","###### Week 1 - Introduction\n"," * Course Scope \n"," * Prerequisites \n"," * Course Structure \n"," * Assignments and Assessment\n"," * Coding \n"," * Getting started with Colab \n","\n","###### Week 2 - Linear Regression\n"," * Introduction to Linear Regression \n"," * Fitting functions to data\n"," * Cost Functions \n"," * Gradient Descent \n"," * Normalization of Data\n"," \n","######  Week 3 - Logistic Regression\n"," * Modeling Binary Data\n"," * The Logistic Function\n"," * The Cost Function for Logistic Units\n"," * Limits for Logistic Function\n"," * Higher Order Functions\n"," * From Linear to Non-Linear Logistic Classifiers\n","\n","###### Week 4 - Neural Network Essentials \n"," * Units\n"," * Layers\n"," * Bias Units\n"," * Building non-linear functions\n"," * The Feed Forward Algorithm\n"," * Loss \n","\n","###### Week 5 - Training\n"," * Overview of Backpropagation Methods\n"," * Deriving the Backpropagation Equations\n"," * The Backpropagation Algorithm\n"," * Cross Entropy Loss function \n"," * Softmax Layers \n"," \n","###### Week 6 - Improving Performance\n"," * Alternative Units: Hyperbolic Tanget Units and RELU\n"," * Preventing Overfitting\n"," * Regularisation to Linear and Logistic Regression\n"," * Regularization in Neural Networks\n"," * Early Stopping\n"," * Dropout\n"," \n","###### Week 7 - Convolutional Neural Networks \n"," * Images as multi-channel data \n"," * Convolutions \n"," * Pooling Layers\n"," * Implementing a CNN\n"," * Assignment Workshop \n"," \n","###### Week 8 - Sequences 1 - Recurrent Neural Networks and Test\n"," * Basic topology\n"," * Motivating examples\n"," * Long Short Term Memory\n"," * Language Modelling Example \n"," * Online Open Book Test. \n"," \n","###### Week 9 - Representations & Transfer Learning\n"," * Re-Using Weights\n"," * Challenges of Fine-Tuning \n"," * When does Transfer Learning not work. \n"," * Assignment Workshop \n"," \n","###### Class 10 - Sequences II - Long Short Term Memory and Attention\n"," * Long Short Term Memory detail.  \n"," * Intro to Attention Models\n"," * Assignment Workshop. \n","\n","######  Week 11 - Generative AI and Transformers\n"," * Generative Models \n"," * Encoder-Decoder Technologies\n"," * Transformers \n"," * More on Transformers -- Bert, ELMO, Roberta and their friends \n"," * Working with Transformers \n"," * Assignment Workgoups \n","\n","###### Week 12 - Two Types of Optimization  \n"," * Optimization\n"," * Stochastic Gradient Descent and Variants\n"," * Momentum in Optimization \n"," * Randomness in Optimization \n"," * Hardware Optimisation \n"," * Training versus Inference Hardware \n","\n","###### Week 13 - Return of the Transformers \n"," * Final Class Test  \n"," * Quick Reminder on Transformers\n"," * ChatGPT and the Future \n","\n","\n","Classes will be delivered approximately weekly on TUESDAY evening between 6pm and 9pm. The class will be delivered in a Hybrid Fashion. Extra focused labs are also delivered on Monday afternoon for full-time students or those part-time students who wish to drop in. \n","\n","### Delivery Model\n","\n","The course is a 5 ECTS course at Level 9/10. There are three contact hours per week, but students are expected to engage in a significant amount of self-study each week. The contact hours will be used for content delivery, group discussion, tests, reviewing assignments, and addressing any issues with respect to the delivery of the course. With the exception of Week 1, I will try to make lecture notes available in advance of the class. Therefore, students are strongly encouraged to study the content for each module in advance of that class. The example for the class in Week 2, it is expected that students will have studied the course notes on Linear Regression, reviewed any relevant videos, and completed any relevant questions or assignments. The lecture on Tuesday will cover key highlights in the module content provided in Jupyter notebooks, but will not be going through each notebook in fine detail. All content will be made available in the Deep Learning module on Brightspace. \n","\n","## Assignments and Assessment\n","This is a 100\\% Continuous Assessment course. The assessment of the course will be broken down as follows:\n","\n"," * 40\\% on in-class tests\n"," * 60\\% on Project Work\n","\n","There will be two in-class tests to encourage students to continuously engage with the material and take on assignments. The first test is in Week 8 (subject to confirmation or change) while the second test is at the end of the semester. These are open book tests that are designed to be challenging by asking questions around problem solving rather than just memorisation. Therefore genuine study will be required to make sure you get a good grade on these tests. \n","\n","The Project is a coding / modeling task which will allow the student to demonstrate a clear understanding of the concepts covered in the course. A dataset will be provided and students will be required to submit operational code and a short but detailed report on their model. Specific instructions on the project will be provided early in the course. The project will be due at the end of the semester. All students are required to orally present on their project and answer questions on their models / code at the end of the year. The project has an optional group element. \n","\n","## Coding \n","In this course we will use Python extensively for all examples and assignments. Rather than using vanilla Python we will where appropriate make use of Python packages that provide enhanced functionality for numerical computing and Deep Learning. Examples have been coded up using Python 3.7. You should set up your environment for Python 3.7 for minimal problems replicating code and examples. \n","\n","The following packages are some of the most frequently used in this course:\n"," * **numpy**\n"," * **scipy** - often referred to as SciKit\n"," * **matplotlib**\n"," * **tensorflow**\n"," * **pytorch**\n","\n","If you aren't familiar with either Python or the first three of these specific packages, now is the time to get familiar. We will use tutorail time in week 1 to discuss these and work through installations. \n","\n","Course notes and all examples will be coded up in the **Jupyter Notebook** environment. Notebooks will be made available on Brightspace for download. Students who do not already use Jupyter Notebook should install Jupyter Notebook. Note however that most notebooks will also be tested and deployed on Google Co-Lab. Again, this will be discussed in the Week 1 tutorial. \n","\n","For your work environment, you should make use of a virtual environment to manage your python code. I suggest that you setup your environment using venv and pip. I discourage the use of Anaconda as it takes away too much control from you. For coding, Jupyter is good for short examples. But for longer projects, there are many good IDEs available including Atom (with suitable plugins) and PyCharm. \n","\n","As indicated, the course will place emphasis on practical understanding - with attention placed on both the practical implementation and use of important model types. As such early models will be explained with two types of examples: **The Hard Way** and **The Easy Way**. \n","\n","### The Hard Way\n","\n","In early weeks material will be explained from first principles. We refer to this way of doing things as **The Hard Way**. In these examples we will make use of the **numpy** library for performing numerical operations such as matrix multiplication or transposition. However we will in general be designing and coding important concepts such as neuron types, the backpropogation algorithm etc. with little reliance on well known libraries. The emphasis here will be on understanding how the algorithm works. In these cases many simplifying assumptions will often be made.\n","\n","### The Easy Way\n","\n","In many weeks we will also use examples to show how the newly introduced concept can be quickly implemented using well known 3rd party libraries such as **scipy** or **TensorFlow**, or **PyTorch**. The emphasis here will often be on more complex examples which demonstrate the true power or limitations of the models that we are investigating. The easy way tends to refer to classic Tensorflow type operations where we see the computational graph that is being built up. \n","\n","Even within the Easy Way of coding, there can be variants in how we tackle certain types of models. Pure Tensorflow is very cumbersome -- although it makes it very clear what is actually going on in training and inference. Wrappers for underlying logic like Tensorflow are common. Keras is the best known example of such a wrapper. Keras allows us to rapidly prototype models in Tensorflow (and in other Deep Learning libraries) but it is not always transparent about what is going on 'under the hood'. "]},{"cell_type":"markdown","metadata":{"id":"i3yqd55_1uNx"},"source":["### Hardware Required \n","\n","Students are expected to have running installations of scipy and TensorFlow running on their own machines for testing. Your machine does not have to be particularly fast for most problems. For the first half of the course in particular, you should be able to run sample code on a moderate machine. A GPU is not needed for that code. However for some models discussed in the 2nd part of the course, more serious hardware would be required. \n","\n","Google Codelab is a nice platform where you can run even big models for free -- though you are limited in the amount of time the model can be left running for. \n"]},{"cell_type":"markdown","metadata":{"id":"YWdQ8m3D1uN3"},"source":["# Appendices \n","\n","Below you will find a couple of useful reference posts for issues such as notation and linear algebra in python. \n","\n","## Appendix A - Notation\n","\n","Given a training set we talk about:\n"," * $m$ = number of training examples\n"," * $x$'s = input variables or features\n"," * $y$'s = output variable or target\n"," * $(x,y)$ = one training example\n"," * $(x^{i},y^{i})$ = refers specifically to the ith training case\n","\n","## Appendix B - Linear Algebra in Python\n","\n","In python we can use the `numpy` library to easily define and perform operations on vectors and matrices. \n","\n","### Matrices\n","The term **matrix** refers to a 2D rectangular array of numbers. "]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":18,"status":"ok","timestamp":1674493388055,"user":{"displayName":"Robert Ross","userId":"11121896988139609834"},"user_tz":0},"id":"PPtsdAgn1uN3","outputId":"edcfdb6b-50d5-4600-91a1-fa9fbfc1b788"},"outputs":[{"name":"stdout","output_type":"stream","text":["[[1 2 3]\n"," [4 5 6]]\n","[[1 2 3]\n"," [4 5 6]]\n","[[1 2 3]\n"," [4 5 6]]\n"]}],"source":["import numpy as np\n","\n","# We can define a matrix directly from a string of numbers where rows are delimited by semi-colons\n","A = np.matrix('1 2 3; 4 5 6')\n","print(A)\n","\n","# Alternatively we can define the same matrix from a series of vectors where each vector defines a row of the matrix\n","B = np.matrix([[1, 2, 3], [4, 5, 6]])\n","print(B)\n","\n","# Alternatively we can use numpy's array constructor to creaea a 2D array, i.e., our matrix\n","B = np.array([[1, 2, 3], [4, 5, 6]])\n","print(B)"]},{"cell_type":"markdown","metadata":{"id":"nlujbV9_1uN4"},"source":["A matrix will have a certain *dimensionality* defined in terms of the number of rows and the number of columns. We can use standard number theory notation to define the dimensionality of the matrix. For example we can define the set of matrices of real numbers with 3 columns and 2 rows as $R^{3x2}$. \n","\n","The **shape** function in numpy will return the number of rows and columns for a given matrix. "]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":16,"status":"ok","timestamp":1674493388055,"user":{"displayName":"Robert Ross","userId":"11121896988139609834"},"user_tz":0},"id":"8V8vHH-G1uN4","outputId":"43be3563-a9e1-4876-a834-1dfa122ac34b"},"outputs":[{"name":"stdout","output_type":"stream","text":["(2, 3)\n"]}],"source":["print(A.shape)"]},{"cell_type":"markdown","metadata":{"id":"3pNIMv5_1uN5"},"source":["We refer to individual objects within the matrix as **elements**. We use subscript notation on the matrix name in order to refer to individual objects. In general $M_{i,j}$ will refer to the element found on the $i^{th}$ row and $j^{th}$ column of M. \n","\n","Numpy supports a wide range of methods for indexing and slicing arrays which we will not cover here. In the simple case we can however use indices to operate directly on the matrix as follows. "]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":14,"status":"ok","timestamp":1674493388055,"user":{"displayName":"Robert Ross","userId":"11121896988139609834"},"user_tz":0},"id":"K6K2_s151uN5","outputId":"52d995c1-fe5c-4447-ad0f-7042a604b56f"},"outputs":[{"name":"stdout","output_type":"stream","text":["5\n"]}],"source":["print(A[1,1])"]},{"cell_type":"markdown","metadata":{"id":"bML7Zn5s1uN5"},"source":["Note that indexing on the rows and columns of numpy matrices begins at 0. \n","\n","### Vectors\n","A vector is a 1D array and as such can be thought of as a special case of a matrix with only one column. \n","\n","While the vector is in general a special case of a matrix, we typically use different operators to create and work with vectors. "]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":13,"status":"ok","timestamp":1674493388055,"user":{"displayName":"Robert Ross","userId":"11121896988139609834"},"user_tz":0},"id":"su-17X0E1uN6","outputId":"7a38fc51-6fb7-43d0-8577-2c523bfad5b4"},"outputs":[{"name":"stdout","output_type":"stream","text":["[2 3 1 0]\n","[[2 3 1 0]]\n","3\n"]}],"source":["# create a numpy vector by way of a stadard python list fed into the array constructor\n","v1 = np.array([2,3,1,0])\n","print(v1)\n","\n","# note that this is not equivilent to attempting to create the array as a single row of a matrix\n","v2 = np.matrix([[2,3,1,0]])\n","print(v2)\n","\n","# index an element in the vector\n","print(v1[1])"]},{"cell_type":"markdown","metadata":{"id":"jx84oi831uN6"},"source":["### Matrix and Vector Basic Operations\n","\n","We can add and subtract matrices which are of the same dimensionality to result in a new matrix which is of that same dimensionality. "]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12,"status":"ok","timestamp":1674493388055,"user":{"displayName":"Robert Ross","userId":"11121896988139609834"},"user_tz":0},"id":"t5q52Rx91uN6","outputId":"95323e93-d0ab-42d1-9d3a-dcf35f1e7be3"},"outputs":[{"name":"stdout","output_type":"stream","text":["[[ 2  4  6]\n"," [ 8 10 12]]\n","[[1 2 3]\n"," [4 5 6]]\n"]}],"source":["C = A + B\n","print(C)\n","D = C - A\n","print(D)"]},{"cell_type":"markdown","metadata":{"id":"xttY4_Bv1uN7"},"source":["We can do the same for vectors"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12,"status":"ok","timestamp":1674493388056,"user":{"displayName":"Robert Ross","userId":"11121896988139609834"},"user_tz":0},"id":"X5UZXjzG1uN7","outputId":"b5a5ffed-8997-4e33-8411-18d0e44a791a"},"outputs":[{"name":"stdout","output_type":"stream","text":["[12 23 31 40]\n","[ 8 17 29 40]\n"]}],"source":["v2 = np.array([10,20,30,40])\n","\n","v3 = v1 + v2\n","print(v3)\n","v3 = v2 - v1\n","print(v3)"]},{"cell_type":"markdown","metadata":{"id":"H4NZSB4j1uN7"},"source":["We can also directly apply scalar multiplication and division operations to matrices and vectors.  "]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11,"status":"ok","timestamp":1674493388056,"user":{"displayName":"Robert Ross","userId":"11121896988139609834"},"user_tz":0},"id":"O11ScObD1uN7","outputId":"e76b79ed-917a-4ac6-ee7d-2b9d166202b9"},"outputs":[{"name":"stdout","output_type":"stream","text":["[[ 2  4  6]\n"," [ 8 10 12]]\n","[[0.5 1.  1.5]\n"," [2.  2.5 3. ]]\n","[20 40 60 80]\n","[ 5. 10. 15. 20.]\n"]}],"source":["E = A * 2\n","print(E)\n","F = A / 2\n","print(F)\n","v4 = v2 * 2\n","print(v4)\n","v5 = v2 / 2\n","print(v5)"]},{"cell_type":"markdown","metadata":{"id":"91j1yr131uN8"},"source":["### Inner / Scalar Product\n","The Inner Product or Scalar Product of two vectors is the scalar result of summing the pairwise products of elements in two vectors of equal length. In geometric space the Scalar Product is often interpreted as a distance metric between two points in that space. This is often used for example to calculate document similarities and in clustering. \n","\n","In python we can calculate the scalar product of two vectors using numpy's inner function. "]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10,"status":"ok","timestamp":1674493388056,"user":{"displayName":"Robert Ross","userId":"11121896988139609834"},"user_tz":0},"id":"Uauxb5cY1uN8","outputId":"158ad915-9209-4e08-be73-016626539e4e"},"outputs":[{"name":"stdout","output_type":"stream","text":["110\n"]}],"source":["v3 = np.dot(v1,v2)\n","print(v3)"]},{"cell_type":"markdown","metadata":{"id":"F0OxytQK1uN8"},"source":["Note that the dot function will also produce this result when applied to two vectors. However the dot function can also be applied to matrices and higher-order arrays. "]},{"cell_type":"markdown","metadata":{"id":"mkuQZ-cJ1uN9"},"source":["### Matrix-Matrix Multiplication\n","Given two matrices we can calculate the cross product of these matrices so long as the number of rows in the first matrix equals the number of columns in the second. \n","\n","For two matrices A and B, the dimensionality of the resultant matrix product is given by: \n","\n","\\begin{equation}\n","R_{A}C_{A} \\times R_{B}C_{B} = R_{A}C_{B}\n","\\end{equation}\n","\n","The operation for calculating the matrix product is straightforward: the entry for row i column j in the resultant matrix C is the dot product of the i$^{th}$ row of A and the j$^{th}$ column of B. \n","\n","![matrix matrix multiplication](figures/img792.gif)\n","\n","In python we can use the numpy function **matmul** to perform matrix-matrix multiplication. "]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1674493388056,"user":{"displayName":"Robert Ross","userId":"11121896988139609834"},"user_tz":0},"id":"gy-Df74U1uN9","outputId":"6c4e0e57-c1bd-4c6c-c570-cf0c61ad4cfc"},"outputs":[{"name":"stdout","output_type":"stream","text":["[[1 2]\n"," [3 4]\n"," [5 6]]\n","[[22 28]\n"," [49 64]]\n"]}],"source":["G = np.matrix('1 2; 3, 4; 5, 6')\n","print(G)\n","H = np.matmul(A,G)\n","print(H)"]},{"cell_type":"markdown","metadata":{"id":"3SDm-6X81uN9"},"source":["Remember that matrix matrix multiplication is not commutative. \n","\\begin{equation}\n","  A \\times B \\neq B \\times A\n","\\end{equation}"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1674493388056,"user":{"displayName":"Robert Ross","userId":"11121896988139609834"},"user_tz":0},"id":"UKX76aMJ1uN9","outputId":"cd650ee2-6a33-4bd3-c450-7526ccbbf1b9"},"outputs":[{"name":"stdout","output_type":"stream","text":["[[1 2 3]\n"," [4 5 6]\n"," [7 8 9]]\n","[[1 1 1]\n"," [2 2 2]\n"," [3 3 3]]\n","[[14 14 14]\n"," [32 32 32]\n"," [50 50 50]]\n","[[12 15 18]\n"," [24 30 36]\n"," [36 45 54]]\n"]}],"source":["A = np.matrix('1, 2, 3; 4, 5, 6; 7, 8, 9')\n","B = np.matrix('1, 1, 1; 2, 2, 2; 3, 3, 3')\n","print(A)\n","print(B)\n","print(np.matmul(A,B))\n","print(np.matmul(B,A))"]},{"cell_type":"markdown","metadata":{"id":"WRrDpNuB1uN9"},"source":["### Matrix Identity and Inverse \n","\n","We can define the identity $I$ of a matrix which in general allows the commutation property to hold. \n","\n","\\begin{equation}\n"," A \\times I = I \\times A = A\n","\\end{equation}\n","\n","Here $I$ is the Identity Matrix which is a square matrix where all diagonal elements are = 1 and all non-diagonal elements are = 0. \n","\n","Numpy allows us to easily define an identity matrix with a specified number of rows and columns. "]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1674493388057,"user":{"displayName":"Robert Ross","userId":"11121896988139609834"},"user_tz":0},"id":"e06NjeYC1uN-","outputId":"5c5eff31-11e2-4a1c-b688-461e151b7108"},"outputs":[{"name":"stdout","output_type":"stream","text":["[[1. 0. 0.]\n"," [0. 1. 0.]\n"," [0. 0. 1.]]\n","[[1 2 3]\n"," [4 5 6]\n"," [7 8 9]]\n","[[1. 2. 3.]\n"," [4. 5. 6.]\n"," [7. 8. 9.]]\n","[[1. 2. 3.]\n"," [4. 5. 6.]\n"," [7. 8. 9.]]\n"]}],"source":["I = np.identity(3)\n","print(I)\n","print(A)\n","print(np.matmul(A,I))\n","print(np.matmul(I,A))"]},{"cell_type":"markdown","metadata":{"id":"Frq8ZW041uN-"},"source":["Just as we can define the inverse for a real number $X \\in R$ as $\\frac{1}{X}$, we can also define the inverse for a matrix. \n","\n","Beginning first with the case of real numbers, we note that: \n","\n","\\begin{equation}\n"," X \\times INV(X) = I\n","\\end{equation}\n","\n","where I is the identity for real numbers - which is 1. \n","\n","This gives us an intuition of how the inverse is defined for matrices:  \n","\n","\\begin{equation}\n"," A \\times INV(A) = I \n","\\end{equation}\n","\n","i.e., the inverse of a matrix A should be defined such that the matrix product of A by it produces an identity matrix. \n","\n","While we straightforwardly calculate the inverse of a real number X as $\\frac{1}{X}$, the calculation of the inverse of a matrix is more complicated and involves calculations of Determinants and Cofactors of matrices which we will not consider here. Fortunately we can of course calculate the inverse directly in numpy. "]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1674493388057,"user":{"displayName":"Robert Ross","userId":"11121896988139609834"},"user_tz":0},"id":"StBk_MCH1uN-","outputId":"6188835e-2168-4239-bf03-3644f13796d4"},"outputs":[{"name":"stdout","output_type":"stream","text":["[[ 0.00746988 -0.00349497  0.01506633]\n"," [-0.0023959   0.00211775  0.00542254]\n"," [-0.00056121  0.00191823 -0.00104746]]\n","[[ 1.00000000e+00  4.31512465e-17  2.29850861e-17]\n"," [ 4.47775497e-17  1.00000000e+00 -5.16080234e-17]\n"," [ 4.32596667e-17 -3.10081821e-17  1.00000000e+00]]\n","Uncomment lines in the block to run this - but expect an error!\n"]}],"source":["import numpy.linalg as la\n","X = np.matrix('100, -200, 403; 44, -5, 607; 27, 98, -59')\n","B = la.inv(X)\n","print(B)\n","print(np.matmul(X,B))\n","\n","# note that the process doesn't work very well when the candidate matrix is close to 0\n","print(\"Uncomment lines in the block to run this - but expect an error!\") \n","# B = la.inv(A)\n","# print(np.matmul(A,B))"]},{"cell_type":"markdown","metadata":{"id":"OqEWX7wH1uN_"},"source":["### Matrix Transpose\n","We can also define the transpose of a matrix $A^{T}$ as a matrix such that rows and columns of $A$ are reversed. \n","\n","\\begin{equation}\n","   A_{ij} = A_{ji}\n","\\end{equation}"]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1674493388057,"user":{"displayName":"Robert Ross","userId":"11121896988139609834"},"user_tz":0},"id":"BpJl-Ysj1uN_","outputId":"f0d22534-689e-46c4-db57-a220e598b344"},"outputs":[{"name":"stdout","output_type":"stream","text":["[[1 2 3]\n"," [4 5 6]\n"," [7 8 9]]\n","[[1 4 7]\n"," [2 5 8]\n"," [3 6 9]]\n"]}],"source":["C = A.T\n","print(A)\n","print(C)"]},{"cell_type":"markdown","metadata":{"id":"G0G83AGD1uN_"},"source":["### Do I need all this Linear Algebra? \n","\n","Knowledge of the issues above is typically not needed just to run basic Deep learning modeling tasks. However, in the first number of weeks while we are figuring out why things work they way that they do, this is essential. "]}],"metadata":{"accelerator":"GPU","anaconda-cloud":{},"colab":{"provenance":[]},"kernelspec":{"display_name":"venv_research","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"},"vscode":{"interpreter":{"hash":"5794a404577a32ff418895c331e736516539893dd666baf27c4118b4263cef3b"}}},"nbformat":4,"nbformat_minor":0}
