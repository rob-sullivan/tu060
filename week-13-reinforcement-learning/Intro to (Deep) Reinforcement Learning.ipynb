{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.5"},"colab":{"name":"Intro to (Deep) Reinforcement Learning.ipynb","provenance":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"SErwWzYxpOKB"},"source":["# Reinforcement Learning\n","\n","\n","## Introduction \n","\n","Reinforcement Learning focuses on situations where we want to learn to perform the correct action in a given environment, but where the complexity of the environment is just too great for us to hope to enumerate a comprehensive set of training data as we would in Supervised Learning. The \"environment\" is a situation or configuration of variables that describe our current state. This environment could be a physical environment, a virtual environment, or it could just be a set of variables or features. \n","\n","To guide the learning process we usually take advantage of some type of reward. The reward can be positive or negative and during training is available to tell us if an action we select is useful in achieving our goals. \n","\n","What makes this problem far more complex is that we may not have a clearly easily defined reward for every action we take in the environment at a given point in time. Instead the reward might only be observable after a sequence of actions is complete. We can't just associate the reward with the final action. Instead we need to be able to distribute that reward back across the earlier actions in the sequence. (While this idea sounds a little bit like back-propagation, keep in mind that this reward distribution is a very different process that exists independently of neural networks). Therefore when we learn to make an action at time T, we are typically doing this not for a greedy reward at time T, but for some reward that is only available to us from some time in the future. \n","\n","As an abstract illustration of this, think about studying. An evening of study can be a boring and tiring event. The reward for us doing this now, i.e., at time T, is not very apparent. Instead we might need to look all the way forward to some promotion in work, or the smile on someone's face at graduation, to imagine what the reward might be. We select our action based not on a greedy view of now, but partly on some portion of the future reward. \n","\n","Unfortunately, we can't even assume that the an optimum policy for deciding on what action to take will always lead to us to a successful outcome. In the real world there are almost random occurrences and things we can't control that can derail our best laid plans. Therefore our policies need to do their best to account for such environmental changes when we don't known exactly what those changes will be. For example, we sit down to study for the eventing (good plan), but just as we do it, the power is cut, or a friend text's us and tells us about a party your absolutely have to go to. In this case we selected an appropriate action, i.e., study, but the environment changed and the immediate state change that we expected from selecting a particular action is not actually going to come to fruition. \n","\n","Turning to more computational examples, we might want to use reinforcement learning when learning to play games, or learning what to say in a given situation. In both these circumstances the number of different situations we can be in are huge, the range of actions we can perform is often very large, and importantly the ultimate goal of the activity is not going to be experienced until many steps in the future -- and is subject to circumstances that we can't fully control. \n","\n","### Contrast with Supervised Learning\n","\n","For any student starting with Reinforcement Learning, a common questions is whether Reinforcement Learning is an example of Supervised or Unsupervised Learning. In practice it is easiest to think about Reinforcement Learning as its own thing. While it shares some features with Supervised Learning, the methods of training and lack of training samples make it something quite different from Supervised Learning. \n","\n","In supervised learning the complexity of the environment relative to the range of target labels is usually small enough to learn a mapping between environment and target explicitly, in Reinforcement Learning we don't have a chance of learning that mapping in a static way. \n","\n","What we do instead is try performing the best possible action as far as we understand things so far, we then find out the consequence of the action - i.e., whether it was good or bad - and then adjust our action selection policy based on that result. For Supervised Learning we don't need to worry about the future -- we are only ever making the best possible prediction for now with no interference from future considerations. \n","\n","### How do we train in practice\n","\n","Since we can't collect a set of labeled training data, instead we need to learn to make our decisions based on experience. The question then becomes, how do we gain this experience? \n","\n","For games and game like environments, the answer is easy. We can play the games multiple times and learn from our mistakes. This is exactly analogous to how people actually play games and learn to beat the system. Better still, we don't necessarily have to play the games in real time. Since we are training computational systems we can speed up the game play. This ability to speed up the game play for training is important, because, as we will see later, Reinforcement Learning needs a lot of iterations to achieve good results. \n","\n","If we are learning to perform actions in a real environment -- for example physical actions for a robot or spoken actions for a dialogue system -- we need to be able to map our real environment to some sort of simulation for training. Again, once we have this simulation we can usually speed up the simulation to assist in training. While building a simulation is usually straightforward, the challenge for this type of real-world oriented task is to design a suitable mechanism that can give us a reward after every action. Designing this 'oracle' that can give us the reward during training is often one of the toughest tasks in putting reinforcement learning to work. \n","\n","### (Deep?) Reinforcement Learning \n","\n","In this lecture series we are primarily focused on Deep Reinforcement Learning, i.e., Reinforcement Learning executed in a Deep Learning framework. However it is worth keeping in mind that Reinforcement Learning has a history that goes back long before Deep Learning -- this is similar to how Supervised Learning exists as a domain that is much bigger than Deep learning based Supervised Learning. \n","\n","With this in mind, we will begin this lecture by looking at Q-Learning -- a classical approach to Reinforcement Learning. We will use that to illustrate the ideas underpinning Reinforcement Learning, and then go on to show how in can be re-cast as a Deep Learning problem. \n","\n","These notes are based on a number of on-line tutorials for Deep Reinforcement Learning. Below I have links to the source materials. In most cases I have changed the code used slightly -- usually just to make things a little bit more detailed so that you don't have to try to spend so long figuring out what is going on. \n","\n","https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-0-q-learning-with-tables-and-neural-networks-d195264329d0\n","https://gym.openai.com/envs/FrozenLake-v0/\n","https://github.com/streklin/learnAIGymAtari/wiki\n","\n","\n","# A Game: FROZEN LAKE \n","\n","To illustrate the ideas behind Reinforcement Learning we will make use of a very simple text game environment. The game is called Frozen-Lake-V0. For more information on the game, let's quote from the game page which was referenced above. \n","\n","> Winter is here. You and your friends were tossing around a frisbee at the park when you made a wild throw that left the frisbee out in the middle of the lake. The water is mostly frozen, but there are a few holes where the ice has melted. If you step into one of those holes, you'll fall into the freezing water. At this time, there's an international frisbee shortage, so it's absolutely imperative that you navigate across the lake and retrieve the disc. However, the ice is slippery, so you won't always move in the direction you intend.\n","\n","For our Frozen Lake we will assume a 4x4 grid environment where every cell is a space on the frozen lake. The cells can be one of four types: (S) a start cell where we set off from, (G) a goal cell where the frisbee is located, (F) a frozen cell that is safe to move on to, and (H) a hole cell that will cause is to fall into the icy water. The specific layout generated by default is given below: \n","\n","| |Column 1 | Column 2 | Column 3 | Column 4 |\n","|-|-|-|-|-|\n","|Row 1 |S|F|F|F|\n","|Row 2 |F|H|F|H|\n","|Row 3 |F|F|F|H|\n","|Row 4 |H|F|F|G|\n","\n","We can give each of these cells numbered labels for easy reference:\n","\n","| |Column 1 | Column 2 | Column 3 | Column 4 |\n","|-|-|-|-|-|\n","|Row 1 |S-0|F-1|F-2|F-3|\n","|Row 2 |F-4|H-5|F-6|H-7|\n","|Row 3 |F-8|F-9|F-10|H-11|\n","|Row 4 |H-12|F-13|F-14|G-15|\n","\n","Looking at the problem this way, we want to get from cell 0 to cell 15 without moving through cells 5, 7, 11 or 12. \n","\n","The agent has four actions available to it, these correspond to moving LEFT, RIGHT, UP, or DOWN. Only one cell at a time can be moved to. Internally the game 'engine' encodes these four moves as integers as per the list below: \n","\n","* LEFT = 0\n","* DOWN = 1\n","* RIGHT = 2\n","* UP = 3\n","\n","The final thing to note is that the actions we make are not always guaranteed to work out as we expect them. From a game play perspective we can think of the Frozen Lake environment as being slippery, and that even when we plan to say move DOWN there is no guarantee that this is what will happen. Instead we might slip off in a different direction. This means that it is impossible for us to learn to play the game perfectly and that the best action to take might not always correspond to our initial intuition. \n","\n","The slippery movement algorithm for Frozen Lake means that (when thinking from the game character's perspective) if we try to move \"forward\", we might end up moving forward, left, or right with equal probability. We won't end up moving backward. Keep in mind also that we aren't allowed to walk off the edges -- though aw we will see later, that doesn't stop us trying.  \n","\n","## Approach 1: Random Walk\n","\n","Before we look at the Q-Learning algorithm in detail, lets try to play a simplified from of the game with a random walk strategy. In other words, at any give point on the lake we might randomly decide to walk in any of the four directions. We'll disable the slippery nature of the walk for now for illustration. \n","\n","As we might expect it is going to be tough to complete the game in this way. As a record of our success we will count the occasions that we complete the course out of a long sequence of game 'episodes'. \n","\n","Lets being by importing the libraries that we need. In addition to some usual libraries, we also import 'gym' which is a library that contains the Frozen Lake game implementation and an API to control it. "]},{"cell_type":"code","metadata":{"id":"CwUdcWr0pOLc"},"source":["import gym\n","import numpy as np\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","from gym.envs.registration import register\n","register(\n","    id='FrozenLakeNotSlippery-v0',\n","    entry_point='gym.envs.toy_text:FrozenLakeEnv',\n","    kwargs={'map_name' : '4x4', 'is_slippery': False},\n","    max_episode_steps=100,\n","    reward_threshold=0.78, # optimum = .8196\n",")\n","\n","frozen_env = gym.make('FrozenLakeNotSlippery-v0')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4SwJfTWkpOLd"},"source":["Rather than using the standard form of the game, we created our own variant that has the slippery property turned off. Once we have designed that game variant, we created an instance of the game and note how many episodes of the game we want to play. "]},{"cell_type":"markdown","metadata":{"id":"_JYZ0X6CpOLe"},"source":["Finally, let's set up the code to create the game. We will have two loops. The outer loop controls the number of game instances that we play -- we will refer to these as episodes. The inner loop controls the maximum number of steps we can take per game episode. We will always execute all episodes of the game. But for the inner loop, we can leave the loop early if we either win (reach the goal state) or fail (fall into a hole). \n","\n","We are going to take an Object Oriented approach to our design to make it easier to experiment and change things later. "]},{"cell_type":"code","metadata":{"id":"CSPOBjIhpOLf"},"source":["from abc import ABC, abstractmethod\n","\n","class FrozenLakeFramework(ABC):\n","    \"\"\"Learning Framework for Frozen Lake\"\"\"\n","    \n","    def __init__(self, env, num_episodes):\n","        self.env = env\n","        self.num_episodes = num_episodes\n","        # set up list for recording game logs, wins, and steps per game\n","        self.logs = []\n","        self.wins = []\n","        self.steps = []\n","        super().__init__()\n","        \n","    @abstractmethod\n","    def selectAction(self,j,s,i):\n","        pass        \n","                   \n","    def updatePolicy(self,s,a,r,s1):\n","        \"\"\"\n","        Update the selection policy. Does nothing by default but is not abstract. \n","\n","        Parameters: \n","        s (int): current state\n","        a (int): action selected\n","        r (int): reward for the state-action pair. \n","        s1 (int): the state we moved to after performing a. \n","        \"\"\"\n","        return\n","    \n","    def archive(self,i):\n","        \"\"\"\n","        At the end of a game play, archive any information necessary for diagnostics. \n","        \n","        Default version does nothing. Note that this isn't part of the learning algorithm, \n","        it is just producing records for debugging. \n","        \"\"\"\n","        return 0         \n","        \n","    def playFrozenLake(self,gen_logs=True):\n","        \"\"\"\n","        Execute a training session for Frozen Lake and print out the results. \n","        \n","        The main function takes care of training and records a lot of information to return to us for visualisation and debugging purposes. \n","\n","        Parameters: \n","        gen_logs (binary): Indicate whether logs of runs should be generated and returned. Defaults to True.\n","\n","        \"\"\"\n","        # Iterate through all episodes of game play\n","        for i in range(self.num_episodes):\n","            if i % 100 == 0: print(str(i)+\", \",end = '')\n","            # Reset environment and get first state\n","            s = self.env.reset()\n","            # make sure done flag is set to false and reward is 0\n","            done, reward = False, 0\n","            # create list to record trace of this game episode\n","            trace = []\n","            for j in range(100):\n","                # select an action to perform\n","                action = self.selectAction(j,s,i)\n","                # apply action and get new state, reward and done flag from enviroment \n","                s1,reward,done,_ = self.env.step(action)\n","                # Update the action selection policy\n","                self.updatePolicy(s,action,reward,s1)\n","                # record the current state transition tuple -- used for printing and reviewing \n","                trace.append([s,action,s1,reward,done])\n","                # new state becomes current state\n","                s = s1\n","                # if we are done we exit the loop\n","                if done == True: \n","                    break\n","            # Clean up information after training loop \n","            if gen_logs == True: \n","                self.logs.append(trace)\n","\n","            # if we won - record win and number of steps required, else record loss. \n","            self.wins.append(reward)\n","            self.steps.append(j)\n","\n","            # call for any custom archiving \n","            self.archive(i) \n","        \n","        return\n","    \n","    def preTrain(self,num_pre_train_episodes):\n","        \"\"\"\n","        Perform Pre-Training. We will use the model to\n","        \n","        The main function takes care of training and records a lot of information to return to us for visualisation and debugging purposes. \n","\n","        Parameters: \n","        gen_logs (binary): Indicate whether logs of runs should be generated and returned. Defaults to True.\n","\n","        \"\"\"\n","        # Iterate through all episodes of game play\n","        for i in range(self.num_pre_train_episodes):\n","            # Reset environment and get first state\n","            s = self.env.reset()\n","            # make sure done flag is set to false and reward is 0\n","            done, reward = False, 0\n","            for j in range(100):\n","                # select an action to perform\n","                action = self.selectAction(j,s,i)\n","                # apply action and get new state, reward and done flag from enviroment \n","                s1,reward,done,_ = self.env.step(action)\n","                # Update the action selection policy\n","                self.updatePolicy(s,action,reward,s1)\n","                # new state becomes current state\n","                s = s1\n","                # if we are done we exit the loop\n","                if done == True: \n","                    break\n","        \n","        return\n","        \n","    def test(self,episodes):\n","        \"\"\"\n","        Execute a testing session for Frozen Lake and print out the results. \n","        \n","        This is basically the same as training, but we don't train. \n","\n","        Parameters: \n","        gen_logs (binary): Indicate whether logs of runs should be generated and returned. Defaults to True.\n","\n","        \"\"\"    \n","        # Iterate through all episodes of game play\n","        r = 0 \n","        for i in range(episodes):\n","            # Reset environment and get first state\n","            s = self.env.reset()\n","            # make sure done flag is set to false and reward is 0\n","            done, reward = False, 0\n","            for j in range(100):\n","                # select an action to perform\n","                action = self.selectAction(j,s,i)\n","                # apply action and get new state, reward and done flag from enviroment \n","                s1,reward,done,_ = self.env.step(action)\n","                # new state becomes current state\n","                s = s1\n","                # if we are done we exit the loop\n","                if done == True: \n","                    break\n","\n","            # if we won - record win and number of steps required, else record loss. \n","            r += reward\n","\n","        return (r / episodes) * 100\n","    \n","    def printLogs(self,max_n):\n","        \"\"\"Print up to max_n number of episode entries from logs.\"\"\"\n","        if(max_n > len(self.logs)):\n","            max_n = len(self.logs)\n","\n","        for i in range(max_n):\n","            print(\"Printing \" + str(i))\n","            l = self.logs[i]\n","            for j in range(len(l)):\n","                step = l[j]\n","                print(\"S: \" + str(step[0]) + \", a: \" + str(step[1]) + \", s1: \" + str(step[2])  + \", r: \" + str(step[3])   + \", d: \" + str(step[4]))\n","    \n","    def printResults(self,plot=True,l=0):\n","        if l > 0: \n","            printLogs(self.l)\n","\n","        if plot:\n","            plt.plot(self.wins)\n","            plt.title('Wins against number of episodes. Every spike was a win.')    \n","        \n","        return "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6i1FjKvqpOLl"},"source":["Now that we have the basic abstract class for our game, lets instantiate it for the RandomWalk case. \n","\n","To make things easier for later we have split out the selection of an action into its own function. We do this with argmax. This might seem like overkill for now, but will be useful later. Also note that we aren't using the variables j or s yet, but we will update this function later and it is useful to have the function signature in place now. We will also create a function for updating our selection policy. For the moment we don't have anything to update since we are selecting randomly, but it is useful to have this defined as a placeholder for later. "]},{"cell_type":"code","metadata":{"id":"AFEo3-28pOLm"},"source":["class RandomFL(FrozenLakeFramework):\n","    \"\"\"Random Walk based approach to Frozen Lake\"\"\"\n","    \n","    def __init__(self, env, num_episodes):\n","        FrozenLakeFramework.__init__(self,env,num_episodes)\n","        \n","    def selectAction(self,j,s,i):\n","        \"\"\"\n","        Randomly select an action to return. \n","\n","        Parameters: \n","        j (int): number of steps taken in the game \n","        s (int): current state of the game\n","        i (int): training episode number\n","\n","        Returns: \n","        int: indicates the next action to be performed. \n","        \"\"\"\n","        return np.argmax(np.random.randn(1,self.env.action_space.n))       "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"a3bjFgFZpOLm"},"source":["Lets define the number of training episodes and start training. "]},{"cell_type":"code","metadata":{"id":"H0nhiq2mpOLn","outputId":"4eb35ed0-ec58-4566-a0c7-8a8ee611eaf2"},"source":["game = RandomFL(frozen_env,1000)\n","game.playFrozenLake()\n","game.printResults()\n","print(\"\\nTest results: % 12.2f\" % game.test(500))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["0, 100, 200, 300, 400, 500, 600, 700, 800, 900, \n","Test results:         1.40\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAXkAAAEICAYAAAC6fYRZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8li6FKAAAgAElEQVR4nO3debwcZZ3v8c+XhAAJS8BEliQQkOiAiIIZxGWujKACKngd7x0YGVER9I6MuIxeGGcQUcdxGRWFUUGRcQMzoEzEKF7ZFJAlKIMsRsKahCVh35fA7/7xPAcq7Tnpqj7dp6urv+/X67xOd1X1U7+qp+pXVc9T3aWIwMzMmmmdfgdgZma94yRvZtZgTvJmZg3mJG9m1mBO8mZmDeYkb2bWYLVM8pIekrRdv+MoQ9LWOd5J/Y6lWyTNlRSSJvdp/q+UdH1er2/uctnXSNqjy2WeIulT3Syz6Yr1IOkYSd/rc0i1IOltkn7RzTJ7nuQlHSXpZy3Drh9j2AEAEbFhRNzY69i6ISJuzfE+NZ5yJJ0v6d3dimvAHQscn9frmd0sOCJeGBHnd7PMiSDpZkmP5gPfyN/x/Y6rU4NaD70WEd+PiNd1s8yJOJP/FfCKkTNdSVsC6wK7tAzbPk9rDdLh1cA2wDXdjqUB3pQPfCN/h3ez8H5duVlvTUSSv5yU1F+S3/8FcB6wpGXYDRFxG0BuKtg+vz5F0gmSfirpQUmXSnpeHidJX5K0UtIDkn4vaafRgpD0TknX5TJulPSelvEflXS7pNskvbslhjdI+l2exzJJxxQ+t0bTRj4j/6Ski/K8fiFpRh63vqTvSbpb0n2SLpe0uaRP53Vw/FhnaIX5HCzpVkl3SfpYYfwaTQaS9pC0vPD+ZkkfkXSVpIclfSvP+2c5zl9K2rRltu/K6+N2Sf9QKGsdSUdKuiEvywJJm7XEeYikW4Fzx6iPQyUtlXSPpIWStsrDbwC2A36S18V6o3x2K0lnSFol6SZJ7y+MO0bS6ZJ+mJfrt5Je3LIe9sqvd5O0ONfrnZK+WJhuP6Umhftyne5QGLdLLvdBST8E1m+J742SrsyfvVjSzoVx/1fSivzZJZL2HG39lCVpvTyfnQrDZiqd9T+3RDw355iuAh7O28gZLfP4iqTjxpj/qMtTpR5ayltX0qm5fqesbVsb5bMXSPqr/PqVeTt8Q36/p6Qr8+vnSTo3l3eXpO9Lmt5umUaZ35h5YRyxvUPShYXPhaT3KrV03KeUCzXWfEYVET3/IyX1D+bXxwPvAj7dMuzkwvQBbJ9fnwLcDewGTAa+D5yWx70euAKYDgjYAdhyjBjeADwvT/dq4BFg1zxub+AO4IXAVOB7LTHsAbyIdFDcGbgTeHMeNzdPOzm/Px+4AXg+sEF+/6953HuAn+R5TAJeCmxc+Ny717IOR+ZzUi73xcDjwA6F9fSpwvR7AMsL728GLgE2B2YBK4HfAruQktS5wMdb5nUqMC0v+ypgrzz+iFzWbGA94BvAqS2f/U7+7AajLMtrgLuAXfPnvwr8qiXWvcZYD+vkOj8amEI6INwIvD6PPwZ4Engr6eTiH4CbgHVbywZ+A/xtfr0hsHt+/XzgYeC1uYyPAkvz/KYAtwAfzOPemuf3qfzZXfK6fVmu44PzPNcDXgAsA7YqrKvnldyH1rZOTgY+XXj/PuDn7eIplHslMIe0XW2Zl316Hj85f/6lo8x3zOWpWA/HkPa5DYCfkrblSe22tVHiORb4an79j6T98LOFccfl19vnul0PmElqQfhyu2UaZX57MEZeGEds7wAubMmFZ5Fy3Nak/XDvSvm308RdaSapEn+cX/83MI+UWIvDDm5ZsGKS/2Zh3L7AHwrJ4o/A7sA6FWM6EziisJN8pjBu+2IMo3z2y8CXChtBa5L/p8K0f8ezO9y7gIuBnUcp83zKJfnZhWGXAQcU1lO7JP+2wvszgK8V3v89cGbLvP6sMP5zwLfy6+uAPQvjtiTt0JMLn91uLcvyLeBzhfcb5s/PLcQ6VkJ7GXBry7CjgG8XtrVLCuPWAW4H/qK1bNLO/QlgRkt5/wwsaCljRV6n/wO4DVBh/MU8m+S/BnyypbwlpBOL7UkJcy9ysquwvd4MPATcV/g7NI/bi3QlPDLtRcDb28VTKPddLeN/Vij7jcC1Y8Q05vJUrIdjgIXABcBXWtbtmNvaKPHsCVyVX/8cePdIDLnst4yxHG8GftdumUrU0TN5odPYGD3Jv6rwfgFwZJW4Juruml8Br8qXWTMj4nrSjvGKPGwn1t4ef0fh9SOkpEBEnEu6CjgBWCnpREkbj1aApH0kXaLUPHAf6WAxI4/einT0HrGs5bMvk3Rebh64H3hv4bOl4wW+C5wNnKbUDPI5SeuupZwqZZdxZ+H1o6O8by2ruB5uIa0nSG3mP86Xj/eRdsSnSFcJo3221Va5PAAi4iHS1dqsEsuwDbDVyLzz/P9xrHlHxNPA8kLsRYeQztr/oNR09sYx4ns6lzkrj1sReY/Lbim83gb4cEt8c0hnhkuBD5CS2kpJpyk3U5X05oiYXvg7KQ8/D5iat9O5pGbQH7eLp1Bua139B3BQfn0Qabv9EyWWp2w9QDpR25l01Vtct2W2tRG/AZ4vaXPSOvgOMEepuXQ3co5RaqY8LTfJPEC6iphRcpmeUTEvlIptDOPZ5ycsyf8G2AQ4lHSWQUQ8QDojOhS4LSJu6qTgiPhKRLwU2JG0w36kdRqldt0zgC8Am0fEdGARqekG0hnG7MJH5rQU8QPSmcaciNgE+Hrhs1VifTIiPhEROwKvIJ0lvX1kdNXyWjxMagYascU4y4M118PWpPqCtPPu05Jw1o+IFYXp17Y8t5F2XgAkTQOeQzpbbmcZcFPLvDeKiH1Hi1vSOqS6va21oIi4PiIOBJ4LfBY4PcfSGp9ymStI28qslnbRrVvi+3RLfFMj4tQ8zx9ExKty+ZHnOy6R7uxaAByY/86KiAfLxDNSREuRZwI7K7Xzv5HURDrWvNe2PKXqIfsF8BngnJwIR5TZ1kZieYTUlHcEcHVEPEE6mfwQ6Urnrjzpv+RYXxQRG5MOZCqUU7aOSueFCrF13YQk+Yh4FFhMWqBfF0ZdmId1dFeNpD/PR9N1SUnuMeDpUSadQmp/WwWslrQPULxNaQHwTkk7SJpKulwv2gi4JyIek7Qb8DcdxvuXkl6kdFfRA6TLzpF47yS1L3fqSmBfSZtJ2oJ0NjJe/yxpqqQXAu8EfpiHfx34tKRt4JmOvv0rlHsqaX2/JB+A/wW4NCJuLvHZy4AHc+fYBpImSdpJ0p8XpnmppLcodYZ/gNR3cUlrQZIOkjQzn2Xelwc/Tdoe3pA7xNYFPpzLuJh0wrIaeL9SJ+FbSGdiI04C3pu3S0maljvoNpL0Akmvycv8GOnqabTttRM/AP4aeFt+3TaesQqKiMeA03M5l0XEraNNV2J5StVDYb6fy/M8J5/hQvVt7QLg8PwfUjNo8T2k/fkh4H5JsyicGFaso6p5oUxsXTeRX4a6gHTGdGFh2K/zsE5vndyYtBHfS7pkvhv4fOtE+azm/aSd915SZSwsjP8ZqS3wPFIH28iG+Hj+/3fAsZIeJHX4Legw3i1IO88DpMvOC3j2Uvg44K2S7pX0lQ7K/i6pb+Nm0lnRD9c6dTkXkNbHOcAXImLkSxrHkdbfL/I6uYTUVl5KRPySdCA9g3Rm/DzggJKffYp0dvkSUkfeXcA3SVeKI/6LlPDuBf6W1N755CjF7Q1cI+mhvEwHRMSjEbGEdHb31Vz+m0i3Lz6Rz8DeQmo7vSfP50eF+BaTrk6Pz/NfmqeFdKLxr7nMO0jb/lHwzJdg2t02OnLH0cjfSJMMEXEp6URnK1Kbepl41uY/SJ2KozbVtFuerGw9PCMiPkm6kvilUlNu1W3tAlLy/dUY7yH1w+wK3E/q6P1RYVy7ZSqqmhfKxFaapL/I2+7ap1uz+csAlG6Xu5p0B8Lqfsdj5SndxrZ9RBzUblobm6StgT8AW+Sm1aqfPwbXQy3U8mcN+kHS/1S653hTUhvcT5zgbRjl9vMPkW5VrpzgrV6c5J/1HtKtUzeQeu//T3/DMZt4ueP5AdJ95B/vczjWBW6uMTNrMJ/Jm5k1WN9+kGjGjBkxd+7cfs3ezGwgXXHFFXdFxMyy0/ctyc+dO5fFixf3a/ZmZgNJ0i3tp3qWm2vMzBrMSd7MrMGc5M3MGsxJ3syswZzkzcwarG2Sl3Sy0uP1rh5jvJQeD7ZU6dFyu3Y/TDMz60SZM/lTSL/WN5Z9SE96mgccRnoSjZmZ1UDbJB8RvyL9pOpY9ge+E8klwHRJW3YrwLq566HH+fnVd7SfMLto6V3cdNfDPYyof36//H6uWn5f+wnH6dIb7+b6Ox9sP2ENXfDHVSy755F+hzGqx558imN/ci2/vfXefofSdxdefxe33D3+/fSGVQ9x8Q09e/5HR7rRJj+LNR8ftpwxHuMm6TBJiyUtXrVqVRdmPfHe8e3LeO/3ruDBx9b6s9jPeNs3L+Uvv3B+b4PqkzcdfyH7HX9Rz+fz1ydewmu/1OkjB/rr4JMvY4+a1v+//WIJJ190E2/594v7HUrfHfStS3n1588fdzl7/tsF/M1Jl44/oC6a0I7XiDgxIuZHxPyZM0t/K7dWbr07nZU93a3n+VjjPfV0PX8EcOWDj7efyAZeN5L8CtZ8Fuhsyj2r08zMeqwbSX4h8PZ8l83uwP0RcXsXyjUzs3Fq+wNlkk4F9gBmSFpOepDAugAR8XVgEbAv6dmRj5Ae+GxmZjXQNslHxIFtxgfwvq5FZGZmXeNvvJqZNZiTvJlZgznJm5k1mJO8mVmDOcmbmTWYk7yZWYM5yZuZNZiTvJlZgznJm5k1mJO8mVmDOcmbmTWYk7yZWYM5yZuZNZiTvJlZgznJm5k1mJO8mVmDOclXVM9HMptVF96Yh4KTfIfC6d7MBoCTfEXqdwBmXSJvzEPBSb5DvtQ1s0HgJG9m1mBO8mZmDeYk3yG31pjZIHCSNzNrMCf5DoV7Xs1sADjJm5k1mJO8mVmDOcl3yI01ZjYInOTNzBrMSd7MrMGc5Dvkm2vMbBCUSvKS9pa0RNJSSUeOMn5rSedJ+p2kqyTt2/1QzcysqrZJXtIk4ARgH2BH4EBJO7ZM9k/AgojYBTgA+PduB1o3/qlhMxsEZc7kdwOWRsSNEfEEcBqwf8s0AWycX28C3Na9EM3MrFNlkvwsYFnh/fI8rOgY4CBJy4FFwN+PVpCkwyQtlrR41apVHYRrZmZVdKvj9UDglIiYDewLfFfSn5QdESdGxPyImD9z5swuzbpP3FpjZgOgTJJfAcwpvJ+dhxUdAiwAiIjfAOsDM7oRoJmZda5Mkr8cmCdpW0lTSB2rC1umuRXYE0DSDqQk3+j2GJ/Im9kgaJvkI2I1cDhwNnAd6S6aayQdK2m/PNmHgUMl/TdwKvCO8M80mpn13eQyE0XEIlKHanHY0YXX1wKv7G5oZmY2Xv7Ga4d8nWLt+GLW6sBJ3syswZzkO+RvvFo7PpG3OnCSNzNrMCd5M7MGc5LvkC/FrR1vIlYHTvJmZg3mJN8hn6VZO76F0urASd7MrMGc5M3MGsxJvkO+FLd2vIVYHTjJm5k1mJO8mVmDOcl3yK011o63EasDJ/mKvN9aU/ggNByc5M16xD9iZ3XgJF+R+h2AWZfIG/NQcJI3s6HW9NuhneQ71PDtwrrA24jVgZN8Rd5vrSl8EEqavh6c5DvkTjUzGwRO8hW5r8qawh2vw8FJ3syGWtOvyZ3kO9T0djwbP28jVgdO8hV5v7Wm8EEo8S2UNqoym0XTNx5bO3fOWx04yVfkviprCne8DgcneTMbak2/3nKS71CZphi31gw317/VgZN8Rd5vrSl8EEqavh5KJXlJe0taImmppCPHmOZ/S7pW0jWSftDdMOunVMdrz6OwOnP9Wx1MbjeBpEnACcBrgeXA5ZIWRsS1hWnmAUcBr4yIeyU9t1cB95v7qqwp3PE6HMqcye8GLI2IGyPiCeA0YP+WaQ4FToiIewEiYmV3wzQz642m3+paJsnPApYV3i/Pw4qeDzxf0kWSLpG092gFSTpM0mJJi1etWtVZxDVRph3P98kPN9e/1UG3Ol4nA/OAPYADgZMkTW+dKCJOjIj5ETF/5syZXZq1mZmNpUySXwHMKbyfnYcVLQcWRsSTEXET8EdS0jczq7WmX3CVSfKXA/MkbStpCnAAsLBlmjNJZ/FImkFqvrmxi3HWUIn75CcgCqsv17/VQdskHxGrgcOBs4HrgAURcY2kYyXtlyc7G7hb0rXAecBHIuLuXgVtZlZndeqPaXsLJUBELAIWtQw7uvA6gA/lv6FQruO193FYfbn+rQ78jVczswZzkjezodaLK646XcU5yXeo3M8a1KimbeK5+q0GnOTNbKj14mSsTsd3J/kOuePV2vGVnNWBk7yZWYM5yZvZUOtNx2t9ruKc5DvkS3Frp0b7uQ0xJ/mKvN9aU/gglPRiNdRp1TrJd8gdr9aOq9/qwEm+Ij9Mx5rCT4YaDk7yZjbUetFJWqereCf5DpVqrvEF+1Cr0x0WNryc5CvybjuxnCh7x6s26U3Ha31WrpN8h8pUonei4ebqtzpwkq/IfVXWFO54HQ5O8lZrvhqyXvNPDduoynW82jCr045uw8tJviLvtxPL67t3fBDKGr4enOTNzBrMSb6iKn1Vvv1vuNXpNrrRuON1ODjJW635QGm91pMnQ9Vos3WS75A7Xq0tbwBWA07yFXm/nVhe371Tp7PNfurJLZQ12nKd5M3MGsxJvqKRvir/rIG1U/fqd8frcHCSt1rzgdJ6rSc/UFaj7dZJvkOlKrFGFW0Tr047ug0vJ/mKvN9OrDp1YDWND0JJTx4a0vUSO+ckb2bWYE7yFT3b8dqez0KHW93r3x2vw6FUkpe0t6QlkpZKOnIt0/2VpJA0v3sh2jBzk4L1Wm86Xuuz4bZN8pImAScA+wA7AgdK2nGU6TYCjgAu7XaQdVSmEmtUz9YHrn+rgzJn8rsBSyPixoh4AjgN2H+U6T4JfBZ4rIvx1Y73W2sKH4SS3nzjtT7KJPlZwLLC++V52DMk7QrMiYifrq0gSYdJWixp8apVqyoHa2Zm1Yy741XSOsAXgQ+3mzYiToyI+RExf+bMmeOddV9U63i1YVb3+nfH63Aok+RXAHMK72fnYSM2AnYCzpd0M7A7sNCdr9YNblKwXvNPDcPlwDxJ20qaAhwALBwZGRH3R8SMiJgbEXOBS4D9ImJxTyI2M7PS2ib5iFgNHA6cDVwHLIiIayQdK2m/XgdYNyMH6FK/J1+nw/mAqvu95mtT9/qveXgTpyf3UPagzA5NLjNRRCwCFrUMO3qMafcYf1hmZtYN/sZrRc/2VZW4T76XgVjt1f1M2R2vw8FJ3mqt7onSBl9vWmvqs+E6yZuZNZiTfIfKdbz2Pg4zs7Vxkrda83HSeq0nP2tQow3XSb5D/qlha6dOO7oNLyf5irzfTqy632s+yLxqk55847XrJXbOSd7MrMGc5Ct65gfK/Atl1kbdm+t8n/xwcJK3Wqt3mrQm6E3Ha322XCf5DpV6MtQExGH1VaP93IaYk3xF3m8nlhNl73jdJg3/fTIneTOzJnOSr6jSk6HqdDi3CVf36nfH63Bwkrd6q3umtIHXi07SOp3gOcl3qNRv1zhDDbU63WHRziDFatU4yVfkXWFi+UDZO87rSU9uoazRduskb2bWYE7yFT3b8VriPvn6HMytD+pe/cWOV2+rzeUkb7Xm5GMDqUbbrZO8mVmDOclXFH/yosS01rFBXod1vwopxlfzUHuqNx2v9eEkb2bWYE7yFVX7xmudjuc28epd/2t2vNY7Vuuck7zVmpOP9VpPngxVo83WSd7MrMGc5CsaOUCX+lmDGh3NB9Ugr8K61787XhN/49XMzAaWk3xFVb7xasOt7luIv/E6HJzkrdacfKzXevJkqBptt6WSvKS9JS2RtFTSkaOM/5CkayVdJekcSdt0P1QzM6uqbZKXNAk4AdgH2BE4UNKOLZP9DpgfETsDpwOf63agdeGO14k1yM1ida//NTteax5sD/XkoSFdL7FzZc7kdwOWRsSNEfEEcBqwf3GCiDgvIh7Jby8BZnc3TDMz60SZJD8LWFZ4vzwPG8shwM9GGyHpMEmLJS1etWpV+ShrpNI3Xmt1PLeJVvf6d8frcOhqx6ukg4D5wOdHGx8RJ0bE/IiYP3PmzG7O2prKycd6rDcdr/XZcCeXmGYFMKfwfnYetgZJewEfA14dEY93JzwzMxuPMmfylwPzJG0raQpwALCwOIGkXYBvAPtFxMruh1kfz3a8+slQI3p51jLIq7Du9V/3+CZKT77xWqN12zbJR8Rq4HDgbOA6YEFEXCPpWEn75ck+D2wI/KekKyUtHKM4MzObQGWaa4iIRcCilmFHF17v1eW4aqtax6sNszqdzY3GHa/Dwd94tXHrZYJw8rHea/ZG5iRvZtZgTvIVVXrGa4NPQ4vL1sulrPu95mtT99j9jddk6DtezcxscDnJV6T2k5gNBHljHgpO8hU9c598icvbGl2xdd0al/q9vE9+gFdi3WNfsw77F0e/9eQbrzXa+53kzcwazEm+Q8P+U8Mxxutezsd6Z5jXsztezcxsYDnJV+S+KmsKd7wOByf5iqo8GarJF8Fr3Cff02+8Du46rHvoE9V5Xne96CSt09p0kjczazAn+Q6V+oGyOh3Ou2zNjlffQjmaOt1G187gRNp9vel4rc8adZI3M2swJ/mK3FdlTeGO1+HgJF9RpSdD9TaUvvK3Jdur+3pxHSY9aa7pfpEdc5I3M2swJ/mKKj0Zqk6H8y4bpE7Ffqn7GlqjuabuwVrHnOSt1pp8oLR66Ml98jXabp3kzcwazEm+oirfeG1yk8ZEddoN8jqs073So/GToZLeVFN91qeTvJlZgznJV/RsX1WJWyjrczC3Pqh79Rc7Xr2tNpeTvI2bf9ZgMHnd9k6d1q2TvJlZgznJV1Sp47VGR/Num7iO18FV9/qfqKd71Z2/8WpmZgPLSb4i/6aTmQ0SJ/kOlfpZg1pdtHVXcdl6+iDvurd5rFW9Y1/z6V71jrWX/I1XMzMbWE7yFbnjNZmo54MO8iqse/274zXpTcdrfdZoqSQvaW9JSyQtlXTkKOPXk/TDPP5SSXO7HaiZmVXXNslLmgScAOwD7AgcKGnHlskOAe6NiO2BLwGf7XagdeGOVzMbJGp3qS3p5cAxEfH6/P4ogIj4TGGas/M0v5E0GbgDmBlrKXz+/PmxePHiygEvuHwZJ/36xsqf65brVz4EwBYbr89G609e67SPrX6KZfc8CsC8527Y89gm0lMR3LjqYQC2mzmNST16ltwTTz3NLXc/AgzeOnz0yadYfm996/+Wex7hidVPA7DdjGlMWmc4T2G6uZ+O5IetN5vKepPHPod+/57zeNOLt+poHpKuiIj5Zadfe5ZKZgHLCu+XAy8ba5qIWC3pfuA5wF0twR0GHAaw9dZbl41xDdOnrsu8zfu3w8ydMY3f3nIvu24zvdT060hsucn6bDZtSo8jm3hTp0wiArZ5ztSezmfSOuI506Ywc6P1ejqfXoiAbWdMY+MNyuxqE2ve5huy6Pd3sNOsjdl6s97W4SCYs+lUpk9dd1xlbDZtCisffJwdttxordNtssH45lPFhG55EXEicCKkM/lOynjdC7fgdS/coqtxmZk1VZmO1xXAnML72XnYqNPk5ppNgLu7EaCZmXWuTJK/HJgnaVtJU4ADgIUt0ywEDs6v3wqcu7b2eDMzmxhtm2tyG/vhwNnAJODkiLhG0rHA4ohYCHwL+K6kpcA9pAOBmZn1Wak2+YhYBCxqGXZ04fVjwP/qbmhmZjZe/sarmVmDOcmbmTWYk7yZWYM5yZuZNVjbnzXo2YylVcAtHX58Bi3fph0CXubh4GUeDuNZ5m0iYmbZifuW5MdD0uIqv93QBF7m4eBlHg4TucxurjEzazAneTOzBhvUJH9ivwPoAy/zcPAyD4cJW+aBbJM3M7NyBvVM3szMSnCSNzNrsIFL8u0eKj6oJM2RdJ6kayVdI+mIPHwzSf9P0vX5/6Z5uCR9Ja+HqyTt2t8l6IykSZJ+J+ms/H7b/DD4pfnh8FPy8EY8LF7SdEmnS/qDpOskvXwI6viDeZu+WtKpktZvYj1LOlnSSklXF4ZVrltJB+fpr5d08GjzqmKgknzJh4oPqtXAhyNiR2B34H152Y4EzomIecA5+T2kdTAv/x0GfG3iQ+6KI4DrCu8/C3wpPxT+XtJD4qE5D4s/Dvh5RPwZ8GLSsje2jiXNAt4PzI+InUg/V34AzaznU4C9W4ZVqltJmwEfJz1idTfg4yMHho5FxMD8AS8Hzi68Pwo4qt9x9WhZ/wt4LbAE2DIP2xJYkl9/AziwMP0z0w3KH+kpY+cArwHOAkT6FuDk1vomPc/g5fn15Dyd+r0MFZd3E+Cm1rgbXscjz3/eLNfbWcDrm1rPwFzg6k7rFjgQ+EZh+BrTdfI3UGfyjP5Q8Vl9iqVn8iXqLsClwOYRcXsedQeweX7dhHXxZeCjwNP5/XOA+yJidX5fXKY1HhYPjDwsfpBsC6wCvp2bqL4paRoNruOIWAF8AbgVuJ1Ub1fQ7Houqlq3Xa/zQUvyjSdpQ+AM4AMR8UBxXKRDeyPueZX0RmBlRFzR71gm0GRgV+BrEbEL8DDPXr4DzapjgNzUsD/pALcVMI0/bdIYCv2q20FL8mUeKj6wJK1LSvDfj4gf5cF3Stoyj98SWJmHD/q6eCWwn6SbgdNITTbHAdPzw+BhzWVqwsPilwPLI+LS/P50UtJvah0D7AXcFBGrIuJJ4Eekum9yPRdVrduu1/mgJfkyDxUfSJJEelbudRHxxcKo4kPSDya11Y8Mf3vupd8duL9wWVh7EXFURMyOiLmkejw3It4GnEd6GDz86fIO9MPiI+IOYJmkF+RBewLX0tA6zm4Fdpc0NW/jI8vc2HpuUbVuzwZeJ2nTfBX0ujysc/3uqOigY2Nf4I/ADcDH+h1PF5frVaRLuauAK/PfvqT2yHOA64FfApvl6UW6035V19UAAACOSURBVOgG4Pekuxf6vhwdLvsewFn59XbAZcBS4D+B9fLw9fP7pXn8dv2Ou8NlfQmwONfzmcCmTa9j4BPAH4Crge8C6zWxnoFTSf0OT5Ku2g7ppG6Bd+XlXwq8c7xx+WcNzMwabNCaa8zMrAIneTOzBnOSNzNrMCd5M7MGc5I3M2swJ3kzswZzkjcza7D/D2hCjUbm3da1AAAAAElFTkSuQmCC\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"markdown","metadata":{"id":"ih_K0q0ApOLq"},"source":["The outputs you get will depend on when you run the code, but here are the outputs from a specific run for reference:\n","\n","```python\n","Number of Logs: 5\n","S: 0, a: 1, s1: 4, r: 0.0, d: False\n","S: 4, a: 1, s1: 8, r: 0.0, d: False\n","S: 8, a: 0, s1: 8, r: 0.0, d: False\n","S: 8, a: 2, s1: 9, r: 0.0, d: False\n","S: 9, a: 2, s1: 10, r: 0.0, d: False\n","S: 10, a: 1, s1: 14, r: 0.0, d: False\n","S: 14, a: 1, s1: 14, r: 0.0, d: False\n","S: 14, a: 1, s1: 14, r: 0.0, d: False\n","S: 14, a: 2, s1: 15, r: 1.0, d: True\n","Percentages of Wins: 1.0%\n","Number of steps that were required for each win: [8, 11, 10, 15, 8]\n","```\n","\n","![image.png](attachment:image.png)\n","\n","Here we see that the state sequence for this win was 0, 4, 8, 9, 10, 14, 15 -- though it is worth noting that we got stuck at state 8 and 14 for multiple steps.\n","\n","For that run there were 5 wins out of 500 -- representing a 1% chance of making it to the end of the route. "]},{"cell_type":"markdown","metadata":{"id":"ibxKjiLkpOMj"},"source":["### Getting Slippery \n","In the real world the successful outcome of an action is often subject to chance. To model this, let's re-run things with the true version of the Frozen Lake game. Remember, in this version we aren't guaranteed that a command to move forward is definitely going to move us forward -- there is a chance we slip and move to a different square. \n","\n","Let's load up the environment and play the game with the random action selection mechanism. "]},{"cell_type":"code","metadata":{"id":"LVg-tQgwpOMk","outputId":"97f114be-f5f5-4f61-cede-8383ec6b9450"},"source":["# let's get the real FrozenLake game - slippery by default\n","slippery_frozen_env = gym.make('FrozenLake-v0')\n","game = RandomFL(slippery_frozen_env,1000)\n","game.playFrozenLake()\n","game.printResults()\n","print(\"\\nTest results: % 12.2f\" % game.test(500))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["0, 100, 200, 300, 400, 500, 600, 700, 800, 900, \n","Test results:         1.60\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"markdown","metadata":{"id":"nk-sXAvgpOMk"},"source":["If you play the game, you should notice two things. First, when you look at the trace displayed for the game you will see that the actions (1,2,3,4) do not always have the effect that you expect them to. If we dig into the code of the Frozen Lake world we discover that the frozen model, isn't just a little slippery, but a lot slippery. Specifically in whichever direction we aim to move, there is an equal chance that we could rotate to the left or the right relative to the direction in which we wanted to move. In other words, if we aim to move up, there, we have a 1/3 chance of moving up, left or right. If we aim to move right, we could end up turning up, right, or down with equal probability. We will see this randomness in the traces we printed. \n","\n","Second, the overall success is not particularly better or worse than in our first attempt to play the game. This should be unsurprising as the slippery property does not change the fact that every step we take is basically random. "]},{"cell_type":"markdown","metadata":{"id":"71cWlrZwpOMl"},"source":["## Approach 2: Hand Crafted Policy\n","\n","Rather than taking a random walk, we could hand craft a policy for this specific scenario. Our hand crafted policy should define an action to be taken at each of our available states. While we have 16 states in total, we know that the game should not allow us to take actions if we enter a hole or goal state (victory laps are not possible). Therefore a policy to map an action to each of our 16 states (in the case of a non-slippery environment) might look like this:\n","\n","|        | Left | Down | Right | Up |\n","|-|-|-|-|-|\n","|State 0  | 0.0 | 1.0 | 1.0 | 0.0 |\n","|State 1  | 0.5 | 0.0 | 1.0 | 0.0 |\n","|State 2  | 0.5 | 1.0 | 0.5 | 0.0 |\n","|State 3  | 1.0 | 0.0 | 0.0 | 0.0 |\n","|State 4  | 0.0 | 1.0 | 0.0 | 0.5 |\n","|State 5  | 0.0 | 0.0 | 0.0 | 0.0 |\n","|State 6  | 0.0 | 1.0 | 0.0 | 0.5 |\n","|State 7  | 0.0 | 0.0 | 0.0 | 0.0 |\n","|State 8  | 0.0 | 0.0 | 1.0 | 0.5 |\n","|State 9  | 0.5 | 1.0 | 1.0 | 0.0 |\n","|State 10 | 0.5 | 1.0 | 0.0 | 0.5 |\n","|State 11 | 0.0 | 0.0 | 0.0 | 0.0 |\n","|State 12 | 0.0 | 0.0 | 0.0 | 0.0 |\n","|State 13 | 0.0 | 0.0 | 1.0 | 0.5 |\n","|State 14 | 0.5 | 0.0 | 1.0 | 0.0 |\n","|State 15 | 0.0 | 0.0 | 0.0 | 0.0 |\n","\n","Here we have assigned a value of 1.0 to any actions that move us positively towards the goal, 0.0 to any actions that will cause a fall or not result in any movement, and 0.5 to any actions that take us backwards. We can code this up as a table and select our actions from this map. Since we are going to be looking at q-learning shortly, we will call this the Q-Table. "]},{"cell_type":"code","metadata":{"id":"vDceK5g0pOMm"},"source":["Q = np.array([\n","    [0.0, 0.8, 1.0, 0.0],\n","    [0.5, 0.0, 1.0, 0.0],\n","    [0.5, 1.0, 0.5, 0.0],\n","    [1.0, 0.0, 0.0, 0.0], \n","    \n","    [0.0, 1.0, 0.0, 0.5], \n","    [0.0, 0.0, 0.0, 0.0],\n","    [0.0, 1.0, 0.0, 0.5],\n","    [0.0, 0.0, 0.0, 0.0], \n","    \n","    [0.0, 0.0, 1.0, 0.5],\n","    [0.5, 1.0, 1.0, 0.0],\n","    [0.5, 1.0, 0.0, 0.5],\n","    [0.0, 0.0, 0.0, 0.0],\n","    \n","    [0.0, 0.0, 0.0, 0.0],\n","    [0.0, 0.0, 1.0, 0.5], \n","    [0.5, 0.0, 1.0, 0.0], \n","    [0.0, 0.0, 0.0, 0.0]\n","])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"l4IDRWLIpOMm"},"source":["We also need to redinfe our action selection function to select the most relevant action from our table. "]},{"cell_type":"code","metadata":{"id":"0J2oG92wpOMn"},"source":["class HandCraftedFL(FrozenLakeFramework):\n","    \"\"\"Hand Crafted based approach to Frozen Lake\"\"\"\n","    \n","    def __init__(self, env, num_episodes,Q):\n","        FrozenLakeFramework.__init__(self,env,num_episodes)\n","        self.Q = Q\n","\n","    def selectAction(self,j,s,i):\n","        \"\"\"\n","        Select an action from a Q table. \n","\n","        Parameters: \n","        env (environment): Instance of the Frozen Lake game enviornment\n","        j (int): number of steps taken in the game \n","        s (int): current state of the game\n","        i (int): training episode number    \n","\n","        Returns: \n","        int: indicates the next action to be performed. \n","        \"\"\"\n","        return np.argmax(self.Q[s,:])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"t-0R483rpOMn"},"source":["Lets reset and play with the deterninistic environment. "]},{"cell_type":"code","metadata":{"id":"71GXeLSgpOMn","outputId":"97097cd9-79d5-47c7-92fb-6210853a847c"},"source":["game = HandCraftedFL(frozen_env,1000,Q)\n","game.playFrozenLake()\n","game.printResults()\n","print(\"\\nTest results: % 12.2f\" % game.test(500))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["0, 100, 200, 300, 400, 500, 600, 700, 800, 900, \n","Test results:       100.00\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"markdown","metadata":{"id":"hPRZdHo0pOMo"},"source":["Unsurprisingly we always win -- with an optimum policy that takes only 6 steps to work through the environment. \n","\n","#### Application to the Slippery Environment\n","\n","If we try using the same policy but with the slippery environment, we find that the results aren't very good at all. That is because the slippery environment means that some actions from our optimal policy are now sub-optimal. For example, deciding to move down when in State 6 is a bad idea as there is a more likely chance that we will end up in states 5 or 7 rather than progressing on to state 10 as expected. \n","\n","Taking that point into consideration, a better policy might be one like this:"]},{"cell_type":"code","metadata":{"id":"1i1QJnhVpOMo"},"source":["Q = np.array([\n","    [1.0, 0.0, 0.0, 0.0],\n","    [0.0, 0.0, 0.0, 1.0],\n","    [0.0, 0.0, 0.0, 1.0],\n","    [0.0, 0.0, 0.0, 1.0], \n","    \n","    [1.0, 0.0, 0.0, 0.0], \n","    [0.0, 0.0, 0.0, 0.0],\n","    [0.0, 0.0, 1.0, 0.0],\n","    [0.0, 0.0, 0.0, 0.0], \n","    \n","    [0.0, 0.0, 0.0, 1.0],\n","    [0.0, 1.0, 0.0, 0.0],\n","    [1.0, 0.0, 0.0, 0.0],\n","    [0.0, 0.0, 0.0, 0.0],\n","    \n","    [0.0, 0.0, 0.0, 0.0],\n","    [0.0, 0.0, 1.0, 0.0], \n","    [0.0, 0.0, 0.0, 1.0], \n","    [0.0, 0.0, 0.0, 0.0]\n","])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VrAPovNhpOMp"},"source":["This policy has us always selecting actions that point away from a hole state. A notable exception for this is state 6 where we actively point towards a hole. This is because given the slippery property this actually reduces the chance of falling into a hole relative to if we had decided to use moving up or moving down as the selected action in that state. \n","\n","Let's see how this policy actually performs on the slippery environment. "]},{"cell_type":"code","metadata":{"id":"YTxYxbHMpOMp","outputId":"953cb962-7fba-47f2-d262-fcc81134d7e1"},"source":["game = HandCraftedFL(slippery_frozen_env,1000,Q)\n","game.playFrozenLake()\n","game.printResults()\n","print(\"\\nTest results: % 12.2f\" % game.test(500))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["0, 100, 200, 300, 400, 500, 600, 700, 800, 900, \n","Test results:        57.20\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"markdown","metadata":{"id":"GqmY5V9_pOMp"},"source":["We can see that the policy certainly did better than chance on the slippery environment -- though performance is still far from perfect as there are several actions we have to take that could lead us to slipping into a hole. We see that a 50-60% chance is our upper bound on what we can expect to get from a policy. "]},{"cell_type":"markdown","metadata":{"id":"HCJbHrzapOMq"},"source":["## Q Learning\n","\n","In the code above we have used hand crafted policies to illustrate the properties of the world, but what we are really interested in is a way to learn these properties automatically rather than hand crafting them. To do this we will take advantage of the idea of the reward, and use an equation that allows us to have that reward be distributed back to early points in the game -- even though we only ever see the reward at the last step of the game. \n","\n","What we need to do is learn the utility of selecting a given action given a specific state. Traditionally we refer to this utility as the \"Quality\" of an action which we record in a table called the Q-Table. As in the examples above, the Q-Table's rows are the states of the environment, and the columns are the individual actions that are available to our agent. \n","\n","Each cell in the table represents the utility of selecting a given action in that state, and importantly the Q-Learning algorithm gives us an elegant way of updating the values in those cells based on our game playing experience. The update equation takes many forms but the most universal is: \n","\n","\\begin{equation*}\n","Q(s,a) = (1 - \\alpha)Q(s,a) + \\alpha(r + \\gamma (max(Q(s',a'))))\n","\\end{equation*}\n","\n","where $Q(s,a)$ is the utility or Q-value of the state action pair $(s,a)$; $\\alpha$ is our learning rate; $r$ is the reward picked up at the current point in time; $\\gamma$ is the potential influence of Q-values estimates that we have for the next step into the future, finally $max(Q(s',a'))$ defines the maximal Q-value available from our next state $s'$. \n","\n","There is a lot to unpack here, so let's take it one element at a time. \n","\n","First of all keep in mind that we are making an update to a Q-value for a given state action tuple. The state action tuple are defined as state $s$ action $a$, and the update we make doesn't completely overwrite what was there before. Therefore we can see that the new Q-value for that tuple is influenced in part by the current Q-value. To emphasize the fact that this is an assignment operation, I've used the assignment operator $\\leftarrow$ rather than a simple equals sign. \n","\n","Since we want to take account of the original value of the Q-value in our assignment, we need some way to balance the influence of the original Q-value versus the new information we are now going to add to it. To control this we make use of a learning rate $\\alpha$. We see that the learning rate provides a balance between the influence of the original Q-value and the element we will take from another part in the Q table. When $\\alpha$ is high we will pay more attention to the new information. When $\\alpha$ is low we will pay most attention to the existing Q-value and only slowly update that value. \n","\n","The new information $r + \\gamma (max(Q(s',a')))$ has two elements. First we have the potential influence of a reward that we have just received -- this is $r$. Second we have the potential influence of the Q-values from another part of the table. Specifically we can update by the maximum Q-value that is available in $s$ -- the state we transition to next given executing action $a$ in state $s$. Therefore if our current action leads to a state with a high Q-value rather than a low Q-value, our current Q-value will increase by a larger amount. This way positive Q-Values spread back through the network during the training process. The proportion of Q-value we get from the next step is mediated by a factor $\\gamma$. \n","\n","To illustrate, let's consider the updates we would get in a couple of key state-action pairs in the Frozen Lake environment. For the moment let's assume we are dealing with the non-slippery form of the environment. \n","\n","In state 15 (the goal state), we have already won and won't make any further moves. The reward component is 0, and since we will never move out of this state, the second component of the equation will always give us 0s as well -- we aren't going anywhere. Therefore the Q-value update (and hence the Q-value) of all actions in state 15 is always going to be 0. \n","\n","How about the case of state-action pairs that lead to state 15. Consider the case of selecting action 2 (move right) while in state 14. In this case we should get a reward of 1, i.e., $r=1$ according to the game rules. There will however be no value for the second component of the Q-value update as when we follow that action we end up in state 15 which we know will always have a Q-value of 0. Therefore our Q-value updates for action 2 in state 14 will always come directly from the reward. The same is true for state 11 action 1. \n","\n","Now how about state 10 when we select action 1, i.e., DOWN. Well the reward is always going to be 0, but since we move into state 14 which we know in turn will lead to state 15, there should be some reward. Therefore the q-value for this pair should update with some content over time. \n","\n","It is worth keeping in mind that this is an iterative process where rewards flow backwards from state-action pairs (14,2) and (10,1) in our game. \n","\n","To code this selection policy up, it is quite simple. First we will reset our Q table to 0s. Then we will define our action selection strategy to select the best action in a given state (subject to a bit of noise). We also redefine our update policy function to make sure that the Q-table gets updated after every run. \n","\n","One last thing, it is important to note that the update equation above is not recursive. Our update of the current state is based on the value for a different state that has been computed in a previous step. "]},{"cell_type":"code","metadata":{"id":"PWUn2_a2pOMs"},"source":["class QLearningFL(FrozenLakeFramework):\n","    \"\"\"Traditional Q-Learning based approach to Frozen Lake\"\"\"\n","    \n","    def __init__(self, env, num_episodes):\n","        FrozenLakeFramework.__init__(self,env,num_episodes)\n","        # Initialize the Q table with all zeros\n","        self.Q = np.zeros([env.observation_space.n,env.action_space.n])\n","        self.alpha = 0.8\n","        self.y = .95\n","\n","    def selectAction(self,j,s,i):\n","        \"\"\"\n","        Select an action from a Q table. \n","\n","        The action we select is due in part to the actual value recorded in the Q table, but \n","        also due to a random element that we introduce. The random element is quie high at the \n","        start of game play, but it decreases over time. Random selection is very important at the\n","        start when we have not yet won any games (q-table is all 0s) and need some process to \n","        select the actions. But as we move through the game play episodes the influence of this \n","        random element reduces. \n","\n","        Note that the addition of the random element is vectorised. \n","\n","        Parameters: \n","        j (int): number of steps taken in the game \n","        s (int): current state of the game\n","        i (int): training episode number\n","\n","        Returns: \n","        int: indicates the next action to be performed. \n","        \"\"\"\n","        # the influence of random noise will decrease over episodes (i)\n","        # we will have a random value for eacho the\n","        a = np.argmax(self.Q[s,:] + np.random.randn(1,self.env.action_space.n)*(1./(i+1)))\n","        return a\n","\n","    def updatePolicy(self,s,a,r,s1):\n","        \"\"\"\n","        Update the selection policy. \n","\n","        We apply the Q-Learning update policy. \n","\n","        Parameters: \n","        s (int): current state\n","        a (int): action selected\n","        r (int): reward for the state-action pair. \n","        s1 (int): the state we moved to after performing a. \n","        \"\"\"\n","\n","        # Update Q-Table with new knowledge\n","        self.Q[s,a] = (1-self.alpha)*self.Q[s,a] + self.alpha*(r + self.y*np.max(self.Q[s1,:]))\n","\n","        return          "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-DALxFnopOMt"},"source":["With our game defined, we can now run some training."]},{"cell_type":"code","metadata":{"id":"1O5xZ6e2pOMt","outputId":"d30e9ecd-fc27-40e0-a8ac-3acc40c2a3e4"},"source":["game = QLearningFL(slippery_frozen_env,2000)\n","game.playFrozenLake()\n","game.printResults()\n","print(\"\\nTest results: % 12.2f\" % game.test(500))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["0, 100, 200, 300, 400, 500, 600, 700, 800, 900, 1000, 1100, 1200, 1300, 1400, 1500, 1600, 1700, 1800, 1900, \n","Test results:        70.80\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"markdown","metadata":{"id":"7gnevIx9pOMu"},"source":["Looking at the plot of wins we we see that performance isn't great for the first 200-250 runs. Here and at the start in particular we are effectively randomly selecting actions. Since we have so few wins there is no reward to try to guide our progress -- this is a common problem for scenarios where the reward is only doled out at the end of the game. Nevertheless once we begin making progress we quickly start winning the game in the scenario of situations. Remember that it is impossible to always win due to the slippery nature of the environment. \n","\n","Let's have a look at the Q-Table that is produced. Remember that this is for the slippery environment, so the utility of actions in a given state don't necessarily correspond to our initial intuitions. "]},{"cell_type":"code","metadata":{"id":"3eW_rgfgpONq","outputId":"b3758f32-1a5e-4bd3-8f41-198156db3156"},"source":["print(game.Q)\n","print(np.sum(game.Q))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[[1.37044396e-01 4.21732050e-03 1.24419170e-02 7.57252804e-03]\n"," [8.42677422e-04 2.63297162e-03 8.37997991e-08 1.93520066e-01]\n"," [0.00000000e+00 1.89531550e-03 5.60884270e-03 5.98452532e-02]\n"," [1.23086728e-03 2.65264363e-04 8.80458524e-04 5.80250499e-02]\n"," [1.69623372e-01 5.98102437e-05 3.88915329e-03 7.95295140e-04]\n"," [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n"," [2.91909699e-06 3.73728001e-04 6.31498041e-02 4.99610711e-06]\n"," [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n"," [4.41661758e-04 4.32738893e-04 9.96542038e-04 3.65713592e-01]\n"," [8.88143070e-05 1.64112531e-01 3.66623485e-03 3.63990058e-04]\n"," [7.48765489e-01 0.00000000e+00 1.22645674e-03 2.57738112e-05]\n"," [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n"," [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n"," [9.31930876e-05 0.00000000e+00 7.34316445e-01 1.28319236e-03]\n"," [0.00000000e+00 9.91657296e-01 0.00000000e+00 0.00000000e+00]\n"," [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]]\n","3.7371060409261885\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"drj_gu-VpONr"},"source":["## Q-Learning with a Neural Network\n","\n","Now that we have a clear understanding of the Q-Learning process in general, we can sit down and substitute in a neural approach to the learning mechanism. To make this substitution we only need two things: 1. a mechanism to return utility values (and subsequently a selected action) for a given state; and 2. a mechanism to update the utility values based on the experience from applying the selected action to the current state. \n","\n","Thinking in deep learning algorithm terms, this should be pretty straightforward. Let's create a network that maps states as the input variable to Q-Values for that state as the output variable. Our input is a one hot encoding of state will hence have dimension [1,16]. Our output meanwhile [1,4] and corresponds to the utility of each of our four possible actions. For this process we don't need a true deep network -- we will simply use one two layers (input and output) that are fully connected but with no bias unit. Since our utilities are real valued, we won't even be bothering with an activation function. \n","\n","So far this is very straightforward. We predict utility values for a given state -- this is very similar to what the Q-table itself does for us. Where things get a bit more interesting is where we think about the application of the Bellman equation to update the utility values following some learning. Normally the Bellman equation lets us directly update the utility values for a given state-action pair -- that works for a static lookup table but does not work directly for a functional (neural) approach. Instead what we need to do is figure out a way to adjust the network such that after learning our network provides an updated utility value. \n","\n","Backpropagation gives us a mechanism to update our weights based on some loss -- can we use it in this case to update our utility values? The short answer is yes. Backpropagation takes the first order derivative of the loss between some prediction and a 'true' value and passes this back through the network adjusting weights such that our prediction better matches the 'true' value. To apply this to our learning case here we need to think about the loss in different terms. Instead the loss is the difference between our current predicted value and what the value should be after application of the Bellman equation. \n","\n","Beyond this conceptual difference in how we think of losses, the application of the neural approach is straightforward -- though more complex to set up than a static Q table. \n","\n","To implement the approach, let's begin by importing Tensorflow, setting up a variable to record training time, and then reset our computing graph if we have already been using it. "]},{"cell_type":"code","metadata":{"id":"wdE9WvCtpONs","outputId":"aadf8990-c975-4e52-9d76-6845bfdcd3be"},"source":["import tensorflow.compat.v1 as tf\n","tf.disable_v2_behavior()\n","import time \n","start_time = time.time()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From /Users/rob/virtualenvs/dl-env/lib/python3.7/site-packages/tensorflow_core/python/compat/v2_compat.py:65: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","non-resource variables are not supported in the long term\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"0xx4HYsJpONs"},"source":["Next, let's set up the actual graph elements. We have an input layer, weights, and an output layer which is the logit only. We also need a placeholder for our target value (the next Q value) as well as standard operations to compute loss and apply training. \n","\n","Next, let's set up our selectAction and updatePolicy functions. For selectAction we just need to apply the state to our variable and get the result. updatePolicy is a little bit more complex than we have dealt with previously. Fundamentally though, it is still achieving the same goal. Since we are running this as a Tensorflow 1 style piece of code, we need to explicitly set up a session before call "]},{"cell_type":"code","metadata":{"id":"VeLM9ouupONs"},"source":["class NQL(FrozenLakeFramework):\n","    \"\"\"Neural (not really Deep) Q-Learning. \"\"\"\n","    \n","    def __init__(self, env, num_episodes,session):\n","        FrozenLakeFramework.__init__(self,env,num_episodes)\n","        self.sess = session \n","        \n","        # Construct very simple two layer network\n","        self.inputs = tf.placeholder(shape=[1,16],dtype=tf.float32)\n","        self.W = tf.Variable(tf.random_uniform([16,4],0,0.01))\n","        self.Qout = tf.matmul(self.inputs, self.W)\n","        self.predict = tf.argmax(self.Qout, 1)\n","\n","        # construct operations for training \n","        self.nextQ = tf.placeholder(shape=[1,4],dtype=tf.float32)\n","        self.loss = tf.reduce_sum(tf.square(self.nextQ - self.Qout))\n","        self.trainer = tf.train.GradientDescentOptimizer(learning_rate=0.1)\n","        self.updateModel = self.trainer.minimize(self.loss)\n","\n","        # initialise the operations \n","        self.init = tf.initialize_all_variables()\n","        \n","        # Next we set up the hyper parameters. \n","        self.y = .98\n","        self.alpha = 0.8\n","        self.epsilon = 0.1\n","\n","    def selectAction(self,j,s,i):\n","        \"\"\"\n","        Select an action by running the state s through the network. \n","\n","        Parameters: \n","        j (int): number of steps taken in the game \n","        s (int): current state of the game\n","        i (int): training episode number\n","\n","        Returns: \n","        int: indicates the next action to be performed. \n","        \"\"\"\n","        [action, self.Q_] = self.sess.run([self.predict, self.Qout],feed_dict={self.inputs:np.identity(16)[s:s+1]})\n","        if np.random.rand(1) < self.epsilon:\n","            a = self.env.action_space.sample()\n","        else:\n","            a = action[0]\n","        return a\n","\n","    def updatePolicy(self,s,a,r,s1):\n","        \"\"\"\n","        Update the selection by applying backpropogation. \n","\n","        Parameters: \n","        s (int): current state\n","        a (int): action selected\n","        r (int): reward for the state-action pair. \n","        s1 (int): the state we moved to after performing a. \n","        \"\"\"\n","        # Obtain the Q' values by feeding the new state s1 through our network\n","        Qprime = self.sess.run(self.Qout,feed_dict={self.inputs:np.identity(16)[s1:s1+1]})\n","\n","        #Obtain maxQ' and set our target value for chosen action.\n","        maxQprime = np.max(Qprime)\n","\n","        Q_target = np.copy(self.Q_)\n","        Q_target[0,a] = (1-self.alpha)*Q_target[0,a] + self.alpha*(r + self.y*maxQprime)    \n","\n","        #Train our network using target and predicted Q values\n","        self.sess.run([self.updateModel],feed_dict={self.inputs:np.identity(16)[s:s+1],self.nextQ:Q_target})\n","        \n","        return \n","    \n","    def archive(self,i): \n","        self.epsilon = 1. / ((i / 50) + 10)\n","           "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"R5f-7bVPpONt","outputId":"4207cd89-0ba7-449f-bbda-4dfe3040ea03"},"source":["def runTests(runs):\n","    results = []\n","    for k in range(runs):\n","        tf.reset_default_graph()\n","        with tf.Session() as sess:\n","            nql = NQL(slippery_frozen_env,2000,sess)\n","            sess.run(nql.init)\n","            nql.playFrozenLake()\n","            nql.printResults(plot=False)\n","            r = nql.test(500)\n","            print(\"\\nTest results: % 12.2f\" % r)\n","            results.append(r)\n","    print(results)\n","    print(np.mean(results))\n","    \n","runTests(10)    "],"execution_count":null,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From /Users/rob/virtualenvs/dl-env/lib/python3.7/site-packages/tensorflow_core/python/util/tf_should_use.py:198: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n","Instructions for updating:\n","Use `tf.global_variables_initializer` instead.\n","0, 100, 200, 300, 400, 500, 600, 700, 800, 900, 1000, 1100, 1200, 1300, 1400, 1500, 1600, 1700, 1800, 1900, \n","Test results:        60.80\n","0, 100, 200, 300, 400, 500, 600, 700, 800, 900, 1000, 1100, 1200, 1300, 1400, 1500, 1600, 1700, 1800, 1900, \n","Test results:        36.40\n","0, 100, 200, 300, 400, 500, 600, 700, 800, 900, 1000, 1100, 1200, 1300, 1400, 1500, 1600, 1700, 1800, 1900, \n","Test results:        65.40\n","0, 100, 200, 300, 400, 500, 600, 700, 800, 900, 1000, 1100, 1200, 1300, 1400, 1500, 1600, 1700, 1800, 1900, \n","Test results:        10.60\n","0, 100, 200, 300, 400, 500, 600, 700, 800, 900, 1000, 1100, 1200, 1300, 1400, 1500, 1600, 1700, 1800, 1900, \n","Test results:        65.40\n","0, 100, 200, 300, 400, 500, 600, 700, 800, 900, 1000, 1100, 1200, 1300, 1400, 1500, 1600, 1700, 1800, 1900, \n","Test results:        62.60\n","0, 100, 200, 300, 400, 500, 600, 700, 800, 900, 1000, 1100, 1200, 1300, 1400, 1500, 1600, 1700, 1800, 1900, \n","Test results:         0.00\n","0, 100, 200, 300, 400, 500, 600, 700, 800, 900, 1000, 1100, 1200, 1300, 1400, 1500, 1600, 1700, 1800, 1900, \n","Test results:        59.20\n","0, 100, 200, 300, 400, 500, 600, 700, 800, 900, 1000, 1100, 1200, 1300, 1400, 1500, 1600, 1700, 1800, 1900, \n","Test results:        69.00\n","0, 100, 200, 300, 400, 500, 600, 700, 800, 900, 1000, 1100, 1200, 1300, 1400, 1500, 1600, 1700, 1800, 1900, \n","Test results:        59.20\n","[60.8, 36.4, 65.4, 10.6, 65.4, 62.6, 0.0, 59.199999999999996, 69.0, 59.199999999999996]\n","48.86\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"eqHDR_a1pONu"},"source":["We can see from the above that the results are quite good -- though obviously training is a lot slower than for straightforward Q-Learning. It is worth keeping in mind that in a simple case like this that Q-Learning does quite well, however it doesn't scale, and it is for this reason that neural or deep Q-Learning has been found to be very useful. \n","\n","### Using Pre-Training\n","\n","We run the model above multiple times as the training is unstable. There are a number o ways that we can stabilize it to improve our overall results. \n","\n","The first key idea is to use a form of pre-training where our training model is allowed to look at random game plays for a while. It can learn from these random game plays, but at all times our action selection remains random. \n","\n","We can implement this very straightforwardly with by adding a preTrain method to our game class. The pre-train method will iterate through a number of pre-training loops where the action to be performed is selected randomly. Note that to achieve this we temporarily set epsilon=1 to force our selectAction method to always return a random action. During this process epsilon stays fixed at one -- until the end of pre-training where we set it back to its original value. "]},{"cell_type":"code","metadata":{"id":"HQZIxoC-pONv"},"source":["class PretrainedNQL(NQL):\n","    \"\"\"Pretrained neural (not really Deep) Q-Learning. \"\"\"\n","    \n","    def __init__(self, env, num_episodes,session):\n","        NQL.__init__(self,env,num_episodes,session)\n","        \n","    def preTrain(self,num_pre_train_episodes):\n","        eTemp = self.epsilon \n","        self.epsilon = 1.0\n","        for i in range(num_pre_train_episodes):\n","            # Reset environment and get first state\n","            s = self.env.reset()\n","            # make sure done flag is set to false and reward is 0\n","            done, reward = False, 0\n","            for j in range(100):\n","                # select an action to perform\n","                action = self.selectAction(j,s,i)\n","                # apply action and get new state, reward and done flag from enviroment \n","                s1,reward,done,_ = self.env.step(action)\n","                # Update the action selection policy\n","                self.updatePolicy(s,action,reward,s1)\n","                # record the current state transition tuple -- used for printing and reviewing \n","                # new state becomes current state\n","                s = s1\n","                # if we are done we exit the loop\n","                if done == True: \n","                    break\n","        self.epsilon = eTemp      \n","        return        "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"suSZWbgnpONv"},"source":["If we run this modified version with pre-training we see that the results are a lot more stable and that "]},{"cell_type":"code","metadata":{"id":"gckcTF4dpONw","outputId":"3c81d01f-33d0-48e0-e8f0-8a0f017d5762"},"source":["def runTests(runs):\n","    results = []\n","    for k in range(runs):\n","        tf.reset_default_graph()\n","        with tf.Session() as sess:\n","            nql = PretrainedNQL(slippery_frozen_env,2000,sess)\n","            sess.run(nql.init)\n","            nql.preTrain(500)\n","            nql.playFrozenLake()\n","            nql.printResults(plot=False)\n","            r = nql.test(500)\n","            print(\"\\nTest results: % 12.2f\" % r)\n","            results.append(r)\n","    print(results)\n","    print(np.mean(results))\n","    \n","runTests(10)    "],"execution_count":null,"outputs":[{"output_type":"stream","text":["0, 100, 200, 300, 400, 500, 600, 700, 800, 900, 1000, 1100, 1200, 1300, 1400, 1500, 1600, 1700, 1800, 1900, \n","Test results:        62.20\n","0, 100, 200, 300, 400, 500, 600, 700, 800, 900, 1000, 1100, 1200, 1300, 1400, 1500, 1600, 1700, 1800, 1900, \n","Test results:        59.40\n","0, 100, 200, 300, 400, 500, 600, 700, 800, 900, 1000, 1100, 1200, 1300, 1400, 1500, 1600, 1700, 1800, 1900, \n","Test results:        64.20\n","0, 100, 200, 300, 400, 500, 600, 700, 800, 900, 1000, 1100, 1200, 1300, 1400, 1500, 1600, 1700, 1800, 1900, \n","Test results:        64.20\n","0, 100, 200, 300, 400, 500, 600, 700, 800, 900, 1000, 1100, 1200, 1300, 1400, 1500, 1600, 1700, 1800, 1900, \n","Test results:        64.60\n","0, 100, 200, 300, 400, 500, 600, 700, 800, 900, 1000, 1100, 1200, 1300, 1400, 1500, 1600, 1700, 1800, 1900, \n","Test results:        61.60\n","0, 100, 200, 300, 400, 500, 600, 700, 800, 900, 1000, 1100, 1200, 1300, 1400, 1500, 1600, 1700, 1800, 1900, \n","Test results:        63.80\n","0, 100, 200, 300, 400, 500, 600, 700, 800, 900, 1000, 1100, 1200, 1300, 1400, 1500, 1600, 1700, 1800, 1900, \n","Test results:        62.80\n","0, 100, 200, 300, 400, 500, 600, 700, 800, 900, 1000, 1100, 1200, 1300, 1400, 1500, 1600, 1700, 1800, 1900, \n","Test results:        63.20\n","0, 100, 200, 300, 400, 500, 600, 700, 800, 900, 1000, 1100, 1200, 1300, 1400, 1500, 1600, 1700, 1800, 1900, \n","Test results:        62.00\n","[62.2, 59.4, 64.2, 64.2, 64.60000000000001, 61.6, 63.800000000000004, 62.8, 63.2, 62.0]\n","62.8\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"E5UpVnQOpONw"},"source":["Percent of succesful episodes: 66.53999999999999%\n","Elapsed time: 693.5783030986786\n","![image.png](attachment:image.png)"]},{"cell_type":"markdown","metadata":{"id":"HbpPpvYWpONw"},"source":["A couple of notes on this approach. \n","\n","First, why don't we use a bias? The simple answer is that in this case it isn't needed. Since we have a simple mapping from a one hot encoded state we can rely on sets of weights between an input state variable and the Q values to learn the mapping. \n","\n","Second, do our losses ever go to 0? No. The Q-values never stabilise in this game. For state 14 we can always accumulate more reward. That means that the Q-value for state 14 can always keep going up. Therefore the loss, i.e., the difference between the current value of state 14 and the value after applying action 3 can always keep going up. This increase moves backwards through the other states slowly over time as well. This in turn means that even for State 0 that the q-value continues to slowly change over time. \n","\n","Third, why did we write this up in pure Tensorflow rather than Keras? The answer to that is that unfortunately at the time of writing these notes that the Keras version trains very slowly at the time of writing. \n","\n","Overall this is a very basic approach to neural network based Q-learning. There are some very important extensions we have to look at. \n","\n","## Experiential Replay\n","\n","In the approach above we apply updates very linearly. We apply an action, see the consequence, update our network based on that consequence and then apply the next action. This happens again and again and is equivalent to us playing the game multiple times and applying learning at each move in the game. This is highly logical but inefficient. On one hand, we are trying to learn even when there is nothing to learn from. On the other hand we are learning very slowly as we can only learn as fast as we play the game. \n","\n","The first extension to neural Q-Learning that we will consider is called Experiential Replay. From a hand waving perspective what it says is that we should play the game and apply learning multiple times based on the same experiences. This recognizes the fact that neural learning unlike a static application of the Bellman equation is slow. \n","\n","How we apply Experiential Replay is a little bit more nuanced than that. In practice we keep a record of our moves in the game and then randomly select a bunch of those moves every time we are to apply learning. There are two key ideas here. First, we apply a bunch of moves during learning, not just one move. This means that the same move will often be applied multiple times to the training process. Second, we randomly select a bunch of moves rather than just apply a sequence of moves. Therefore we are jumping around our experiential history. This can be shown improve the learning process considerably. \n","\n","\n","## Separate Target Network\n","So far we have used just one network for training. For a given training cycle, that network provides both the actual and target Q values. A problem with this is that it can introduce cycles as the target we are training against is constantly shifting. On the other hand we don't have the situation like in supervised learning where the target is fixed. \n","\n","We can find a middle ground by having two networks rather than one for generating our Q values. In addition to our main network, we introduce a second network to generate the target Q values. Rather than having it as dynamic as our main Q value network, it will be a lot more stable over time. In early models based on this idea, the target network would only periodically be updated to the weights of the base network. However in more recent work the weights in the target network are constantly but slowly being updated to the main network's weights. In this way, you can think about the target network as being a very conservative learner. \n","\n","It is important to note that the target network is not trained in the same way that the main network is trained. All backprop style 'training' is applied to the main network. We then use a clever operation of partially copying our main network's weights over to the target network to effect change in that target network. \n","\n","## Further Optimisations\n","There are various further optimisations available for the Deep Q Network style Reinforcement learning. For anyone interested, Arthur Juliani's Deep Q Network tutorial and code is a very good start. \n","https://medium.com/@awjuliani/simple-reinforcement-learning-with-tensorflow-part-4-deep-q-networks-and-beyond-8438a3e2b8df"]},{"cell_type":"markdown","metadata":{"id":"0Rg8RNz6pONy"},"source":["\n","## Appendix 1 - Neural Q Learning - Single File -\n","\n","The version below usually performs a little bit better than the version above. I haven't got the time to debug the cause, but a bonus hand shake for the person that can figure it out. "]},{"cell_type":"code","metadata":{"id":"0AN60gM2pONy","outputId":"7f51e6bf-840b-4fa1-a946-14facccd6db2"},"source":["import gym\n","import numpy as np\n","import tensorflow.compat.v1 as tf\n","tf.disable_v2_behavior()\n","\n","env = gym.make('FrozenLake-v0')\n","\n","def train_network(test_epochs):\n","    # constants\n","    num_episodes = 2000\n","    num_random_episodes = 500\n","    epsilon = 0.1\n","    gamma = 0.95  # parameter for the bellmen equation\n","    alpha = 0.8\n","\n","    tf.reset_default_graph()\n","\n","    #configure single layer network\n","    inputs1 = tf.placeholder(shape=[1, 16], dtype=tf.float32)\n","    W1 = tf.Variable(tf.random_uniform([16, 4], 0, 0.01))\n","\n","    Qout = tf.matmul(inputs1, W1)\n","    predict = tf.argmax(Qout, 1)\n","\n","    nextQ = tf.placeholder(shape=[1, 4], dtype=tf.float32)\n","    loss = tf.reduce_sum(tf.square(nextQ - Qout)) \n","    \n","    trainer = tf.train.GradientDescentOptimizer(learning_rate=0.1)\n","    updateModel = trainer.minimize(loss)\n","\n","    init = tf.initialize_all_variables()\n","\n","    #run the game\n","    with tf.Session() as session:\n","\n","        #initialize the network\n","        session.run(init)\n","\n","        #let the network watch the game being played for a little while\n","        #before playing the game itself\n","        for i in range(num_random_episodes):\n","            state = env.reset()\n","            moves = 0\n","\n","            while moves < 99:\n","                moves += 1\n","\n","                # get the next predictions\n","                [action, q_values] = session.run([predict, Qout], feed_dict={inputs1: np.identity(16)[state:state + 1]})\n","\n","                # with probability epsilon perform a random action to explore the state space\n","                action[0] = env.action_space.sample()\n","\n","                # update the environment\n","                next_state, reward, done, _ = env.step(action[0])\n","\n","                # get the predicted q values\n","                q_prime = session.run([Qout], feed_dict={inputs1: np.identity(16)[next_state:next_state + 1]})\n","\n","                # bellmen update\n","                maxQ = np.max(q_prime)\n","                q_values[0, action[0]] = (1-alpha)*q_values[0, action[0]] + alpha*(reward + gamma * maxQ)\n","\n","                # train the network\n","                session.run([updateModel], {inputs1: np.identity(16)[state:state + 1], nextQ: q_values})\n","\n","                state = next_state\n","\n","                if done:\n","                    # Reduce chance of random action as we train the model.\n","                    break        \n","\n","        for i in range(num_episodes):\n","            state = env.reset()\n","            moves = 0\n","            \n","            if i % 100 == 0: print(str(i) + \", \", end='')\n","\n","            while moves < 99:\n","                moves += 1\n","\n","                #get the next predictions\n","                [action, q_values] = session.run([predict, Qout], feed_dict={inputs1: np.identity(16)[state:state + 1]})\n","\n","                #with probability epsilon perform a random action to explore the state space\n","                if np.random.rand(1) < epsilon:\n","                    action[0] = env.action_space.sample()\n","                    \n","                #update the environment\n","                next_state, reward, done, _ = env.step(action[0])\n","\n","                #bellmen update\n","                q_prime = session.run([Qout], feed_dict={inputs1: np.identity(16)[next_state:next_state + 1]})\n","                maxQ = np.max(q_prime)\n","                q_values[0, action[0]] = (1-alpha)*q_values[0, action[0]] + alpha*(reward + gamma * maxQ)\n","                \n","                #train the network\n","                session.run([updateModel], {inputs1:np.identity(16)[state:state+1],nextQ:q_values})\n","\n","                state = next_state\n","\n","                if done:\n","                    # Reduce chance of random action as we train the model.\n","                    break\n","\n","            epsilon = 1. / ((i / 50) + 10)\n","\n","        rList = []\n","        for i in range(test_epochs):\n","            # Reset environment and get first new observation\n","            s = env.reset()\n","            rAll = 0\n","            d = False\n","            j = 0\n","            # The Q-Network\n","            while not d:\n","                j += 1\n","                # Choose an action by greedily (with e chance of random action) from the Q-network\n","                a, q = session.run([predict, Qout], feed_dict={inputs1: np.identity(16)[s:s + 1]})\n","                # print q\n","\n","                # Get new state and reward from environment\n","                s1, r, d, _ = env.step(a[0])\n","\n","                rAll += r\n","                s = s1\n","            rList.append(rAll)\n","        return rList\n","\n","kList = []\n","test_epochs = 100\n","for k in range(20):                \n","    rList = train_network(test_epochs)\n","    val = (sum(rList) / test_epochs) * 100\n","    kList.append(val)\n","    print(\"\\nPercent of succesful episodes: \" + str(val) + \"%\")\n","print(np.mean(kList))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["0, 100, 200, 300, 400, 500, 600, 700, 800, 900, 1000, 1100, 1200, 1300, 1400, 1500, 1600, 1700, 1800, 1900, \n","Percent of succesful episodes: 75.0%\n","0, 100, 200, 300, 400, 500, 600, 700, 800, 900, 1000, 1100, 1200, 1300, 1400, 1500, 1600, 1700, 1800, 1900, \n","Percent of succesful episodes: 64.0%\n","0, 100, 200, 300, 400, 500, 600, 700, 800, 900, 1000, 1100, 1200, 1300, 1400, 1500, 1600, 1700, 1800, 1900, \n","Percent of succesful episodes: 59.0%\n","0, 100, 200, 300, 400, 500, 600, 700, 800, 900, 1000, 1100, 1200, 1300, 1400, 1500, 1600, 1700, 1800, 1900, \n","Percent of succesful episodes: 67.0%\n","0, 100, 200, 300, 400, 500, 600, 700, 800, 900, 1000, 1100, 1200, 1300, 1400, 1500, 1600, 1700, 1800, 1900, \n","Percent of succesful episodes: 74.0%\n","0, 100, 200, 300, 400, 500, 600, 700, 800, 900, 1000, 1100, 1200, 1300, 1400, 1500, 1600, 1700, 1800, 1900, \n","Percent of succesful episodes: 66.0%\n","0, 100, 200, 300, 400, 500, 600, 700, 800, 900, 1000, 1100, 1200, 1300, 1400, 1500, 1600, 1700, 1800, 1900, \n","Percent of succesful episodes: 54.0%\n","0, 100, 200, 300, 400, 500, 600, 700, 800, 900, 1000, 1100, 1200, 1300, 1400, 1500, 1600, 1700, 1800, 1900, \n","Percent of succesful episodes: 78.0%\n","0, 100, 200, 300, 400, 500, 600, 700, 800, 900, 1000, 1100, 1200, 1300, 1400, 1500, 1600, 1700, 1800, 1900, \n","Percent of succesful episodes: 50.0%\n","0, 100, 200, 300, 400, 500, 600, 700, 800, 900, 1000, 1100, 1200, 1300, 1400, 1500, 1600, 1700, 1800, 1900, \n","Percent of succesful episodes: 47.0%\n","0, 100, 200, 300, 400, 500, 600, 700, 800, 900, 1000, 1100, 1200, 1300, 1400, 1500, 1600, 1700, 1800, 1900, \n","Percent of succesful episodes: 68.0%\n","0, 100, 200, 300, 400, 500, 600, 700, 800, 900, 1000, 1100, 1200, 1300, 1400, 1500, 1600, 1700, 1800, 1900, \n","Percent of succesful episodes: 38.0%\n","0, 100, 200, 300, 400, 500, 600, 700, 800, 900, 1000, 1100, 1200, 1300, 1400, 1500, 1600, 1700, 1800, 1900, \n","Percent of succesful episodes: 67.0%\n","0, 100, 200, 300, 400, 500, 600, 700, 800, 900, 1000, 1100, 1200, 1300, 1400, 1500, 1600, 1700, 1800, 1900, \n","Percent of succesful episodes: 74.0%\n","0, 100, 200, 300, 400, 500, 600, 700, 800, 900, 1000, 1100, 1200, 1300, 1400, 1500, 1600, 1700, 1800, 1900, \n","Percent of succesful episodes: 75.0%\n","0, 100, 200, 300, 400, 500, 600, 700, 800, 900, 1000, 1100, 1200, 1300, 1400, 1500, 1600, 1700, 1800, 1900, \n","Percent of succesful episodes: 52.0%\n","0, 100, 200, 300, 400, 500, 600, 700, 800, 900, 1000, 1100, 1200, 1300, 1400, 1500, 1600, 1700, 1800, 1900, \n","Percent of succesful episodes: 64.0%\n","0, 100, 200, 300, 400, 500, 600, 700, 800, 900, 1000, 1100, 1200, 1300, 1400, 1500, 1600, 1700, 1800, 1900, \n","Percent of succesful episodes: 76.0%\n","0, 100, 200, 300, 400, 500, 600, 700, 800, 900, 1000, 1100, 1200, 1300, 1400, 1500, 1600, 1700, 1800, 1900, \n","Percent of succesful episodes: 66.0%\n","0, 100, 200, 300, 400, 500, 600, 700, 800, 900, 1000, 1100, 1200, 1300, 1400, 1500, 1600, 1700, 1800, 1900, \n","Percent of succesful episodes: 63.0%\n","63.85\n"],"name":"stdout"}]}]}