{"cells":[{"cell_type":"markdown","metadata":{"id":"9IuArMDfLOWR"},"source":["# Representations & Text Processing\n","\n","Two weeks ago we had a short look at image processing through the convolutional neural network, while next week we will look at recurrent neural networks and how they can be used for processing sequential data including text. \n","\n","This week we aren't going to look at a new specific architecture type, but instead we are going to step back and consider as a whole the issue of how are data is captured or represented by these networks, and ask what impacts that has for issues like efficient data processing and re-use.\n","\n","Some notes in this section - including some images - are based on the excellent tutorials on convolutional network use for Natural Language Processing that you can find online here:\n","http://www.wildml.com/2015/11/understanding-convolutional-neural-networks-for-nlp/ as well as the follow-up with some implementation notes: http://www.wildml.com/2015/12/implementing-a-cnn-for-text-classification-in-tensorflow/\n","\n","The notes here are more compact than the original tutorial. Please refer to the original source for more detail if you need it. \n","\n","Let's start by importing some bits and pieces to get that out of the way. "]},{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":2657,"status":"ok","timestamp":1679320370934,"user":{"displayName":"Robert Ross","userId":"11121896988139609834"},"user_tz":0},"id":"DcGkl6x3LOWY"},"outputs":[],"source":["import numpy as np\n","from tensorflow.keras.preprocessing import sequence\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, Embedding, Conv1D, GlobalMaxPooling1D, GlobalAveragePooling1D\n","from tensorflow.keras.datasets import imdb\n","import tensorflow as tf"]},{"cell_type":"markdown","metadata":{"id":"SJRfK2aBLOWa"},"source":["## Representing the Meaning of Words\n","\n","As we will be primarily looking at text this week, let's begin with its representation options. This question of 'text representation' is not only important for Neural Network approaches to text processing tasks, but is equally important to text processing in Machine Learning in general. \n","\n","The challenge of representing text for Machine Learning tasks is a very broad research topic. Here we unfortunately don't have time to look at the particular motivations and problems with many approaches, but we will cover the key approaches that are used in practice. \n","\n","We need a way of representing text, but text presents us some challenges. Words have variable length, i.e., one word can have length 1, e.g., 'a', while another word can be just a little bit longer, e.g., 'pneumonoultramicroscopicsilicovolcanoconiosis'. Worse still, sentences themselves have variable length. Obviously the string type in a computer can capture a sentence in a way that is great for word processing tasks and similar operations, but it isn't a good representation for Machine Learning. Instead we need a representation that is a bit more predictable in terms of the length of our inputs. \n","\n","### Bag of Word Count Matrices \n","\n","To achieve this, the most basic form of Text Representation is the so-called Bag of Words style approach. Bag of Words (BOW) representations were very popular for many years and are typical of what was used in classical Information Retrieval and Text Classification Tasks. The basic features of a BOW approach are: \n","\n"," 1. Each input text document is represented by a vector of values\n"," 2. The vector length corresponds to the number of entries in our vocabulary\n"," 3. Each entry in the vector represents the presence of a given word occurred in the input text \n"," \n","The image below summarises this approach. \n","\n","<!-- bow.png --> \n","<img width=\"600\" src=\"https://drive.google.com/uc?id=1geHOCUF3eT5_Nwjt6tbso1a18GQJo_I6\"/>\n"," \n","Each row in our table corresponds to an individual document. Depending on the task at hand, the document could be just a sentence, or a paragraph, or indeed, a document, such as a webpage or book. For a given row, each cell represents whether a given word was found in the document. In basic approaches, this might just be a binary flag that says whether the word was in the document or not. Alternatively it can be a count of the number of times that the word is found in the document. \n","\n","The number of columns is equal to the length of our vocabulary. This is something that we need to know in advance for our task. In practice it is often impossible to know this perfectly -- there will be new words that crop up in our test data or at run time that we have not seen before -- so for new unseen words we might represent them as a symbol such an UNK which stands for 'unknown'. \n","\n","Even in English words can have many different forms, e.g,, eat, eats, eaten, ate. This presents a design challenge for BOW style representations. Do we represent such similar but different words with multiple columns or do we create a single column that captures what we call the stem or basic form of a word? There is no perfect answer to this question and this will often again be a task defined decision. By the way, if we think this problem is difficult in English, it is far more difficult in a morphologically rich language such as Turkish. \n","\n","This BOW style approach leads to sparse vectors per document where most entries in a given vector are 0s. In other words, if our vocabulary is say 171,000 (approximate length of the Oxford dictionary), and the sentence we want to encode has say 17 words, then our vector will have 17 non-0 entries and an awful lot of 0s. \n","\n","This Bag of Words approach works surprisingly well for a range of text classification tasks. By limiting the size of our vocabulary and essentially ignoring rarely seen words, we can get very reasonable results for many text classification tasks by simply feeding our sparse vector representations into basic classifiers like Logistic Regression and Naive Bayes classifiers. This is exactly how early spam classifiers were originally designed.  \n","\n","### Word Vectors \n","\n","While BOW representations are useful, they are now very far from the state of the art. There are many problems with them \n","\n"," 1. We loose information on the ordering of words. The representation is a list of words, typically in alphabetical order, and an indication of whether the word was present. The sentences 'John gave Mary 500 Euro' and 'Mary gave John 500 Euro' have the exact same BOW representation. \n"," 2. The vectors are long. For real word tasks the vectors are very long -- and mostly empty -- which is in practice not a great representation for feeding into classifiers. \n"," 3. There is no relationship between different columns. From a semantics (or meaning) perspective, the document represented by the short BOW representation 001000 is as close to 010000 as it is to 000001.  \n","\n","What we need are representations of words that overcome these limitations. \n","\n","To do this, the first thing we might do is stop storing a sentence in a single vector. Instead we switch to representing every word as a single vector. Thus we represent an input document as a full 2D array rather than just a vector as in the BOW case above. \n","\n","In these matrices each row captures the meaning of a single word, and the columns are then the features that are used to represent the meaning of the individual words. We can visualize this approach with the simple example below. \n","\n","<!-- word_vectors.png --> \n","<img width=\"600\" src=\"https://drive.google.com/uc?id=1gvmNOsIqc2h3Z5tYjcV-6OhzykYFqzeg\"/>\n","\n","The major advantage of this approach over our BOW representation is that it retains ordering information on our input text. The key disadvantage of this approach is that we end up needing a two dimensional document to represent each individual input document (sentence or full webpage etc.). Also, the number of rows in our representation of the document is now variable and depends on the length of the sentence. In practice though we can say assume that the maximum length of our sentence is maybe 256 words, and we can pad out sentences with empty vectors up until the end of the array. \n","\n","We have said nothing yet about the contents of our word vectors. In fact how we capture meaning in these word vectors can vary considerably from one approach to another. In a basic approach we take a similar approach to our Bag of Words representation and once again use a one-hot-encoding against vocab features. In this approach each word maps to a single active feature with all other features being inactive. Thus for a 10 word sentence represented with features taken from a 10,000 word vocabulary, we would end up with a 10x10000 matrix. In the visualization below I leave 0s unmarked for clarity. \n","\n","<!-- word_vectors_ohe.png --> \n","<img width=\"600\" src=\"https://drive.google.com/uc?id=1h6-OKjv8aucMIXRoA7ciQvucqZsYYkh6\"/>\n","\n","\n","### Distributed Word Vectors\n","\n","While the approach above can help since the order of words in sentences is now left intact, it isn't enough yet. The representation is still highly sparse and hence difficult to process for large vocabulary sizes. We can get around this problem a little bit by using specialist sparse matrix representations to encode the sparse matrix in a more compact way, but the sparse encoding in itself can make it more difficult to train. The second huge limitation is that our representation fails to account for any similarities between words. As indicated above with my 001000 example, the words 'dog' and 'labrador' are as close to each other in the data space as the words 'dog' and 'deprive'. \n","\n","To overcome these limitations, an alternative approach is to instead use a distributed representation where words are represented in a lower-dimensional feature space (e.g., 128 dimensions) where individual features are real valued (unlike the binary features used in a word identity matrix). This compacted representation is widely known as a distributed representation or word embedding and is a key tool in neural text processing. Keep in mind that each feature in a distributed word embeddings does not necessarily have to 'mean' anything. It can be an aspect of meaning that is well defined, or perhaps only makes sense when we combine it with other features. \n","\n","We can illustrate this approach as follows: \n","\n","<!-- word_vectors_dist.png --> \n","<img width=\"600\" src=\"https://drive.google.com/uc?id=1h0vrvcNw_0HBec3yifGCLeKA48asVWpf\"/>\n","\n","\n","Within the embedding space related words are located nearby each other. For example we would expect the words 'dog' and 'laborador' to be located much closer to each other in our 128 dimensional space than we would expect the words 'dog' and 'deprive'. Generally speaking, related words are clustered together. \n","\n","We can visualise these clusterings at a course grain by taking a high-level view of clusterings of words in our embedding space by projecting them into 2D:\n","\n","\n","<!-- word_vectors_colab.png --> \n","<img width=\"600\" src=\"https://drive.google.com/uc?id=1grGp1yvd0uDvT9ceDIPFJFyDoLFgBVzv\"/>\n","<!-- Sourced from http://sebastianruder.com/content/images/2016/04/word_embeddings_colah.png\" -->\n","\n","If we zoom in to a particular section of our embedding space we will find similar words co-located together:\n","\n","<!-- word_vectors_WordTSNE.png --> \n","<img width=\"600\" src=\"https://drive.google.com/uc?id=1gj_8v1PWi5q2Kc6fhKmjeZMzJYySUi2-\"/>\n","\n","Keep in mind that any 2D visualization of our embedding space is a projection out from our n-dimensional embedding space. \n","\n","Projecting from our n-dimensional embedding space into 2D for visualization has been a big research topic over the years. tSNE is one well regarded tool for making these projections, but there are many other options. \n","\n","## Learning Distributed Representations\n","\n","There are many ways in which we could learn a compacted distributed word embedding for text representation. \n","\n","The easiest way to create a distributed representation of a word is simply to feed a one hot encoding of the word into a network and then take the first hidden layer after training and call that a vector representation. In other words, the first hidden layer in any network where the input vector is a raw BOW style encoding can be thought of as an embedding. Importantly, while the input space for the input layer is theoretically huge, we know that the input vectors are one-hot encoded and hence are very limited in their space. We can take advantage of this fact to use the input (the raw text encoding) to 'lookup' saved embeddings values from the first hidden layer. \n","\n","This is exactly what the standard Embeddings layer of an implementation like Keras is doing, i.e., the embeddings are generated from a weight matrix for a hideen layer. However at runtime, rather than computing the embeddings directly from a layer which could be very big, we can just look up the embeddings values from a table that is indexed by the input word. \n","\n","The advantage of this approach is that it is very easy to implement, and conceptually it is not very different to just plugging in the text into the network. However, since the embeddings are specific to your own trained application, they tend not to be semantically very rich, and are instead usually very oriented towards your own particular task. So for example, if you are predicting whether a review is postive or not, the embeddings are likely to cluster words into postive, negative, and then probably a group that doens't really tell you much. \n","\n","Generally more useful embeddings are generated by different types of non-application specific tasks that can try to capture the full richness of the semantics. To illustrate, let us consider the use of the auto-encoder for example. An autoencoder is a relatively simple architecture which is used to take an input, squeeze it into a more compact representation, and then reconstruct the input in the final layer. No training labels are needed for this type of architecture, and the representations generated (particularly by so-called varitional autoencoders) are quite good. \n","\n","Using this approach, we might for example try using the layer of values at the centre of the autoencoder as a representation / embedding: \n","\n","<!-- auto-embed.png --> \n","<img width=\"600\" src=\"https://drive.google.com/uc?id=1gQnTGyuJNZPQuhO7NcsZgHuK3HD6BdPG\"/>\n","\n","where X is the one-hot-encoding of a word in our vocabulary and Z is our embedding that we can then used whenever we need a distributed representation. \n","\n","In practice this simple auto-encoder method does not work well for the problem at hand, but fortunately a lot of work has gone into devising other methods to learn the compacted word embeddings. Still possibly the best known method is the is the Word2Vec approach from Mikolov et al which learns word embeddings in a supervised learning process between target words and their contexts. Actually there are two variant models in Word2Vec. The first variant, continuous bag of words, learns the embeddings in a task where the target word is to be predicted from a context (set of surrounding words). The second variant, skip-gram, learns the word embeddings in a task where context words are to be predicted from a single target word. \n","\n","Whether we train using the continuous bag of words or skip-gram variants, we still achieve as a side effect a hidden layer which can be used subsequently as a word embedding. These two variants are illustrated below:\n","\n","<!-- word2vec.png --> \n","<img width=\"600\" src=\"https://drive.google.com/uc?id=1gmiOr-gTnCMMhixxmWXebtKZksPzUeFX\"/>\n","\n","For anyone interested in the specifics of learning Word2Vec style embeddings, the TensorFlow website has a nice tutorial that will guide you through the theory and the practical code for learning Word2Vec style embeddings. \n","\n","It should be noted that while Word2Vec is still at interesting example illustrating the general approach of mapping words to embeddings vectors, technologies that map entire sentences to vectors based on transformer achitecture variants (discussed later) are now the standard for serious text processing. \n","\n","## Using Word Embeddings\n","\n","Embeddings can be used in one of 2 ways in practice:\n","\n"," 1. We learn embeddings for the sake of learning embeddings with a Word2Vec model or similar, and then put these vectors to work in a specific task. \n"," 2. We learn word embeddings on the fly for a given task.\n"," \n","While Option 1 is almost often teh more correct way of dealing with embeddings, Option 2 is still encountered very frequently in code. \n","\n","Keras / Tensorflow proves an awful lot of the machinery for us to do this. There is a very short and elegant tutorial on the subject on the Tensorflow website here:\n","https://www.tensorflow.org/tutorials/text/word_embeddings\n","\n","The code below roughly follows the same layout in that tutorial, but we use a different example to give an alternative perspective on the design. The example we work from is based on this code:\n","https://keras.io/examples/imdb_cnn/\n","\n","Tensorflow provides an Embeddings layer for your networks. This Embeddings layer is a very specialist layer that that has all the machinery to both train and apply embeddings all built into it. Specifically the Embeddings layer learns a mapping between the input text that we will provide in a format similar to our 'word vectors' above, and maps them to a real valued space. \n","\n","Let's set up our example and import some text that we will use to illustrate. "]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":10853,"status":"ok","timestamp":1679320381784,"user":{"displayName":"Robert Ross","userId":"11121896988139609834"},"user_tz":0},"id":"WnIxARqTLOWh"},"outputs":[],"source":["max_features = 5000\n","maxlen = 400\n","embedding_dims = 50\n","\n","(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)"]},{"cell_type":"markdown","metadata":{"id":"Sc9oI1GYsI8-"},"source":["Here we have loaded up loading up Keras's inbuilt wrapper for the IMDB dataset and used it to generate training and validation / testing data. The data is split between inputs (_x) and labels (_y). \n","\n","Next we need to trake the inputs and pad them out to all be the same length. This is essential for processing them. "]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":1706,"status":"ok","timestamp":1679320383472,"user":{"displayName":"Robert Ross","userId":"11121896988139609834"},"user_tz":0},"id":"kST-Jfn2sJSO"},"outputs":[],"source":["x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n","x_test = sequence.pad_sequences(x_test, maxlen=maxlen)"]},{"cell_type":"markdown","metadata":{"id":"NPguk1JKLOWp"},"source":["Let's start building up our model / network. We can use the Embeddings layer in a simple Sequential model. (On the Sequential model, note that Keras provides two overall model architectures, sequantial and functional. Sequential models assume a straight feedforward design and are easy to code by stacking layers on top of each other. Functional models can be far more intricate with informaiton flowing in far more arbitrary designs, but are slightly more complex to code as a result). "]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1679320383472,"user":{"displayName":"Robert Ross","userId":"11121896988139609834"},"user_tz":0},"id":"cdaPJAsxLOWp"},"outputs":[],"source":["model = Sequential()"]},{"cell_type":"markdown","metadata":{"id":"V3sx2f0sLOWp"},"source":["The first layer of our network will be our Embeddings layer. When we define the Embeddings layer, we need to specify the expected size of the raw non-distributed vectors, and then the target embedding dimensionality. We will go with an embedding size of just 8 to begin with. This means that each of our words will be encoded simple as a vector of 8 numbers after the first layer. "]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":377,"status":"ok","timestamp":1679320383847,"user":{"displayName":"Robert Ross","userId":"11121896988139609834"},"user_tz":0},"id":"nUkR_AwvLOWq"},"outputs":[],"source":["embedding_size=8 \n","model.add(Embedding(max_features,\n","                    embedding_size,\n","                    input_length=maxlen))"]},{"cell_type":"markdown","metadata":{"id":"0s34AWiYLOWq"},"source":["The Embeddings layer will make the mapping from the non-distributed input text to our numerical embeddings. After the embedding layer we have an output of dimension Batch_Size x Max_Document_Length x Embedding size. The Batch_Size here is just a batch size in the normal sense. In other words we can train in parallel for multiple instances of our training data in parallel. The Max_Document_Length is our expected maximum length of words in each review. Remember that the input encoder will pad the length of shorter documents out so that the tensor size is always as expected --even for much shorter sentences.\n","\n","At this point there are many different ways in which we could deal with our text. In the simple case we can feed this whole document representation into a Dense Layer. Every unit in this dense layer has access to the embedding values for each word in our document. This unfortunately will also include access to all those 0 padded entries between the actual document length and the max document length. "]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1679320383848,"user":{"displayName":"Robert Ross","userId":"11121896988139609834"},"user_tz":0},"id":"UX_jHkw0LOWq"},"outputs":[],"source":["model.add(Flatten())\n","model.add(Dense(200, activation='relu'))\n","model.add(Dense(1))"]},{"cell_type":"markdown","metadata":{"id":"V5Vv12cPLOWq"},"source":["Once that is done we can think of our representation just as we would any other layer in a network. We can now follow it with a number of different layers as required before we eventually finish up with an output layer. "]},{"cell_type":"markdown","metadata":{"id":"3Qp-RzJsLOWs"},"source":["We can now compile and train the model in the usual way. "]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":84302,"status":"ok","timestamp":1679320468147,"user":{"displayName":"Robert Ross","userId":"11121896988139609834"},"user_tz":0},"id":"rw2PLmk-LOWs","outputId":"f05bf8ae-8ecb-405b-80e2-3edc74606f2d"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/4\n","782/782 [==============================] - 15s 17ms/step - loss: 0.4100 - accuracy: 0.7788 - val_loss: 0.2972 - val_accuracy: 0.8619\n","Epoch 2/4\n","782/782 [==============================] - 12s 15ms/step - loss: 0.1886 - accuracy: 0.9240 - val_loss: 0.3319 - val_accuracy: 0.8639\n","Epoch 3/4\n","782/782 [==============================] - 13s 16ms/step - loss: 0.0884 - accuracy: 0.9662 - val_loss: 0.4750 - val_accuracy: 0.8531\n","Epoch 4/4\n","782/782 [==============================] - 13s 16ms/step - loss: 0.0395 - accuracy: 0.9858 - val_loss: 0.6039 - val_accuracy: 0.8472\n"]}],"source":["model.compile(optimizer='adam',\n","              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n","              metrics=['accuracy'])\n","\n","hist = model.fit(x_train, y_train,\n","          batch_size=32,\n","          epochs=4,\n","          validation_data=(x_test, y_test))"]},{"cell_type":"markdown","metadata":{"id":"UmirAMQqHFRx"},"source":["We can have a look at the summary of the model to see how many parameters are needed in practice now to run this model. "]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":17,"status":"ok","timestamp":1679320468147,"user":{"displayName":"Robert Ross","userId":"11121896988139609834"},"user_tz":0},"id":"y1EhnY7gHMNh","outputId":"a0206a03-0ffe-47af-d969-19c9a6f6675b"},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," embedding (Embedding)       (None, 400, 8)            40000     \n","                                                                 \n"," flatten (Flatten)           (None, 3200)              0         \n","                                                                 \n"," dense (Dense)               (None, 200)               640200    \n","                                                                 \n"," dense_1 (Dense)             (None, 1)                 201       \n","                                                                 \n","=================================================================\n","Total params: 680,401\n","Trainable params: 680,401\n","Non-trainable params: 0\n","_________________________________________________________________\n"]}],"source":["model.summary()"]},{"cell_type":"markdown","metadata":{"id":"1FqjS3ofExAd"},"source":["For those feeling adventurous, consider how to implement a version of this embedding approach that doesn't use the embeddings layer but instead just uses a dense layer instead. It will have many more paramters. "]},{"cell_type":"markdown","metadata":{"id":"o6cwFCVcLOWu"},"source":["## Very Simple Sentence Embeddings\n","\n","As an alternative to just flatting all embedded document and passing it straight to a dense layer, we do have other options. Another option that is widely applied is that we instead simplify our representation a little by taking an average of all our word embeddings. In other words we end up with an averaged embedding across the whole document. What this means in practice is that we are assuming that the meaning of a document is equal to the averaged meaning of all the words in the document. We can implement this with a GlobalAveragePooling1D layer. It should be noted that this will throw away the information we wanted to keep on the ordering of words in our representations, but we will come back to better ways of handling this later. "]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":18755,"status":"ok","timestamp":1679320486887,"user":{"displayName":"Robert Ross","userId":"11121896988139609834"},"user_tz":0},"id":"fAdcFLdPLOWu","outputId":"6504c31d-95c6-4527-8dcf-0ce8a58f1888"},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: \"sequential_1\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," embedding_1 (Embedding)     (None, 400, 8)            40000     \n","                                                                 \n"," global_average_pooling1d (G  (None, 8)                0         \n"," lobalAveragePooling1D)                                          \n","                                                                 \n"," dense_2 (Dense)             (None, 200)               1800      \n","                                                                 \n"," dense_3 (Dense)             (None, 1)                 201       \n","                                                                 \n","=================================================================\n","Total params: 42,001\n","Trainable params: 42,001\n","Non-trainable params: 0\n","_________________________________________________________________\n","Epoch 1/4\n","782/782 [==============================] - 5s 5ms/step - loss: 0.4882 - accuracy: 0.7105 - val_loss: 0.3202 - val_accuracy: 0.8495\n","Epoch 2/4\n","782/782 [==============================] - 5s 6ms/step - loss: 0.2729 - accuracy: 0.8872 - val_loss: 0.2823 - val_accuracy: 0.8817\n","Epoch 3/4\n","782/782 [==============================] - 5s 6ms/step - loss: 0.2322 - accuracy: 0.9058 - val_loss: 0.2788 - val_accuracy: 0.8805\n","Epoch 4/4\n","782/782 [==============================] - 4s 5ms/step - loss: 0.2101 - accuracy: 0.9166 - val_loss: 0.2881 - val_accuracy: 0.8730\n"]}],"source":["model = Sequential()\n","model.add(Embedding(max_features,\n","                    embedding_size,\n","                    input_length=maxlen))\n","model.add(GlobalAveragePooling1D())\n","model.add(Dense(200, activation='relu'))\n","model.add(Dense(1))\n","\n","model.summary()\n","\n","model.compile(optimizer='adam',\n","              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n","              metrics=['accuracy'])\n","\n","hist = model.fit(x_train, y_train,\n","          batch_size=32,\n","          epochs=4,\n","          validation_data=(x_test, y_test))"]},{"cell_type":"markdown","metadata":{"id":"vmqX3CpOLOWu"},"source":["Note that (a) the number of parameters that we have to use in our network are considerable less because we averaged the embeddings within a document; (b) as a result, training was much faster; and (c) we often achieve a comparable result! "]},{"cell_type":"markdown","metadata":{"id":"6v7SSLuhLOWv"},"source":["## Convolving Through Word Embeddings \n","\n","In the work above we threw away any processing that was dependent on the order of words in our documents. In practice we want to keep this information. One way in which we can make use of it in a sensible way is through the application of CNNs. \n","\n","What we will be doing is allowing local feature detectors that maybe look at 4 words at a time to convolve through a complete document. This way our feature detectors are able to look for local features in the text, i.e., combinations of 4 words together that have a specific benefit and meaning in our text processing task. \n","\n","In our image processing tasks our feature detectors were convolved across the length and breadth of our input image. A key difference with convolution when applied to text is that we do not convolve across our columns (i.e., the input features). Instead we convolve only down the vertical, i.e., through the words. This makes a lot of sense as the purpose of convolution is to pick up on local features in the data where there is global invariance. For our input representation features we do not have global invariance. However where we are scanning down through the words in sequence it makes a lot of sense to look for features across adjacent words or words which are close by each other. \n","\n","This approach to applying a convolution to an input document is illustrated below. Note that this image comes from the blog past here:\n"," http://www.wildml.com/2015/11/understanding-convolutional-neural-networks-for-nlp/ \n","\n","<!-- conv_text.png --> \n","<img width=\"600\" src=\"https://drive.google.com/uc?id=1ghFyeqjfTt9eAVskp1todD1agxLFT3M5\"/>\n","\n","In our image processing examples we stuck with a consistent sliding window size for each convolution in a given layer. In text processing we however would often want to use different length convolutional windows that operate over say 2, 3, 4 or even more words. For each convolution length we will typically have a number of instances of that convolution length, i.e., we allow training of multiple features at each permitted window length. These convolutions in turn produce our standard convolved features which can then be pooled and combined in the usual way. \n","\n","In the illustration above our word embeddings are of dimension 5 and we have a maximum document length of 7 with an input document of length 7. 6 convolution features (kernels or filters) are being used in this case. Two of these have a convolution width of two, two have a convolution width of three, and two have a convolution width of 4. Convolutional application of these filters to our input results in 6 separate feature maps. Since we do not convolve horizontally across our input, note that our feature maps are 1 dimensional. \n","\n","Max pooling can then optionally be used to reduce the size of our feature maps. In the illustration above an extreme form of max pooling is used where all feature maps are reduced to a single value for each feature map. "]},{"cell_type":"markdown","metadata":{"id":"UgNKvEKTLOWv"},"source":["###  Implementing Convolution to Embeddings \n","\n","In the code below we apply the idea of convolutions to our training case. We assume a fixed filter width of 3. This is in contrast to the image above where filters of length 2, 3 and 4 were investigated. We use 50 different kernel instances - this is much larger than the image above, and we use a fully connected hidden layer of 50 units after the pooling layer. \n"," "]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":143445,"status":"ok","timestamp":1679320630314,"user":{"displayName":"Robert Ross","userId":"11121896988139609834"},"user_tz":0},"id":"Bdv9_la8LOWv","outputId":"1eca20f7-79c3-49be-d64b-b16552272809"},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: \"sequential_2\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," embedding_2 (Embedding)     (None, 400, 8)            40000     \n","                                                                 \n"," conv1d (Conv1D)             (None, 398, 25)           625       \n","                                                                 \n"," flatten_1 (Flatten)         (None, 9950)              0         \n","                                                                 \n"," dense_4 (Dense)             (None, 200)               1990200   \n","                                                                 \n"," dense_5 (Dense)             (None, 1)                 201       \n","                                                                 \n","=================================================================\n","Total params: 2,031,026\n","Trainable params: 2,031,026\n","Non-trainable params: 0\n","_________________________________________________________________\n","Epoch 1/4\n","782/782 [==============================] - 34s 42ms/step - loss: 0.3973 - accuracy: 0.7851 - val_loss: 0.2712 - val_accuracy: 0.8864\n","Epoch 2/4\n","782/782 [==============================] - 37s 48ms/step - loss: 0.2035 - accuracy: 0.9182 - val_loss: 0.2775 - val_accuracy: 0.8759\n","Epoch 3/4\n","782/782 [==============================] - 33s 42ms/step - loss: 0.1315 - accuracy: 0.9490 - val_loss: 0.3506 - val_accuracy: 0.8766\n","Epoch 4/4\n","782/782 [==============================] - 31s 40ms/step - loss: 0.0707 - accuracy: 0.9751 - val_loss: 0.4085 - val_accuracy: 0.8686\n"]}],"source":["model = Sequential()\n","model.add(Embedding(max_features,\n","                    embedding_size,\n","                    input_length=maxlen))\n","model.add(Conv1D(25,3,padding='valid',activation='relu',strides=1))\n","model.add(Flatten())\n","model.add(Dense(200, activation='relu'))\n","model.add(Dense(1))\n","\n","model.summary()\n","\n","model.compile(optimizer='adam',\n","              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n","              metrics=['accuracy'])\n","\n","hist = model.fit(x_train, y_train,\n","          batch_size=32,\n","          epochs=4,\n","          validation_data=(x_test, y_test))"]},{"cell_type":"markdown","metadata":{"id":"_ED_4GM4Lo2B"},"source":["This model does have a lot of paramters and easily overfits. Fortunately though we can simplfy a little but again using the Global Average Layer and get a similar performance."]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":48797,"status":"ok","timestamp":1679320679090,"user":{"displayName":"Robert Ross","userId":"11121896988139609834"},"user_tz":0},"id":"V1-CHDdTLw4Z","outputId":"4b6ffa70-efd7-45d2-d760-3ae6cbabc80f"},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: \"sequential_3\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," embedding_3 (Embedding)     (None, 400, 8)            40000     \n","                                                                 \n"," conv1d_1 (Conv1D)           (None, 398, 25)           625       \n","                                                                 \n"," global_average_pooling1d_1   (None, 25)               0         \n"," (GlobalAveragePooling1D)                                        \n","                                                                 \n"," dense_6 (Dense)             (None, 200)               5200      \n","                                                                 \n"," dense_7 (Dense)             (None, 1)                 201       \n","                                                                 \n","=================================================================\n","Total params: 46,026\n","Trainable params: 46,026\n","Non-trainable params: 0\n","_________________________________________________________________\n","Epoch 1/4\n","782/782 [==============================] - 13s 15ms/step - loss: 0.4289 - accuracy: 0.7618 - val_loss: 0.2997 - val_accuracy: 0.8580\n","Epoch 2/4\n","782/782 [==============================] - 12s 15ms/step - loss: 0.2502 - accuracy: 0.8963 - val_loss: 0.2852 - val_accuracy: 0.8792\n","Epoch 3/4\n","782/782 [==============================] - 12s 15ms/step - loss: 0.2115 - accuracy: 0.9156 - val_loss: 0.2846 - val_accuracy: 0.8788\n","Epoch 4/4\n","782/782 [==============================] - 12s 15ms/step - loss: 0.1893 - accuracy: 0.9252 - val_loss: 0.2996 - val_accuracy: 0.8791\n"]}],"source":["model = Sequential()\n","model.add(Embedding(max_features,\n","                    embedding_size,\n","                    input_length=maxlen))\n","model.add(Conv1D(25,3,padding='valid',activation='relu',strides=1))\n","model.add(GlobalAveragePooling1D())\n","model.add(Dense(200, activation='relu'))\n","model.add(Dense(1))\n","\n","model.summary()\n","\n","model.compile(optimizer='adam',\n","              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n","              metrics=['accuracy'])\n","\n","hist = model.fit(x_train, y_train,\n","          batch_size=32,\n","          epochs=4,\n","          validation_data=(x_test, y_test))"]},{"cell_type":"markdown","metadata":{"id":"wTaPWq2SLOWw"},"source":["From the code above we can see that the CNN based method in this case does not necessarily do much better than in the averaged embeddings case. The point here though is to illustrate that while we often think of particular architectures being associated with particular data types, these rules are not fast and hard. \n","Generally the results are expected to improve though if we use a couple of different types of covolutions, i.e., convolutions of length 2, 3, 4 and 5. \n","\n","## True Sentence Embeddings\n","\n","True sentence embeddings will try to encode the meaning of a sentence into a single vector, but unlike our approach above, they don't just average over a lot of individual word vectors. Sentence embeddings are mostly built from transformer models. We will come back to this achitecture in detail after the easter to understand what how it is training and what it does. For now though we can just assume them to be a black box that will output a vector representation of a sentence that captures the meaning of the sentence -- up to and including word orderings. \n","\n"]},{"cell_type":"markdown","metadata":{"id":"TWYBxqCsLOWw"},"source":["## Using Pre-Trained Word Embeddings\n","\n","In many cases we do not need to learn word embeddings from scratch and can instead use pre-trained word embeddings that have been learned from large text corpora using the skip-gram method or similar. Generally the workflow for using these pre-trained embeddings is to download pre-computed vectors for all of the words in a large vocabulary. We then lookup the word vector for a particular word in a lookup table as needed in our task. \n","\n","There are many trained embedding sets available which vary in terms of: (a) the size and nature of the raw text data they were trained over and (b) the specific training method which was used to learn the embeddings. You can for example download a dataset of word vectors for 3 million words and phrases trained from a 100 Billion words from the Google News dataset, and understandably enough the dataset is big. Incidentally the embeddings in that model are 300 dimensions wide. \n","\n","### Tensorflow and Word Embeddings\n","\n","TensorFlow Hub gives us a lot of the plumbing which we require to quickly start working with pre-trained embeddings (and other types of pre-trained models). Basically we can simply download a set of representations that have already been built for us by others and use them directly as an embedding layer just as we did above. The difference is that the training of these Embeddings has already been done for us, and we often can leave the embeddings as is. Sometimes it makes sense though to fine tune these embeddings during our own training process. \n","\n","The Tensorflow Hub makes it incredibly easy to download pre-trained embeddings information and put them directly to use. For example, on the page below you can learn how to directly download an embeddings definition and use it to build a distributed embedding one sentence at a time. \n","\n","https://tfhub.dev/google/tf2-preview/gnews-swivel-20dim/1\n"]},{"cell_type":"markdown","metadata":{"id":"1WwESbCxuqTS"},"source":["To illustrate, let's import Tensorflow Hub and ask it to load the SWIVEL word embeddings. "]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":21,"status":"ok","timestamp":1679320679090,"user":{"displayName":"Robert Ross","userId":"11121896988139609834"},"user_tz":0},"id":"ru2LJZjtLnH9","outputId":"2e12a931-ecb0-41e5-bde9-acda085d9cb8"},"outputs":[{"name":"stdout","output_type":"stream","text":["loading embedding\n"]}],"source":["import tensorflow_hub as hub\n","\n","print(\"loading embedding\")\n","embed = hub.load(\"https://tfhub.dev/google/tf2-preview/gnews-swivel-20dim/1\")\n"]},{"cell_type":"markdown","metadata":{"id":"hDHDDyMGuzN3"},"source":["We can illustrate the use of the embeddings by just feeding some text in and seeing what we get out. "]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":19,"status":"ok","timestamp":1679320679091,"user":{"displayName":"Robert Ross","userId":"11121896988139609834"},"user_tz":0},"id":"wSQt9ul6u1Sf","outputId":"6ccce414-2427-42e1-be9d-334cd6a94540"},"outputs":[{"name":"stdout","output_type":"stream","text":["tf.Tensor(\n","[[ 0.8666395   0.35917717  0.00579667  0.681002   -0.54226625  0.22343189\n","  -0.38796625  0.62195706  0.22117122 -0.48538068 -1.2674141   0.886369\n","  -0.32849073 -0.13924702 -0.53327686  0.5739708  -0.05905761  0.13629246\n","  -1.1718255  -0.31494334]\n"," [ 0.9602181   0.62520486  0.06261905  0.37425604  0.24782333 -0.39351934\n","  -0.7418429   0.56599647 -0.26197797 -0.69016844 -0.76565284  0.71412426\n","  -0.4537978  -0.50701594 -0.8499377   0.8917156  -0.30278975  0.2149126\n","  -1.1098894  -0.46719775]\n"," [ 1.1263883  -0.46177042 -0.8531583   0.5697219  -0.04634653  0.00869457\n","  -0.41134015  1.0862297   0.9390011   0.53587663 -0.964659    0.9846872\n","  -0.5436216  -0.459042   -1.0998259   0.37084442 -0.05279565  0.2736311\n","  -0.54693335 -0.20116976]], shape=(3, 20), dtype=float32)\n"]}],"source":["embeddings = embed([\"cat is on the mat\", \"dog is in the fog\", \"dog\"])\n","print(embeddings)"]},{"cell_type":"markdown","metadata":{"id":"86LFM4ZZvEzd"},"source":["Note that the particular embeddings model provides us back one embedding per sentence rather than one embedding per word. \n","\n","To use the embedding layer within a network, we simply have to parameterise it appropriately and then directly add it to a model as illustrated below. "]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":907,"status":"ok","timestamp":1679320679981,"user":{"displayName":"Robert Ross","userId":"11121896988139609834"},"user_tz":0},"id":"pymdjUXnvFA-","outputId":"a7ba8d68-c363-4911-a2e1-b051e58ad9c8"},"outputs":[{"name":"stderr","output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.9/dist-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n","Instructions for updating:\n","Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n"]},{"name":"stdout","output_type":"stream","text":["Model: \"sequential_4\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," keras_layer (KerasLayer)    (None, 20)                400020    \n","                                                                 \n"," dense_8 (Dense)             (None, 16)                336       \n","                                                                 \n"," dense_9 (Dense)             (None, 1)                 17        \n","                                                                 \n","=================================================================\n","Total params: 400,373\n","Trainable params: 353\n","Non-trainable params: 400,020\n","_________________________________________________________________\n"]}],"source":["hub_layer = hub.KerasLayer(\"https://tfhub.dev/google/tf2-preview/gnews-swivel-20dim/1\", output_shape=[20],\n","                           input_shape=[], dtype=tf.string)\n","\n","m2 = tf.keras.Sequential()\n","m2.add(hub_layer)\n","m2.add(tf.keras.layers.Dense(16, activation='relu'))\n","m2.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n","\n","m2.summary()"]},{"cell_type":"markdown","metadata":{"id":"JcP4LZ24wUgf"},"source":["As an assignment, make use of a pre-trained embeddings layer to train the IMDB data. How does performance compare against on-the-fly embeddings? Do you have to rework the data in any way? "]},{"cell_type":"markdown","metadata":{"id":"Zi3KXL5IYA5O"},"source":["## Pre-Trained Models & Transfer Learning\n","\n","We have just seen that with embeddings it is possible to use pre-trained word embeddings rather than have to learn the embeddigns on the fly for our current application. This is an important point in Deep Learning -- we have some modularity in our designs and try to take advantage of pre-trained models when possible. \n","\n","To understand this, consider that a neural network is just an architecture (design) and a collection of parameters that can instantiate that design. Networks can in general be saved to disk and loaded back up whenever we like. This is essential in allowing us to save the model as we are going along (just in case anything causes it to crash) but is also essentail in allowing a degree of re-use across models. \n","\n","### Reuse in Image Processing\n","\n","While re-use is used considerably in text embeddings models, it is arguably used even more in image processing. \n","\n","All the major computer vision architectures are available to be used on a pre-trained basis. In most of these cases the networks have been pre-trained using the ImageNet dataset. This is a very large collection of labelled images that have been used for many years to train neural networks. Keras for example provides very convinient wrappers that allow us to load up a pre-trained instance of one of these networks and use it within our own model type. \n","\n","Note that this idea of re-use (from a software or design perspective) is very close to the idea of Transfer Learning from a strct machine learning perspective. Transfer Learning generall refers to the idea of learning a model within one domain, and then applying that model to a new domain. What we are trying to achieve is a transfer of knowledge from one application to another -- this is different to a transfer of data. \n","\n","In Transfer Learning we typically run one background training process once on a VERY large dataset to build a good re-usable model that can then be applied to new domains that typically have less labelled data available. We usually however do not copy over the whole network. Instead we often leave the final few layers of the original network behind. We refer to these as the head of the network, and typically these are very much related to the original application that was used to train the network. For example if the original network was trained for image classification of 100 different classes, these networks will be very focused on providing the final softmax layer that maps directly to 100 classes. \n","\n","The assumption here is that while the head of the network is very focused on the specific task, the earlier layers in the network will be more general purpose and will in fact to re-usable in other domains. \n","\n","In our new application network we typically have to create a new 'head' that is specific to our own application. The head of the network will be trained for our new target data but with lower layers in the network being those that came from our original source network. \n","\n","### Training Policy and Fine-Tuning\n","\n","For the layers imported into our target network we have a choice on how to use those layers. In the extreme we can decide to leave the parameters in those layers exactly as they were (freeze), or allow the training process on the new application to modify these weights for the new domain (fine-tune). \n","\n","In practice there are many different strategies for fine tuning availabile, but these often involve designing your new architecture initially with imported weights that cannot change. Then after a period of training we allow the weights to be adjusted during our training process.\n","\n","What do you suppose is the advantage to this graduated fine tuning process versus allowing full weight editing from the beginning? \n","\n","### Pre-Training and Text Processing\n","\n","In text processing the reused network is often a complex sentence level embedder such as Google's Universal Sentence Enocder or BERT or similar. We will return to the use of these networks in the context of Transformers after the Easter break. "]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.0"}},"nbformat":4,"nbformat_minor":0}
