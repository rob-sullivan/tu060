{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "The goal of the project was to perform a systematic investigation of a number of Deep Learning methods in the context of text processing tasks, and benchmark these methods against classical methods where appropriate. The following was provided: a report and source code complete with a link to Google Colab is contained within a .zip file provided and trained models using Keras / TensorFlow. No alternative data set or coding framework was used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Dataset\n",
    "[Guardian News Articles dataset](https://www.kaggle.com/datasets/adityakharosekar2/guardian-news-articles) on Kaggle was used to perform genre or more precisely section analysis. Since this dataset was large (~150,000 articles / >700MB) the full dataset was not used. Instead a proportion of the dataset (random 10%-20%) was used.\n",
    "### Get 10% of data from large dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(4321) #for reproducibility\n",
    "#10% of 149,828 rows = 14,983 rows\n",
    "df = pd.read_csv(\"guardian_articles.csv\", skiprows=lambda x: x > 0 and random.random() >=.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.0%\n"
     ]
    }
   ],
   "source": [
    "#percentage of data imported from guardian dataset\n",
    "print(str(round((len(df.index) /149828)*100, 2))  + \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['article_id', 'sectionName', 'webTitle', 'webUrl', 'bodyContent',\n",
      "       'webPublicationDate', 'id'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df.columns) #df.head()#look at raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reorder data and oly take the news title and category label\n",
    "df = df[['webTitle', 'sectionName']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>webTitle</th>\n",
       "      <th>sectionName</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>British pilot in Tanzania 'manoeuvred ​to save...</td>\n",
       "      <td>World news</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Jürgen Klopp hails Liverpool youngsters but re...</td>\n",
       "      <td>Football</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Tommy Elphick turns thoughts to Harry Arter af...</td>\n",
       "      <td>Football</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Chelsea draw Manchester City and Arsenal meet ...</td>\n",
       "      <td>Football</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>John Terry to leave Chelsea after refusal of f...</td>\n",
       "      <td>Football</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            webTitle sectionName\n",
       "0  British pilot in Tanzania 'manoeuvred ​to save...  World news\n",
       "1  Jürgen Klopp hails Liverpool youngsters but re...    Football\n",
       "2  Tommy Elphick turns thoughts to Harry Arter af...    Football\n",
       "3  Chelsea draw Manchester City and Arsenal meet ...    Football\n",
       "4  John Terry to leave Chelsea after refusal of f...    Football"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "webTitle       0\n",
       "sectionName    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check missing data\n",
    "df.isnull().sum() #none missing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split sample into 80% training, 10% test & 10% validation datasets\n",
    "Next, 10% of the data was split off for testting, 10% for validation and the remaining 80% was used as for training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Training Dataset**\n",
      "80.0%\n",
      "\n",
      "**Testing Dataset**\n",
      "10.0%\n",
      "\n",
      "**Validation Dataset**\n",
      "10.0%\n"
     ]
    }
   ],
   "source": [
    "# Use ratios to split the dataset into training, testing, and validation sets\n",
    "train_ratio = 0.8\n",
    "val_ratio = 0.1\n",
    "test_ratio = 0.1\n",
    "\n",
    "train_cutoff = int(train_ratio * len(df))\n",
    "val_cutoff = int((train_ratio + val_ratio) * len(df))\n",
    "\n",
    "train_df = df.iloc[:train_cutoff]\n",
    "val_df = df.iloc[train_cutoff:val_cutoff]\n",
    "test_df = df.iloc[val_cutoff:]\n",
    "print(\"**Training Dataset**\")\n",
    "print(str(round((len(train_df.index) /len(df.index))*100, 2)) + \"%\")\n",
    "print(\"\\n**Testing Dataset**\")\n",
    "print(str(round((len(test_df.index) /len(df.index))*100, 2)) + \"%\")\n",
    "print(\"\\n**Validation Dataset**\")\n",
    "print(str(round((len(val_df.index) /len(df.index))*100, 2)) + \"%\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare & embed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 5000\n",
    "maxlen = 400\n",
    "embedding_dims = 50\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section Heading Prediction\n",
    "In this section analysis was perform based on 1) training from scratch and 2) using pre-trained models to predict section heading. Overfitting was minimised, hyperparamters were investigated and functions or metrics were selected where appropriate. After training was completed results were graphed for Training and validation data and the test result."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN Variants: LSTM vs a Basic RNN model & single layer LSTM vs multi-layer LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embeddings: on the fly vs pre-trained word embedding (Tensorflow Hub/HuggingFace) & embeddings vs traditional text encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
