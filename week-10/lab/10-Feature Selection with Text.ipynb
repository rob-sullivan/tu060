{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up Environment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data and generate TDM\n",
    "\n",
    "The vectoriser below uses the `token_pattern` parameter to remove numerics and underscores from the data.\n",
    "Try it out with the parameter specified and not specified (i.e. using the default which uses a regex of `(?u)\\b\\w\\w+\\b` to see the impact on the tokens/features generated.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = ['talk.religion.misc','soc.religion.christian', 'sci.med']\n",
    "newsgroups_train = fetch_20newsgroups(subset='train',\n",
    "                                     categories=categories,\n",
    "                                     remove=('headers', 'footers', 'quotes'),\n",
    "                                     shuffle=True, random_state=42)\n",
    "\n",
    "X, Y = newsgroups_train.data, newsgroups_train.target\n",
    "\n",
    "vectorizer = TfidfVectorizer(token_pattern=r'\\b[^\\d^\\_\\W]+\\b')    \n",
    "\n",
    "X_vec = vectorizer.fit_transform(X)   #transform training data into TDM\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examine the features generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features: 22059\n",
      "First 100: ['a', 'aa', 'aaai', 'aacc', 'aanerud', 'aaron', 'aaronson', 'aasked', 'ab', 'abacus', 'abandon', 'abandoned', 'abandoning', 'abandons', 'abates', 'abba', 'abbasids', 'abbott', 'abbreviated', 'abbreviation', 'abd', 'abdel', 'abdomen', 'abdominal', 'abduction', 'abdullah', 'abeit', 'aberdeen', 'aberrant', 'aberration', 'aberrations', 'abhin', 'abhor', 'abhorent', 'abhorrent', 'abide', 'abideth', 'abiding', 'abilities', 'ability', 'abingdon', 'abington', 'abiogenesis', 'abjuring', 'ablazing', 'able', 'ably', 'abner', 'abnormal', 'abnormalities', 'abnormally', 'aboard', 'abode', 'abodes', 'abolish', 'abolished', 'abolishment', 'abolition', 'abolitionist', 'abolitionists', 'abomb', 'abomination', 'abortion', 'abou', 'abound', 'abounded', 'about', 'above', 'abput', 'abraam', 'abraham', 'abrahamic', 'abram', 'abreast', 'abri', 'abridged', 'abroad', 'abruptly', 'abscence', 'abscess', 'absence', 'absent', 'absol', 'absolute', 'absolutely', 'absolutes', 'absolutist', 'absolutists', 'absorbed', 'absorbtion', 'absorption', 'abstacted', 'abstain', 'abstinence', 'abstract', 'abstraction', 'absurd', 'absurdity', 'absurdly', 'absurdum']\n"
     ]
    }
   ],
   "source": [
    "ftr_names= vectorizer.get_feature_names()\n",
    "print(\"Number of features: %d\"  % len(ftr_names))\n",
    "print(\"First 100: %s\" % ftr_names[:100])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Stopwords  and add in Document Frequency reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features: 7163\n",
      "First 100: ['aaron', 'abandon', 'abandoned', 'abdominal', 'aberrant', 'ability', 'able', 'abnormal', 'abnormalities', 'abolish', 'abolished', 'abortion', 'abraham', 'absence', 'absent', 'absolute', 'absolutely', 'absolutes', 'absolutist', 'absorbed', 'abstinence', 'absurd', 'absurdity', 'abundant', 'abuse', 'ac', 'academia', 'academic', 'accept', 'acceptable', 'acceptance', 'accepted', 'accepting', 'accepts', 'access', 'accident', 'accidentally', 'accompanied', 'accomplish', 'accomplished', 'according', 'accordingly', 'account', 'accountable', 'accounts', 'accumulated', 'accuracy', 'accurate', 'accurately', 'accusation', 'accusations', 'accuse', 'accused', 'accusing', 'accustomed', 'ache', 'achieve', 'achieved', 'acid', 'acidophilus', 'acids', 'acknowledge', 'acknowledged', 'acknowledgement', 'acknowledges', 'acne', 'acquired', 'acsu', 'act', 'acting', 'action', 'actions', 'active', 'actively', 'activities', 'activity', 'acts', 'actual', 'actually', 'acupuncture', 'acute', 'acyclovir', 'ad', 'adam', 'adams', 'adapted', 'add', 'added', 'addict', 'addiction', 'addicts', 'adding', 'addition', 'additional', 'additive', 'additives', 'address', 'addressed', 'addresses', 'addressing']\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(max_df=0.95, min_df=3, token_pattern=r'\\b[^\\d^\\_\\W]+\\b',stop_words=\"english\")\n",
    "X_vec = vectorizer.fit_transform(X)   #transform training data\n",
    "\n",
    "ftr_names= vectorizer.get_feature_names()\n",
    "print(\"Number of features: %d\"  % len(ftr_names))\n",
    "print(\"First 100: %s\" % ftr_names[:100])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use the Chi-Squared test for feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 208,  242,  485,  530,  558,  599,  602,  634,  804,  827,  828,\n",
       "        872,  950,  958,  974,  987,  990,  991,  992, 1002, 1380, 1748,\n",
       "       1765, 1833, 1834, 1893, 1974, 2002, 2026, 2038, 2205, 2364, 2402,\n",
       "       2413, 2539, 2540, 2665, 2730, 2746, 2860, 2890, 2902, 2915, 3027,\n",
       "       3232, 3304, 3424, 3445, 3449, 3519, 3561, 3618, 3760, 3775, 3802,\n",
       "       3841, 3846, 3877, 3879, 3884, 3943, 3946, 4016, 4116, 4117, 4122,\n",
       "       4159, 4240, 4300, 4355, 4537, 4552, 4609, 4610, 4614, 4711, 4732,\n",
       "       4798, 5160, 5357, 5499, 5590, 5695, 5715, 5758, 5833, 5902, 5932,\n",
       "       5997, 6063, 6318, 6349, 6350, 6637, 6669, 6702, 6854, 7017, 7019,\n",
       "       7142])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chi = SelectKBest(chi2, k=100)    #get top k features \n",
    "X_chi_vec= chi.fit_transform(X_vec, Y)  #fit and transform  training data (TDM) into the reduced feature space\n",
    "\n",
    "mask = chi.get_support(indices=True) # mask returns a list of indices into the original vocabulary/feature space\n",
    "\n",
    "mask #print out the mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index: 208, feature name: allergic\n",
      "index: 242, feature name: amorc\n",
      "index: 485, feature name: authority\n",
      "index: 530, feature name: banks\n",
      "index: 558, feature name: batf\n",
      "index: 599, feature name: belief\n",
      "index: 602, feature name: believe\n",
      "index: 634, feature name: bible\n",
      "index: 804, feature name: cadre\n",
      "index: 827, feature name: cancer\n"
     ]
    }
   ],
   "source": [
    "## access the mask\n",
    "for i in mask[:10]:\n",
    "    print(\"index: %d, feature name: %s\" % (i, ftr_names[i]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features: 100\n",
      "First 100: ['allergic', 'amorc', 'authority', 'banks', 'batf', 'belief', 'believe', 'bible', 'cadre', 'cancer', 'candida', 'catholic', 'chastity', 'cheers', 'chinese', 'christ', 'christian', 'christianity', 'christians', 'church', 'corn', 'diagnosed', 'diet', 'disease', 'diseases', 'doctor', 'dsl', 'easter', 'edu', 'effects', 'eternal', 'faith', 'father', 'fbi', 'food', 'foods', 'geb', 'god', 'gordon', 'hare', 'health', 'heaven', 'hell', 'hudson', 'information', 'intellect', 'james', 'jesus', 'jim', 'kent', 'koresh', 'lds', 'lord', 'love', 'lyme', 'malcolm', 'man', 'marriage', 'married', 'mary', 'medical', 'medicine', 'migraine', 'moral', 'morality', 'mormons', 'msg', 'needles', 'normal', 'objective', 'pain', 'paradise', 'patient', 'patients', 'paul', 'physician', 'pitt', 'pope', 'ra', 'religion', 'resurrection', 'rosicrucian', 'sci', 'scripture', 'seizures', 'shameful', 'sin', 'skepticism', 'soon', 'spirit', 'surrender', 'symptoms', 'syndrome', 'treatment', 'truth', 'tyre', 'values', 'weight', 'weiss', 'yeast']\n"
     ]
    }
   ],
   "source": [
    "## create a list of the selected features using the mask\n",
    "new_ftrs = [] # a list to hold your k best features\n",
    "\n",
    "for i in mask:\n",
    "      new_ftrs.append(ftr_names[i])\n",
    "print(\"Number of features: %d\"  % len(new_ftrs))\n",
    "print(\"First 100: %s\" % new_ftrs[:100])        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Mutual Information for feature selection \n",
    "Note that the features selected are different from those selected using chi-squared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features: 100\n",
      "First 100: ['actually', 'agree', 'believe', 'best', 'better', 'bible', 'called', 'case', 'christ', 'christian', 'christians', 'church', 'come', 'course', 'd', 'day', 'did', 'didn', 'different', 'does', 'doesn', 'doing', 'don', 'e', 'edu', 'evidence', 'example', 'fact', 'faith', 'far', 'given', 'god', 'going', 'good', 'great', 'group', 'having', 'help', 'jesus', 'just', 'know', 'let', 'life', 'like', 'little', 'll', 'long', 'look', 'lot', 'love', 'm', 'make', 'man', 'matter', 'mean', 'mind', 'need', 'new', 'non', 'people', 'person', 'place', 'point', 'post', 'probably', 'problem', 'question', 'quite', 'read', 'real', 'really', 'reason', 'religion', 'right', 's', 'said', 'say', 'saying', 'says', 'sure', 't', 'tell', 'thing', 'things', 'think', 'time', 'true', 'truth', 'try', 'trying', 'use', 'used', 've', 'want', 'way', 'word', 'work', 'world', 'wrong', 'years']\n"
     ]
    }
   ],
   "source": [
    "mi = SelectKBest(mutual_info_classif, k=100)    #get top k features using mutual information\n",
    "X_mi_vec= mi.fit_transform(X_vec, Y)  #fit and transform  training data (TDM) into the reduced feature space\n",
    "\n",
    "## create a list of the selected features using the mask\n",
    "mask = mi.get_support(indices=True) #get list of indices into the original feature vector\n",
    "\n",
    "new_ftrs = [] # to hold the list of your K best features\n",
    "for i in mask:\n",
    "    new_ftrs.append(ftr_names[i])\n",
    "print(\"Number of features: %d\"  % len(new_ftrs))\n",
    "print(\"First 100: %s\" % new_ftrs[:100])      \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform classification using chi-squared feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    precision    recall  f1-score   support\n",
      "\n",
      "rec.sport.baseball       0.97      0.66      0.79       397\n",
      "  rec.sport.hockey       0.75      0.98      0.85       399\n",
      "\n",
      "          accuracy                           0.82       796\n",
      "         macro avg       0.86      0.82      0.82       796\n",
      "      weighted avg       0.86      0.82      0.82       796\n",
      "\n"
     ]
    }
   ],
   "source": [
    "categories = ['rec.sport.baseball', 'rec.sport.hockey']\n",
    "newsgroups_train = fetch_20newsgroups(subset='train',\n",
    "                                     categories=categories,\n",
    "                                     remove=('headers', 'footers', 'quotes'),\n",
    "                                     shuffle=True, random_state=42)\n",
    "\n",
    "X, Y = newsgroups_train.data, newsgroups_train.target\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "X_vec = vectorizer.fit_transform(X)   #transform training data\n",
    "\n",
    "chi = SelectKBest(chi2, k=100)    #get top k features \n",
    "X_chi_vec= chi.fit_transform(X_vec, Y)  # fit and transform tdm to reduced feature space\n",
    "\n",
    "newsgroups_test = fetch_20newsgroups(subset='test',     # get test data\n",
    "                                     categories=categories,\n",
    "                                     remove=('headers', 'footers', 'quotes'),\n",
    "                                     shuffle=True,\n",
    "                                     random_state=42)\n",
    "\n",
    "vectors_test = vectorizer.transform(newsgroups_test.data)   #transform test data\n",
    "chi_test = chi.transform(vectors_test)     # transform test data to reduced feature space\n",
    "\n",
    "classifier = MultinomialNB(alpha=.01)\n",
    "classifier.fit(X_chi_vec, Y)\n",
    "predicted = classifier.predict(chi_test)\n",
    "\n",
    "print(metrics.classification_report(newsgroups_test.target, predicted,\n",
    "    target_names=newsgroups_train.target_names))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
