{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Addiction Simulator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Virtual Environment\n",
    "1. make sure python3 is installed on your system.\n",
    "2. make sure you have a python virtual environment setup: \n",
    "python -m pip install --upgrade pip setuptools virtualenv\n",
    "3. create a python environment: \n",
    "python -m venv venv (this creates a virtual environment called venv)\n",
    "4. if applicable add /venv/ to your .gitignore.\n",
    "5. activate the virtual environment: \n",
    "either \\venv\\Scripts\\activate.bat on windows \n",
    "or source venv/bin/activate on mac+linux\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#basic\n",
    "from os import system, name\n",
    "import random\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#pytorch for gpu processing of ML model\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.autograd as autograd\n",
    "from torch.autograd import Variable\n",
    "\n",
    "#rich library for Terminal UI\n",
    "from rich.jupyter import print\n",
    "#from rich import print\n",
    "from rich.prompt import IntPrompt\n",
    "\n",
    "\n",
    "#hide pytorch warnings (should eventually be resolved)\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent's Brain (Deep Q-Learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):  \n",
    "    def __init__(self, input_size, nb_action):\n",
    "        #ref: https://discuss.pytorch.org/t/super-model-in-init/97426\n",
    "        #super(Network, self).__init__()\n",
    "        super().__init__() #pytorch's NN model\n",
    "        self.input_size = input_size\n",
    "        self.nb_action = nb_action\n",
    "        self.fc1 = nn.Linear(input_size, 30)#arbitrarily chose 30 hidden layers\n",
    "        self.fc2 = nn.Linear(30, nb_action)\n",
    "    \n",
    "    #base pytorch NN model runs and we override the\n",
    "    #forward function with our own relu activation function\n",
    "    def forward(self, state):\n",
    "        x = F.relu(self.fc1(state))\n",
    "        q_values = self.fc2(x)\n",
    "        return q_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experience Replay Model\n",
    "This model is used for training our DQN model. It stores the transitions that the agent observes, allowing us to reuse this data later. By sampling from it randomly, the transitions that build up a batch are decorrelated. It has been shown that this greatly stabilizes and improves the DQN training procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory(): \n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "    \n",
    "    def push(self, event):\n",
    "        self.memory.append(event)\n",
    "        if len(self.memory) > self.capacity:\n",
    "            del self.memory[0]\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        samples = zip(*random.sample(self.memory, batch_size))\n",
    "        return map(lambda x: Variable(torch.cat(x, 0)), samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DQN Ensemble\n",
    "Comprised of a neural network model and a memory model. \n",
    "* The NN takes in observation of sensor data (brain chemicals) and chooses actions based on the relu activation function. \n",
    "* The agent will sample some of the sensor data and store in long term memory to be reused later for training. \n",
    "* We also use the Adam Optimisation algorithm. This is an extension to stocastic gradient desent to update weights of the neural network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dqn():\n",
    "    def __init__(self, input_size, nb_action, gamma):\n",
    "        self.gamma = gamma\n",
    "        self.reward_window = []\n",
    "        self.model = Network(input_size, nb_action)\n",
    "        self.memory = ReplayMemory(100000)\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr = 0.001)\n",
    "        self.last_state = torch.Tensor(input_size).unsqueeze(0)\n",
    "        self.last_action = 0\n",
    "        self.last_reward = 0\n",
    "    \n",
    "    # select action for x duration\n",
    "    def select_action(self, state):\n",
    "        #softmax converts numbers into probabilities\n",
    "        #Q values are the output of the neural network\n",
    "        # Temperature value = 100. closer to zero the less sure the NN will be to taking the action\n",
    "        probs = F.softmax(self.model(Variable(state, volatile = True))*100) # T=100\n",
    "        #viz q value for each action, (T value by user choice)\n",
    "        #pie chart 0/1 #seperate action\n",
    "        action = probs.multinomial(num_samples=1)\n",
    "        return action.data[0,0]\n",
    "    \n",
    "    #to train our AI\n",
    "    #forward propagation then backproagation\n",
    "    # get our output, target, compare our output to the target to compute the loss error\n",
    "    # backproagate loss error into the nn and use stochastic gradient descent we update the weights according to how much they contributed to the loss error\n",
    "    def learn(self, batch_state, batch_next_state, batch_reward, batch_action):\n",
    "        outputs = self.model(batch_state).gather(1, batch_action.unsqueeze(1)).squeeze(1)\n",
    "        next_outputs = self.model(batch_next_state).detach().max(1)[0]\n",
    "        target = self.gamma*next_outputs + batch_reward\n",
    "        td_loss = F.smooth_l1_loss(outputs, target)\n",
    "        self.optimizer.zero_grad()\n",
    "        td_loss.backward(retain_graph = True)\n",
    "        self.optimizer.step()\n",
    "    \n",
    "    #When ai reaches a new state we update everything\n",
    "    #update action, last action becomes the new action but also the last state becomes the new state and last reward becomes the new state\n",
    "    # we then get this new transition and update our reward window to track training progress and exploration\n",
    "    def update(self, reward, new_signal):\n",
    "        new_state = torch.Tensor(new_signal).float().unsqueeze(0)\n",
    "        self.memory.push((self.last_state, new_state, torch.LongTensor([int(self.last_action)]), torch.Tensor([self.last_reward])))\n",
    "        action = self.select_action(new_state)\n",
    "        if len(self.memory.memory) > 100:\n",
    "            batch_state, batch_next_state, batch_action, batch_reward = self.memory.sample(100)\n",
    "            self.learn(batch_state, batch_next_state, batch_reward, batch_action)\n",
    "        self.last_action = action\n",
    "        self.last_state = new_state\n",
    "        self.last_reward = reward\n",
    "        self.reward_window.append(reward)\n",
    "        if len(self.reward_window) > 1000:\n",
    "            del self.reward_window[0]\n",
    "        return action\n",
    "    \n",
    "    def score(self):\n",
    "        return sum(self.reward_window)/(len(self.reward_window)+1.)\n",
    "    \n",
    "    def save(self):\n",
    "        torch.save({'state_dict': self.model.state_dict(),\n",
    "                    'optimizer' : self.optimizer.state_dict(),\n",
    "                   }, 'last_brain.pth')\n",
    "    def load(self):\n",
    "        if os.path.isfile('last_brain.pth'):\n",
    "            print(\"=> loading checkpoint... \")\n",
    "            checkpoint = torch.load('last_brain.pth')\n",
    "            self.model.load_state_dict(checkpoint['state_dict'])\n",
    "            self.optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "            print(\"done !\")\n",
    "        else:\n",
    "            print(\"no checkpoint found...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Simulation():    \n",
    "    def __init__(self):\n",
    "        # Agent Brain - a neural network that represents our Q-function\n",
    "        self.agent = Dqn(5,6,0.9) # 5 sensors, 3 actions, gama = 0.9\n",
    "        #Agent's Actions\n",
    "        self.agent_actions = ['Binge on Internet', 'Work', 'Exercise', 'Socialise', 'Drink Alcohol', 'Smoke'] #6 actions\n",
    "        \n",
    "        #Agent Sensors\n",
    "        ##ref habits of a happy brain\n",
    "        ## Happy Chemicals\n",
    "        self.agent_serotonin = 1 #agent feeling of self achievement\n",
    "        self.agent_oxytocin = 1 #rewarding agent for being social\n",
    "        self.agent_dopamine = 1 #agent gets going after a reward\n",
    "        self.agent_endorphins = 1 #agent gets for pushing through physical pain at different times\n",
    "\n",
    "        ### Unhappy Chemicals\n",
    "        self.agent_cortisol = 0 #stress hormone which makes agent feel uncomfortable and wants to do something\n",
    "        \n",
    "        # the mean score curve (sliding window of the rewards) with \n",
    "        # respect to time.\n",
    "        self.scores = []\n",
    "\n",
    "        #the agent's environment\n",
    "        self.end_day = 365 # day simulation ends\n",
    "        self.end_hour = 24 # hour simulation ends\n",
    "        self.current_day = 1 # day agent starts\n",
    "        self.current_hour = 1 # hour agent starts\n",
    "        self.previous_time_left = 0\n",
    "\n",
    "        # temporary. this will change\n",
    "        self.reward_received = 0 # agent wants to maximise this score\n",
    "\n",
    "        #habituation will reduce the experience that makes the agent happy because the action is new\n",
    "        self.habituation = np.zeros((self.end_day,self.end_hour)) # initializing the habituation array with only zeros.\n",
    "\n",
    "        self.finish = False #trigger to end simulation\n",
    "        while not self.finish:\n",
    "            self.finish = self.next_time_interval()\n",
    "\n",
    "    def next_time_interval(self):\n",
    "        ## get the time left\n",
    "        day = self.end_day - self.current_day #difference in current day and end day\n",
    "        hour = self.end_hour - self.current_hour #difference in current hour and end hour\n",
    "        time_left = day*24 + hour\n",
    "\n",
    "        ## agent input state vector, composed of the five brain signals received by being in the environment\n",
    "        current_state = [self.agent_serotonin, self.agent_oxytocin, self.agent_dopamine, self.agent_endorphins, self.agent_cortisol]\n",
    "        action_to_take = self.agent.update(self.reward_received, current_state) # playing the action from the ai (dqn class)\n",
    "        self.scores.append(self.agent.score()) # appending the score (mean of the last 100 rewards to the reward window)\n",
    "\n",
    "        #This is the limbic system to say what action to take.\n",
    "        #we can take the agent's NN and call forward to output q values for each state.\n",
    "        suggested_action = self.agent_actions[action_to_take]\n",
    "        sa = int(1 if suggested_action==\"Binge on Video Games\" else 2 if suggested_action==\"Work\" else 3 if suggested_action==\"Exercise\" else 4 if suggested_action==\"Socialise\" else 5 if suggested_action==\"Drink Alcohol\" else 6 if suggested_action==\"Smoke\" else 0)\n",
    "        \n",
    "        #give user options\n",
    "        self.clear()\n",
    "        print(\"** Current Day: [bold dark_violet]\" + str(self.current_day) + \"[/bold dark_violet], Current Hour: [bold dark_violet]\" + str(self.current_hour) + \"[/bold dark_violet], Time Left: [bold dark_violet]\" + str(time_left) + \"[/bold dark_violet] hrs ** \\n\")\n",
    "        print(\"- [bold dark_green]Serotonin: \" + str(\"#\" * self.agent_serotonin) + \"[/bold dark_green]\")\n",
    "        print(\"- [bold dark_green]Oxytocin: \" + str(\"#\" * self.agent_oxytocin) + \"[/bold dark_green]\")\n",
    "        print(\"- [bold dark_green]Dopamine: \" + str(\"#\"*self.agent_dopamine) + \"[/bold dark_green]\")\n",
    "        print(\"- [bold dark_green]Endorphins: \" + str(\"#\"*self.agent_endorphins) + \"[/bold dark_green]\")\n",
    "        print(\"- [bold dark_red]Cortisol: \" + str(\"#\"*self.agent_serotonin) + \"[/bold dark_red]\")\n",
    "        print(\"\\n1. [bold dark_violet]Binge on Internet[/bold dark_violet]\\n2. [bold dark_violet]Work[/bold dark_violet]\\n3. [bold dark_violet]Exercise[/bold dark_violet]\\n4. [bold dark_violet]Socialise[/bold dark_violet]\\n5. [bold dark_violet]Drink Alcohol[/bold dark_violet]\\n6. [bold dark_violet]Smoke[/bold dark_violet]\\n\")\n",
    "        action_taken = 0\n",
    "        action_taken = IntPrompt.ask(\"Choose from 1 to 6\", default=sa)\n",
    "\n",
    "        #update agent brain chemicals after action taken\n",
    "        if(action_taken == 1): #Binge on Internet\n",
    "            self.agent_serotonin += 0\n",
    "            self.agent_oxytocin += 0\n",
    "            self.agent_dopamine += 1\n",
    "            self.agent_endorphins += 0\n",
    "            self.agent_cortisol += 0\n",
    "        elif(action_taken == 2): # Work\n",
    "            self.agent_serotonin += 1\n",
    "            self.agent_oxytocin += 0\n",
    "            self.agent_dopamine -= 1\n",
    "            self.agent_endorphins += 0\n",
    "            self.agent_cortisol += 1\n",
    "        elif(action_taken == 3): #Exercise\n",
    "            self.agent_serotonin += 1\n",
    "            self.agent_oxytocin += 0\n",
    "            self.agent_dopamine += 1\n",
    "            self.agent_endorphins += 0\n",
    "            self.agent_cortisol += 0\n",
    "        elif(action_taken == 4): #Socialise\n",
    "            self.agent_serotonin += 0\n",
    "            self.agent_oxytocin += 1\n",
    "            self.agent_dopamine += 1\n",
    "            self.agent_endorphins += 0\n",
    "            self.agent_cortisol += 0\n",
    "        elif(action_taken == 5): #Drink Alcohol\n",
    "            self.agent_serotonin += 0\n",
    "            self.agent_oxytocin += 0\n",
    "            self.agent_dopamine += 1\n",
    "            self.agent_endorphins += 0\n",
    "            self.agent_cortisol += 0\n",
    "        elif(action_taken == 6): #Smoke\n",
    "            self.agent_serotonin += 0\n",
    "            self.agent_oxytocin += 0\n",
    "            self.agent_dopamine += 1\n",
    "            self.agent_endorphins += 0\n",
    "            self.agent_cortisol += 0\n",
    "        elif(action_taken == 0):#quit\n",
    "            #print(\"saving brain...\")\n",
    "            #brain.save()\n",
    "            plt.title(\"Scores\")\n",
    "            plt.xlabel(\"Epochs\")\n",
    "            plt.ylabel(\"Reward\")\n",
    "            plt.plot(self.scores)\n",
    "            plt.show()\n",
    "            return True   #end simulation\n",
    "\n",
    "#indp. only dopamine is max reward,\n",
    "# seperate good actions and bad action\n",
    "# user action vs inertia of agent\n",
    "# user action at start import..,\n",
    "# repeated user actions can eventually change agent's actions\n",
    "# viz what past actions were taken by user.\n",
    "\n",
    "#e.g represent person who is alcoholic.\n",
    "        #reward and punishment conditions\n",
    "        self.reward_received = self.agent_serotonin + self.agent_oxytocin + self.agent_dopamine + self.agent_endorphins - self.agent_cortisol\n",
    "        if(self.habituation[0,0] > 0):\n",
    "            self.reward_received = -1 # reward -1 for repeating an action\n",
    "        else:\n",
    "            #otherwise\n",
    "            self.reward_received = -0.2 # it gets bad reward (-0.2)\n",
    "            if time_left < self.previous_time_left: # however if it getting close to the goal\n",
    "                self.reward_received = 0.1 # it still gets slightly positive reward 0.1 just for surviving\n",
    "\n",
    "        #check if this is the last round otherwise continue\n",
    "        if(time_left <= 0):\n",
    "            return True   #end simulation\n",
    "        else:    \n",
    "            # Updating the last time from the agent to the end time (goal)\n",
    "            self.current_day += 1  #update to next day interval\n",
    "            if(self.current_day > self.end_day):\n",
    "                self.current_day = self.end_day\n",
    "            self.current_hour += 1 #update to next hour interval\n",
    "            if(self.current_hour > self.end_hour):\n",
    "                self.current_hour = 0\n",
    "            self.previous_time_left = time_left\n",
    "            return False\n",
    "\n",
    "    def clear(self): \n",
    "        \"\"\"\n",
    "        This function was taken from https://www.geeksforgeeks.org/clear-screen-python/ to\n",
    "        allow the terminal to be cleared when changing menus or showing the user important\n",
    "        messages. It checks what operating system is being used and uses the correct \n",
    "        clearing command.\n",
    "        \"\"\"\n",
    "        # for windows \n",
    "        if name == 'nt': \n",
    "            _ = system('cls') \n",
    "\n",
    "        # for mac and linux(here, os.name is 'posix')\n",
    "        else: \n",
    "            _ = system('clear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    Simulation() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* change to focus on dopamine, other happy chemicals will change based on dopamine\n",
    "* viz q values and past user actions and past exp for agent\n",
    "* update ui with duration value -1\n",
    "* update good actions in green and bad actions in red.\n",
    "* add tooltip button to give info on happy chemicals.\n",
    "* work, exercise, socialise = good, others = red.\n",
    "* gradient colour if action is over used, e.g working too much.\n",
    "\n",
    "# DQN info\n",
    "# brain info\n",
    "# addiction info\n",
    "# hardware\n",
    "# help kids understand how the brain works in a viz way and understand actions taken today affect tomorrow\n",
    "# sim of how brain works in terms of addiction to help people learn how it works and using RL to implement it, e.g alpha go, etc."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "63fc9bad62e87c8f4520331cf897eeae3cca7d7504ec2bc20d709f20b08251da"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
