{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Addiction Simulator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Virtual Environment\n",
    "1. make sure python3 is installed on your system.\n",
    "2. make sure you have a python virtual environment setup: \n",
    "python -m pip install --upgrade pip setuptools virtualenv\n",
    "3. create a python environment: \n",
    "python -m venv venv (this creates a virtual environment called venv)\n",
    "4. if applicable add /venv/ to your .gitignore.\n",
    "5. activate the virtual environment: \n",
    "either \\venv\\Scripts\\activate.bat on windows \n",
    "or source venv/bin/activate on mac+linux\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#basic\n",
    "from os import system, name\n",
    "import random\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#pytorch for gpu processing of ML model\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.autograd as autograd\n",
    "from torch.autograd import Variable\n",
    "\n",
    "#rich library for Terminal UI\n",
    "from rich.jupyter import print\n",
    "#from rich import print\n",
    "from rich.prompt import IntPrompt\n",
    "\n",
    "\n",
    "#hide pytorch warnings (should eventually be resolved)\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent's Brain (Deep Q-Learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):  \n",
    "    def __init__(self, input_size, nb_action):\n",
    "        #ref: https://discuss.pytorch.org/t/super-model-in-init/97426\n",
    "        #super(Network, self).__init__()\n",
    "        super().__init__() #pytorch's NN model\n",
    "        self.input_size = input_size\n",
    "        self.nb_action = nb_action\n",
    "        self.fc1 = nn.Linear(input_size, 30)#arbitrarily chose 30 hidden layers\n",
    "        self.fc2 = nn.Linear(30, nb_action)\n",
    "    \n",
    "    #base pytorch NN model runs and we override the\n",
    "    #forward function with our own relu activation function\n",
    "    def forward(self, state):\n",
    "        x = F.relu(self.fc1(state))\n",
    "        q_values = self.fc2(x)\n",
    "        return q_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experience Replay Model\n",
    "This model is used for training our DQN model. It stores the transitions that the agent observes, allowing us to reuse this data later. By sampling from it randomly, the transitions that build up a batch are decorrelated. It has been shown that this greatly stabilizes and improves the DQN training procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory(): \n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "    \n",
    "    def push(self, event):\n",
    "        self.memory.append(event)\n",
    "        if len(self.memory) > self.capacity:\n",
    "            del self.memory[0]\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        samples = zip(*random.sample(self.memory, batch_size))\n",
    "        return map(lambda x: Variable(torch.cat(x, 0)), samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DQN Ensemble\n",
    "Comprised of a neural network model and a memory model. \n",
    "* The NN takes in observation of sensor data (brain chemicals) and chooses actions based on the relu activation function. \n",
    "* The agent will sample some of the sensor data and store in long term memory to be reused later for training. \n",
    "* We also use the Adam Optimisation algorithm. This is an extension to stocastic gradient desent to update weights of the neural network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dqn():\n",
    "    def __init__(self, input_size, nb_action, gamma):\n",
    "        self.gamma = gamma\n",
    "        self.reward_window = []\n",
    "        self.model = Network(input_size, nb_action)\n",
    "        self.memory = ReplayMemory(100000)\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr = 0.001)\n",
    "        self.last_state = torch.Tensor(input_size).unsqueeze(0)\n",
    "        self.last_action = 0\n",
    "        self.last_reward = 0\n",
    "    \n",
    "    def select_action(self, state):\n",
    "        #converts numbers into probabilities\n",
    "        probs = F.softmax(self.model(Variable(state, volatile = True))*100) # T=100\n",
    "        action = probs.multinomial(num_samples=1)\n",
    "        return action.data[0,0]\n",
    "    \n",
    "    def learn(self, batch_state, batch_next_state, batch_reward, batch_action):\n",
    "        outputs = self.model(batch_state).gather(1, batch_action.unsqueeze(1)).squeeze(1)\n",
    "        next_outputs = self.model(batch_next_state).detach().max(1)[0]\n",
    "        target = self.gamma*next_outputs + batch_reward\n",
    "        td_loss = F.smooth_l1_loss(outputs, target)\n",
    "        self.optimizer.zero_grad()\n",
    "        td_loss.backward(retain_graph = True)\n",
    "        self.optimizer.step()\n",
    "    \n",
    "    def update(self, reward, new_signal):\n",
    "        new_state = torch.Tensor(new_signal).float().unsqueeze(0)\n",
    "        self.memory.push((self.last_state, new_state, torch.LongTensor([int(self.last_action)]), torch.Tensor([self.last_reward])))\n",
    "        action = self.select_action(new_state)\n",
    "        if len(self.memory.memory) > 100:\n",
    "            batch_state, batch_next_state, batch_action, batch_reward = self.memory.sample(100)\n",
    "            self.learn(batch_state, batch_next_state, batch_reward, batch_action)\n",
    "        self.last_action = action\n",
    "        self.last_state = new_state\n",
    "        self.last_reward = reward\n",
    "        self.reward_window.append(reward)\n",
    "        if len(self.reward_window) > 1000:\n",
    "            del self.reward_window[0]\n",
    "        return action\n",
    "    \n",
    "    def score(self):\n",
    "        return sum(self.reward_window)/(len(self.reward_window)+1.)\n",
    "    \n",
    "    def save(self):\n",
    "        torch.save({'state_dict': self.model.state_dict(),\n",
    "                    'optimizer' : self.optimizer.state_dict(),\n",
    "                   }, 'last_brain.pth')\n",
    "    def load(self):\n",
    "        if os.path.isfile('last_brain.pth'):\n",
    "            print(\"=> loading checkpoint... \")\n",
    "            checkpoint = torch.load('last_brain.pth')\n",
    "            self.model.load_state_dict(checkpoint['state_dict'])\n",
    "            self.optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "            print(\"done !\")\n",
    "        else:\n",
    "            print(\"no checkpoint found...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Simulation():    \n",
    "    def __init__(self):\n",
    "        # Agent Brain - a neural network that represents our Q-function\n",
    "        self.agent = Dqn(5,6,0.9) # 5 sensors, 3 actions, gama = 0.9\n",
    "        #Agent's Actions\n",
    "        self.agent_actions = ['Binge on Internet', 'Work', 'Exercise', 'Socialise', 'Drink Alcohol', 'Smoke'] #6 actions\n",
    "        \n",
    "        #Agent Sensors\n",
    "        ##ref habits of a happy brain\n",
    "        ## Happy Chemicals\n",
    "        self.agent_serotonin = 1 #agent feeling of self achievement\n",
    "        self.agent_oxytocin = 1 #rewarding agent for being social\n",
    "        self.agent_dopamine = 1 #agent gets going after a reward\n",
    "        self.agent_endorphins = 1 #agent gets for pushing through physical pain at different times\n",
    "\n",
    "        ### Unhappy Chemicals\n",
    "        self.agent_cortisol = 0 #stress hormone which makes agent feel uncomfortable and wants to do something\n",
    "        \n",
    "        # the mean score curve (sliding window of the rewards) with \n",
    "        # respect to time.\n",
    "        self.scores = []\n",
    "\n",
    "        #the agent's environment\n",
    "        self.end_day = 365 # day simulation ends\n",
    "        self.end_hour = 24 # hour simulation ends\n",
    "        self.current_day = 1 # day agent starts\n",
    "        self.current_hour = 1 # hour agent starts\n",
    "        self.previous_time_left = 0\n",
    "\n",
    "        # temporary. this will change\n",
    "        self.reward_received = 0 # agent wants to maximise this score\n",
    "\n",
    "        #habituation will reduce the experience that makes the agent happy because the action is new\n",
    "        self.habituation = np.zeros((self.end_day,self.end_hour)) # initializing the habituation array with only zeros.\n",
    "\n",
    "        self.finish = False #trigger to end simulation\n",
    "        while not self.finish:\n",
    "            self.finish = self.next_time_interval()\n",
    "\n",
    "    def next_time_interval(self):\n",
    "        ## get the time left\n",
    "        day = self.end_day - self.current_day #difference in current day and end day\n",
    "        hour = self.end_hour - self.current_hour #difference in current hour and end hour\n",
    "        time_left = day*24 + hour\n",
    "\n",
    "        ## agent input state vector, composed of the five brain signals received by being in the environment\n",
    "        current_state = [self.agent_serotonin, self.agent_oxytocin, self.agent_dopamine, self.agent_endorphins, self.agent_cortisol]\n",
    "        action_to_take = self.agent.update(self.reward_received, current_state) # playing the action from the ai (dqn class)\n",
    "        self.scores.append(self.agent.score()) # appending the score (mean of the last 100 rewards to the reward window)\n",
    "\n",
    "        #This is the limbic system to say what action to take.\n",
    "        #we can take the agent's NN and call forward to output q values for each state.\n",
    "        suggested_action = self.agent_actions[action_to_take]\n",
    "        sa = int(1 if suggested_action==\"Binge on Internet\" else 2 if suggested_action==\"Work\" else 3 if suggested_action==\"Exercise\" else 4 if suggested_action==\"Socialise\" else 5 if suggested_action==\"Drink Alcohol\" else 6 if suggested_action==\"Smoke\" else 0)\n",
    "        \n",
    "        #give user options\n",
    "        self.clear()\n",
    "        print(\"** Current Day: [bold dark_violet]\" + str(self.current_day) + \"[/bold dark_violet], Current Hour: [bold dark_violet]\" + str(self.current_hour) + \"[/bold dark_violet], Time Left: [bold dark_violet]\" + str(time_left) + \"[/bold dark_violet] hrs ** \\n\")\n",
    "        print(\"- [bold dark_green]Serotonin: \" + str(\"#\" * self.agent_serotonin) + \"[/bold dark_green]\")\n",
    "        print(\"- [bold dark_green]Oxytocin: \" + str(\"#\" * self.agent_oxytocin) + \"[/bold dark_green]\")\n",
    "        print(\"- [bold dark_green]Dopamine: \" + str(\"#\"*self.agent_dopamine) + \"[/bold dark_green]\")\n",
    "        print(\"- [bold dark_green]Endorphins: \" + str(\"#\"*self.agent_endorphins) + \"[/bold dark_green]\")\n",
    "        print(\"- [bold dark_red]Cortisol: \" + str(\"#\"*self.agent_serotonin) + \"[/bold dark_red]\")\n",
    "        print(\"\\n1. [bold dark_violet]Binge on Internet[/bold dark_violet]\\n2. [bold dark_violet]Work[/bold dark_violet]\\n3. [bold dark_violet]Exercise[/bold dark_violet]\\n4. [bold dark_violet]Socialise[/bold dark_violet]\\n5. [bold dark_violet]Drink Alcohol[/bold dark_violet]\\n6. [bold dark_violet]Smoke[/bold dark_violet]\\n\")\n",
    "        action_taken = 0\n",
    "        action_taken = IntPrompt.ask(\"Choose from 1 to 6\", default=sa)\n",
    "\n",
    "        #update agent brain chemicals after action taken\n",
    "        if(action_taken == 1): #Binge on Internet\n",
    "            self.agent_serotonin += 0\n",
    "            self.agent_oxytocin += 0\n",
    "            self.agent_dopamine += 1\n",
    "            self.agent_endorphins += 0\n",
    "            self.agent_cortisol += 0\n",
    "        elif(action_taken == 2): # Work\n",
    "            self.agent_serotonin += 1\n",
    "            self.agent_oxytocin += 0\n",
    "            self.agent_dopamine -= 1\n",
    "            self.agent_endorphins += 0\n",
    "            self.agent_cortisol += 1\n",
    "        elif(action_taken == 3): #Exercise\n",
    "            self.agent_serotonin += 1\n",
    "            self.agent_oxytocin += 0\n",
    "            self.agent_dopamine += 1\n",
    "            self.agent_endorphins += 0\n",
    "            self.agent_cortisol += 0\n",
    "        elif(action_taken == 4): #Socialise\n",
    "            self.agent_serotonin += 0\n",
    "            self.agent_oxytocin += 1\n",
    "            self.agent_dopamine += 1\n",
    "            self.agent_endorphins += 0\n",
    "            self.agent_cortisol += 0\n",
    "        elif(action_taken == 5): #Drink Alcohol\n",
    "            self.agent_serotonin += 0\n",
    "            self.agent_oxytocin += 0\n",
    "            self.agent_dopamine += 1\n",
    "            self.agent_endorphins += 0\n",
    "            self.agent_cortisol += 0\n",
    "        elif(action_taken == 6): #Smoke\n",
    "            self.agent_serotonin += 0\n",
    "            self.agent_oxytocin += 0\n",
    "            self.agent_dopamine += 1\n",
    "            self.agent_endorphins += 0\n",
    "            self.agent_cortisol += 0\n",
    "\n",
    "\n",
    "        #reward and punishment conditions\n",
    "        self.reward_received = self.agent_serotonin + self.agent_oxytocin + self.agent_dopamine + self.agent_endorphins - self.agent_cortisol\n",
    "        if(self.habituation[0,0] > 0):\n",
    "            self.reward_received = -1 # reward -1 for repeating an action\n",
    "        else:\n",
    "            #otherwise\n",
    "            self.reward_received = -0.2 # it gets bad reward (-0.2)\n",
    "            if time_left < self.previous_time_left: # however if it getting close to the goal\n",
    "                self.reward_received = 0.1 # it still gets slightly positive reward 0.1 just for surviving\n",
    "\n",
    "        #check if this is the last round otherwise continue\n",
    "        if(time_left <= 0):\n",
    "            return True   #end simulation\n",
    "        else:    \n",
    "            # Updating the last time from the agent to the end time (goal)\n",
    "            self.current_day += 1  #update to next day interval\n",
    "            if(self.current_day > self.end_day):\n",
    "                self.current_day = self.end_day\n",
    "            self.current_hour += 1 #update to next hour interval\n",
    "            if(self.current_hour > self.end_hour):\n",
    "                self.current_hour = 0\n",
    "            self.previous_time_left = time_left\n",
    "            return False\n",
    "\n",
    "    def clear(self): \n",
    "        \"\"\"\n",
    "        This function was taken from https://www.geeksforgeeks.org/clear-screen-python/ to\n",
    "        allow the terminal to be cleared when changing menus or showing the user important\n",
    "        messages. It checks what operating system is being used and uses the correct \n",
    "        clearing command.\n",
    "        \"\"\"\n",
    "        # for windows \n",
    "        if name == 'nt': \n",
    "            _ = system('cls') \n",
    "\n",
    "        # for mac and linux(here, os.name is 'posix')\n",
    "        else: \n",
    "            _ = system('clear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">** Current Day: <span style=\"color: #af00d7; text-decoration-color: #af00d7; font-weight: bold\">1</span>, Current Hour: <span style=\"color: #af00d7; text-decoration-color: #af00d7; font-weight: bold\">1</span>, Time Left: <span style=\"color: #af00d7; text-decoration-color: #af00d7; font-weight: bold\">8759</span> hrs ** \n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "** Current Day: \u001b[1;38;5;128m1\u001b[0m, Current Hour: \u001b[1;38;5;128m1\u001b[0m, Time Left: \u001b[1;38;5;128m8759\u001b[0m hrs ** \n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">- <span style=\"color: #005f00; text-decoration-color: #005f00; font-weight: bold\">Serotonin: #</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "- \u001b[1;38;5;22mSerotonin: #\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">- <span style=\"color: #005f00; text-decoration-color: #005f00; font-weight: bold\">Oxytocin: #</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "- \u001b[1;38;5;22mOxytocin: #\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">- <span style=\"color: #005f00; text-decoration-color: #005f00; font-weight: bold\">Dopamine: #</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "- \u001b[1;38;5;22mDopamine: #\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">- <span style=\"color: #005f00; text-decoration-color: #005f00; font-weight: bold\">Endorphins: #</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "- \u001b[1;38;5;22mEndorphins: #\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">- <span style=\"color: #870000; text-decoration-color: #870000; font-weight: bold\">Cortisol: #</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "- \u001b[1;38;5;88mCortisol: #\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>. <span style=\"color: #af00d7; text-decoration-color: #af00d7; font-weight: bold\">Binge on Internet</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>. <span style=\"color: #af00d7; text-decoration-color: #af00d7; font-weight: bold\">Work</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>. <span style=\"color: #af00d7; text-decoration-color: #af00d7; font-weight: bold\">Exercise</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span>. <span style=\"color: #af00d7; text-decoration-color: #af00d7; font-weight: bold\">Socialise</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span>. <span style=\"color: #af00d7; text-decoration-color: #af00d7; font-weight: bold\">Drink Alcohol</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6</span>. <span style=\"color: #af00d7; text-decoration-color: #af00d7; font-weight: bold\">Smoke</span>\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1;36m1\u001b[0m. \u001b[1;38;5;128mBinge on Internet\u001b[0m\n",
       "\u001b[1;36m2\u001b[0m. \u001b[1;38;5;128mWork\u001b[0m\n",
       "\u001b[1;36m3\u001b[0m. \u001b[1;38;5;128mExercise\u001b[0m\n",
       "\u001b[1;36m4\u001b[0m. \u001b[1;38;5;128mSocialise\u001b[0m\n",
       "\u001b[1;36m5\u001b[0m. \u001b[1;38;5;128mDrink Alcohol\u001b[0m\n",
       "\u001b[1;36m6\u001b[0m. \u001b[1;38;5;128mSmoke\u001b[0m\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Choose from 1 to 6 <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">(5)</span>: </pre>\n"
      ],
      "text/plain": [
       "Choose from 1 to 6 \u001b[1;36m(5)\u001b[0m: "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [8], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m__name__\u001b[39m \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m__main__\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m----> 2\u001b[0m     Simulation()\n",
      "Cell \u001b[1;32mIn [5], line 38\u001b[0m, in \u001b[0;36mSimulation.__init__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfinish \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m \u001b[39m#trigger to end simulation\u001b[39;00m\n\u001b[0;32m     37\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfinish:\n\u001b[1;32m---> 38\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfinish \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnext_time_interval()\n",
      "Cell \u001b[1;32mIn [5], line 66\u001b[0m, in \u001b[0;36mSimulation.next_time_interval\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m1. [bold dark_violet]Binge on Internet[/bold dark_violet]\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m2. [bold dark_violet]Work[/bold dark_violet]\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m3. [bold dark_violet]Exercise[/bold dark_violet]\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m4. [bold dark_violet]Socialise[/bold dark_violet]\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m5. [bold dark_violet]Drink Alcohol[/bold dark_violet]\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m6. [bold dark_violet]Smoke[/bold dark_violet]\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     65\u001b[0m action_taken \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m---> 66\u001b[0m action_taken \u001b[39m=\u001b[39m IntPrompt\u001b[39m.\u001b[39;49mask(\u001b[39m\"\u001b[39;49m\u001b[39mChoose from 1 to 6\u001b[39;49m\u001b[39m\"\u001b[39;49m, default\u001b[39m=\u001b[39;49msa)\n\u001b[0;32m     68\u001b[0m \u001b[39m#update agent brain chemicals after action taken\u001b[39;00m\n\u001b[0;32m     69\u001b[0m \u001b[39mif\u001b[39;00m(action_taken \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m): \u001b[39m#Binge on Internet\u001b[39;00m\n",
      "File \u001b[1;32md:\\Projects\\repos\\tu060\\venv\\lib\\site-packages\\rich\\prompt.py:141\u001b[0m, in \u001b[0;36mPromptBase.ask\u001b[1;34m(cls, prompt, console, password, choices, show_default, show_choices, default, stream)\u001b[0m\n\u001b[0;32m    119\u001b[0m \u001b[39m\"\"\"Shortcut to construct and run a prompt loop and return the result.\u001b[39;00m\n\u001b[0;32m    120\u001b[0m \n\u001b[0;32m    121\u001b[0m \u001b[39mExample:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    131\u001b[0m \u001b[39m    stream (TextIO, optional): Optional text file open for reading to get input. Defaults to None.\u001b[39;00m\n\u001b[0;32m    132\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    133\u001b[0m _prompt \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39m(\n\u001b[0;32m    134\u001b[0m     prompt,\n\u001b[0;32m    135\u001b[0m     console\u001b[39m=\u001b[39mconsole,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    139\u001b[0m     show_choices\u001b[39m=\u001b[39mshow_choices,\n\u001b[0;32m    140\u001b[0m )\n\u001b[1;32m--> 141\u001b[0m \u001b[39mreturn\u001b[39;00m _prompt(default\u001b[39m=\u001b[39;49mdefault, stream\u001b[39m=\u001b[39;49mstream)\n",
      "File \u001b[1;32md:\\Projects\\repos\\tu060\\venv\\lib\\site-packages\\rich\\prompt.py:274\u001b[0m, in \u001b[0;36mPromptBase.__call__\u001b[1;34m(self, default, stream)\u001b[0m\n\u001b[0;32m    272\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpre_prompt()\n\u001b[0;32m    273\u001b[0m prompt \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmake_prompt(default)\n\u001b[1;32m--> 274\u001b[0m value \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_input(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconsole, prompt, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpassword, stream\u001b[39m=\u001b[39;49mstream)\n\u001b[0;32m    275\u001b[0m \u001b[39mif\u001b[39;00m value \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mand\u001b[39;00m default \u001b[39m!=\u001b[39m \u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m:\n\u001b[0;32m    276\u001b[0m     \u001b[39mreturn\u001b[39;00m default\n",
      "File \u001b[1;32md:\\Projects\\repos\\tu060\\venv\\lib\\site-packages\\rich\\prompt.py:203\u001b[0m, in \u001b[0;36mPromptBase.get_input\u001b[1;34m(cls, console, prompt, password, stream)\u001b[0m\n\u001b[0;32m    185\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[0;32m    186\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_input\u001b[39m(\n\u001b[0;32m    187\u001b[0m     \u001b[39mcls\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    191\u001b[0m     stream: Optional[TextIO] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    192\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mstr\u001b[39m:\n\u001b[0;32m    193\u001b[0m     \u001b[39m\"\"\"Get input from user.\u001b[39;00m\n\u001b[0;32m    194\u001b[0m \n\u001b[0;32m    195\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    201\u001b[0m \u001b[39m        str: String from user.\u001b[39;00m\n\u001b[0;32m    202\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 203\u001b[0m     \u001b[39mreturn\u001b[39;00m console\u001b[39m.\u001b[39;49minput(prompt, password\u001b[39m=\u001b[39;49mpassword, stream\u001b[39m=\u001b[39;49mstream)\n",
      "File \u001b[1;32md:\\Projects\\repos\\tu060\\venv\\lib\\site-packages\\rich\\console.py:2102\u001b[0m, in \u001b[0;36mConsole.input\u001b[1;34m(self, prompt, markup, emoji, password, stream)\u001b[0m\n\u001b[0;32m   2100\u001b[0m         result \u001b[39m=\u001b[39m stream\u001b[39m.\u001b[39mreadline()\n\u001b[0;32m   2101\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 2102\u001b[0m         result \u001b[39m=\u001b[39m \u001b[39minput\u001b[39;49m()\n\u001b[0;32m   2103\u001b[0m \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[1;32md:\\Projects\\repos\\tu060\\venv\\lib\\site-packages\\ipykernel\\kernelbase.py:1177\u001b[0m, in \u001b[0;36mKernel.raw_input\u001b[1;34m(self, prompt)\u001b[0m\n\u001b[0;32m   1173\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_allow_stdin:\n\u001b[0;32m   1174\u001b[0m     \u001b[39mraise\u001b[39;00m StdinNotImplementedError(\n\u001b[0;32m   1175\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1176\u001b[0m     )\n\u001b[1;32m-> 1177\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_input_request(\n\u001b[0;32m   1178\u001b[0m     \u001b[39mstr\u001b[39;49m(prompt),\n\u001b[0;32m   1179\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_parent_ident[\u001b[39m\"\u001b[39;49m\u001b[39mshell\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[0;32m   1180\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_parent(\u001b[39m\"\u001b[39;49m\u001b[39mshell\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[0;32m   1181\u001b[0m     password\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m   1182\u001b[0m )\n",
      "File \u001b[1;32md:\\Projects\\repos\\tu060\\venv\\lib\\site-packages\\ipykernel\\kernelbase.py:1219\u001b[0m, in \u001b[0;36mKernel._input_request\u001b[1;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[0;32m   1216\u001b[0m             \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m   1217\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyboardInterrupt\u001b[39;00m:\n\u001b[0;32m   1218\u001b[0m     \u001b[39m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[1;32m-> 1219\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyboardInterrupt\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mInterrupted by user\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m   1220\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[0;32m   1221\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlog\u001b[39m.\u001b[39mwarning(\u001b[39m\"\u001b[39m\u001b[39mInvalid Message:\u001b[39m\u001b[39m\"\u001b[39m, exc_info\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    Simulation() "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "63fc9bad62e87c8f4520331cf897eeae3cca7d7504ec2bc20d709f20b08251da"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
