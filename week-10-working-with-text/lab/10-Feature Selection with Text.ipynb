{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up Environment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data and generate TDM\n",
    "\n",
    "The vectoriser below uses the `token_pattern` parameter to remove numerics and underscores from the data.\n",
    "Try it out with the parameter specified and not specified (i.e. using the default which uses a regex of `(?u)\\b\\w\\w+\\b` to see the impact on the tokens/features generated.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = ['talk.religion.misc','soc.religion.christian', 'sci.med']\n",
    "newsgroups_train = fetch_20newsgroups(subset='train',\n",
    "                                     categories=categories,\n",
    "                                     remove=('headers', 'footers', 'quotes'),\n",
    "                                     shuffle=True, random_state=42)\n",
    "\n",
    "X, Y = newsgroups_train.data, newsgroups_train.target\n",
    "\n",
    "vectorizer = TfidfVectorizer(token_pattern=r'\\b[^\\d^\\_\\W]+\\b')    \n",
    "\n",
    "X_vec = vectorizer.fit_transform(X)   #transform training data into TDM\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examine the features generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features: 22059\n",
      "First 100: ['a' 'aa' 'aaai' 'aacc' 'aanerud' 'aaron' 'aaronson' 'aasked' 'ab'\n",
      " 'abacus' 'abandon' 'abandoned' 'abandoning' 'abandons' 'abates' 'abba'\n",
      " 'abbasids' 'abbott' 'abbreviated' 'abbreviation' 'abd' 'abdel' 'abdomen'\n",
      " 'abdominal' 'abduction' 'abdullah' 'abeit' 'aberdeen' 'aberrant'\n",
      " 'aberration' 'aberrations' 'abhin' 'abhor' 'abhorent' 'abhorrent' 'abide'\n",
      " 'abideth' 'abiding' 'abilities' 'ability' 'abingdon' 'abington'\n",
      " 'abiogenesis' 'abjuring' 'ablazing' 'able' 'ably' 'abner' 'abnormal'\n",
      " 'abnormalities' 'abnormally' 'aboard' 'abode' 'abodes' 'abolish'\n",
      " 'abolished' 'abolishment' 'abolition' 'abolitionist' 'abolitionists'\n",
      " 'abomb' 'abomination' 'abortion' 'abou' 'abound' 'abounded' 'about'\n",
      " 'above' 'abput' 'abraam' 'abraham' 'abrahamic' 'abram' 'abreast' 'abri'\n",
      " 'abridged' 'abroad' 'abruptly' 'abscence' 'abscess' 'absence' 'absent'\n",
      " 'absol' 'absolute' 'absolutely' 'absolutes' 'absolutist' 'absolutists'\n",
      " 'absorbed' 'absorbtion' 'absorption' 'abstacted' 'abstain' 'abstinence'\n",
      " 'abstract' 'abstraction' 'absurd' 'absurdity' 'absurdly' 'absurdum']\n"
     ]
    }
   ],
   "source": [
    "ftr_names= vectorizer.get_feature_names_out()\n",
    "print(\"Number of features: %d\"  % len(ftr_names))\n",
    "print(\"First 100: %s\" % ftr_names[:100])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Stopwords  and add in Document Frequency reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features: 7163\n",
      "First 100: ['aaron', 'abandon', 'abandoned', 'abdominal', 'aberrant', 'ability', 'able', 'abnormal', 'abnormalities', 'abolish', 'abolished', 'abortion', 'abraham', 'absence', 'absent', 'absolute', 'absolutely', 'absolutes', 'absolutist', 'absorbed', 'abstinence', 'absurd', 'absurdity', 'abundant', 'abuse', 'ac', 'academia', 'academic', 'accept', 'acceptable', 'acceptance', 'accepted', 'accepting', 'accepts', 'access', 'accident', 'accidentally', 'accompanied', 'accomplish', 'accomplished', 'according', 'accordingly', 'account', 'accountable', 'accounts', 'accumulated', 'accuracy', 'accurate', 'accurately', 'accusation', 'accusations', 'accuse', 'accused', 'accusing', 'accustomed', 'ache', 'achieve', 'achieved', 'acid', 'acidophilus', 'acids', 'acknowledge', 'acknowledged', 'acknowledgement', 'acknowledges', 'acne', 'acquired', 'acsu', 'act', 'acting', 'action', 'actions', 'active', 'actively', 'activities', 'activity', 'acts', 'actual', 'actually', 'acupuncture', 'acute', 'acyclovir', 'ad', 'adam', 'adams', 'adapted', 'add', 'added', 'addict', 'addiction', 'addicts', 'adding', 'addition', 'additional', 'additive', 'additives', 'address', 'addressed', 'addresses', 'addressing']\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(max_df=0.95, min_df=3, token_pattern=r'\\b[^\\d^\\_\\W]+\\b',stop_words=\"english\")\n",
    "X_vec = vectorizer.fit_transform(X)   #transform training data\n",
    "\n",
    "ftr_names= vectorizer.get_feature_names()\n",
    "print(\"Number of features: %d\"  % len(ftr_names))\n",
    "print(\"First 100: %s\" % ftr_names[:100])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use the Chi-Squared test for feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'SelectKBest' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [12]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m chi \u001b[38;5;241m=\u001b[39m \u001b[43mSelectKBest\u001b[49m(chi2, k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m)    \u001b[38;5;66;03m#get top k features \u001b[39;00m\n\u001b[0;32m      2\u001b[0m X_chi_vec\u001b[38;5;241m=\u001b[39m chi\u001b[38;5;241m.\u001b[39mfit_transform(X_vec, Y)  \u001b[38;5;66;03m#fit and transform  training data (TDM) into the reduced feature space\u001b[39;00m\n\u001b[0;32m      4\u001b[0m mask \u001b[38;5;241m=\u001b[39m chi\u001b[38;5;241m.\u001b[39mget_support(indices\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;66;03m# mask returns a list of indices into the original vocabulary/feature space\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'SelectKBest' is not defined"
     ]
    }
   ],
   "source": [
    "chi = SelectKBest(chi2, k=100)    #get top k features \n",
    "X_chi_vec= chi.fit_transform(X_vec, Y)  #fit and transform  training data (TDM) into the reduced feature space\n",
    "\n",
    "mask = chi.get_support(indices=True) # mask returns a list of indices into the original vocabulary/feature space\n",
    "\n",
    "mask #print out the mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## access the mask\n",
    "for i in mask[:10]:\n",
    "    print(\"index: %d, feature name: %s\" % (i, ftr_names[i]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create a list of the selected features using the mask\n",
    "new_ftrs = [] # a list to hold your k best features\n",
    "\n",
    "for i in mask:\n",
    "      new_ftrs.append(ftr_names[i])\n",
    "print(\"Number of features: %d\"  % len(new_ftrs))\n",
    "print(\"First 100: %s\" % new_ftrs[:100])        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Mutual Information for feature selection \n",
    "Note that the features selected are different from those selected using chi-squared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mi = SelectKBest(mutual_info_classif, k=100)    #get top k features using mutual information\n",
    "X_mi_vec= mi.fit_transform(X_vec, Y)  #fit and transform  training data (TDM) into the reduced feature space\n",
    "\n",
    "## create a list of the selected features using the mask\n",
    "mask = mi.get_support(indices=True) #get list of indices into the original feature vector\n",
    "\n",
    "new_ftrs = [] # to hold the list of your K best features\n",
    "for i in mask:\n",
    "    new_ftrs.append(ftr_names[i])\n",
    "print(\"Number of features: %d\"  % len(new_ftrs))\n",
    "print(\"First 100: %s\" % new_ftrs[:100])      \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform classification using chi-squared feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = ['rec.sport.baseball', 'rec.sport.hockey']\n",
    "newsgroups_train = fetch_20newsgroups(subset='train',\n",
    "                                     categories=categories,\n",
    "                                     remove=('headers', 'footers', 'quotes'),\n",
    "                                     shuffle=True, random_state=42)\n",
    "\n",
    "X, Y = newsgroups_train.data, newsgroups_train.target\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "X_vec = vectorizer.fit_transform(X)   #transform training data\n",
    "\n",
    "chi = SelectKBest(chi2, k=100)    #get top k features \n",
    "X_chi_vec= chi.fit_transform(X_vec, Y)  # fit and transform tdm to reduced feature space\n",
    "\n",
    "newsgroups_test = fetch_20newsgroups(subset='test',     # get test data\n",
    "                                     categories=categories,\n",
    "                                     remove=('headers', 'footers', 'quotes'),\n",
    "                                     shuffle=True,\n",
    "                                     random_state=42)\n",
    "\n",
    "vectors_test = vectorizer.transform(newsgroups_test.data)   #transform test data\n",
    "chi_test = chi.transform(vectors_test)     # transform test data to reduced feature space\n",
    "\n",
    "classifier = MultinomialNB(alpha=.01)\n",
    "classifier.fit(X_chi_vec, Y)\n",
    "predicted = classifier.predict(chi_test)\n",
    "\n",
    "print(metrics.classification_report(newsgroups_test.target, predicted,\n",
    "    target_names=newsgroups_train.target_names))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
