{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "See Section 7.2 in the sklearn User Guide  http://scikit-learn.org/stable/user_guide.html for the dataset used in this notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in the data - a subset from 20 News groups "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "categories = ['rec.sport.baseball', 'talk.politics.guns','comp.graphics', 'sci.med']\n",
    "twentyTrain = fetch_20newsgroups(subset='train', categories=categories, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['comp.graphics', 'rec.sport.baseball', 'sci.med', 'talk.politics.guns']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# You can check the target names (categories) and some data files by following commands.\n",
    "list(twentyTrain.target_names) #prints all the categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sklearn.utils.Bunch"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(twentyTrain)  # the type "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2321"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(twentyTrain.data)  # the size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2321"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(twentyTrain.filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From: marcbg@feenix.metronet.com (Marc Grant)\n",
      "Subject: Adult Chicken Pox\n",
      "Organization: Tx Metronet Communications Services, Dallas Tx\n",
      "Distribution: usa\n",
      "Lines: 13\n",
      "\n",
      "I am 35 and am recovering from a case of Chicken Pox which I contracted\n",
      "from my 5 year old daughter.  I have quite a few of these little puppies\n",
      "all over my bod.  At what point am I no longer infectious?  My physician's\n",
      "office says when they are all scabbed over.  Is this true?\n",
      "\n",
      "Is there any medications which can promote healing of the pox?  Speed up\n",
      "healing?  Please e-mail replies, and thanks in advance.\n",
      "\n",
      "-- \n",
      "|Marc Grant          | Internet: marcbg@feenix.metronet.com |\n",
      "|POB 850472          | Amateur Radio Station N5MEI          |\n",
      "|Richardson, TX 75085| Voice/Fax: 214-231-3998              |\n",
      "    - .... .- - ...  .- .-.. .-..    ..-. --- .-.. -.- ...\n",
      "\n",
      "From: marcbg@feenix.metronet.com (Marc Grant)\n",
      "Subject: Adult Chicken Pox\n",
      "Organization: Tx Metronet Communications Services, Dallas Tx\n",
      "Target class is sci.med\n"
     ]
    }
   ],
   "source": [
    "print(twentyTrain.data[0])   # print one instance of the data - .data is the data\n",
    "print(\"\\n\".join(twentyTrain.data[0].split(\"\\n\")[:3]))  #print out the first 3 lines only\n",
    "print(\"Target class is {}\".format(twentyTrain.target_names[twentyTrain.target[0]]))   #print the class of that instance - .target is the class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove the meta data so the classifier doesn't overfit to the headers etc.,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am 35 and am recovering from a case of Chicken Pox which I contracted\n",
      "from my 5 year old daughter.  I have quite a few of these little puppies\n",
      "all over my bod.  At what point am I no longer infectious?  My physician's\n",
      "office says when they are all scabbed over.  Is this true?\n",
      "\n",
      "Is there any medications which can promote healing of the pox?  Speed up\n",
      "healing?  Please e-mail replies, and thanks in advance.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "categories = ['rec.sport.baseball', 'talk.politics.guns','comp.graphics', 'sci.med']\n",
    "twentyTrain = fetch_20newsgroups(subset='train', \n",
    "                                 categories=categories, \n",
    "                                 remove=('headers', 'footers', 'quotes'), \n",
    "                                 shuffle=True, \n",
    "                                 random_state=42)    # random seed \n",
    "print(twentyTrain.data[0])   # print one instance of the data - .data is the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 1 1 1 1 1 0 3 3 1]\n"
     ]
    }
   ],
   "source": [
    "print(twentyTrain.target[:10])   #.target are the classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sci.med\n",
      "rec.sport.baseball\n",
      "rec.sport.baseball\n",
      "rec.sport.baseball\n",
      "rec.sport.baseball\n",
      "rec.sport.baseball\n",
      "comp.graphics\n",
      "talk.politics.guns\n",
      "talk.politics.guns\n",
      "rec.sport.baseball\n"
     ]
    }
   ],
   "source": [
    "for t in twentyTrain.target[:10]:\n",
    "    print(twentyTrain.target_names[t])  # .target_names are the class names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Tokenising\n",
    "\n",
    "The tokenising can be changed by changing the parameters to the Vectorizer:  \n",
    "- `analyser` and `ngram_range` params will allow tokenising by char n-grams.  \n",
    "- `max_df` and `min_df` will allow document frequency reduction to be performed\n",
    "\n",
    "Look up the documentation to see what can be changed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'analyzer': 'word',\n",
       " 'binary': False,\n",
       " 'decode_error': 'strict',\n",
       " 'dtype': numpy.int64,\n",
       " 'encoding': 'utf-8',\n",
       " 'input': 'content',\n",
       " 'lowercase': True,\n",
       " 'max_df': 1.0,\n",
       " 'max_features': None,\n",
       " 'min_df': 1,\n",
       " 'ngram_range': (1, 1),\n",
       " 'preprocessor': None,\n",
       " 'stop_words': None,\n",
       " 'strip_accents': None,\n",
       " 'token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       " 'tokenizer': None,\n",
       " 'vocabulary': None}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vect = CountVectorizer()       \n",
    "count_vect.get_params()      #shows the default parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Term-Document Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2321, 31164)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tdm = count_vect.fit_transform(twentyTrain.data)   #tdm is a matrix - 2-d array\n",
    "tdm.shape     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4204"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vect.vocabulary_.get('and')  #count_vect is a dictionary - show the freq of word 'and'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform the TDM to a normalised tf or tf-idf matrix \n",
    "\n",
    "Check the `TfidfTransformer` parameters - they allow for tf vs tfidf and l1 vs l2 normalisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'norm': 'l2', 'smooth_idf': True, 'sublinear_tf': False, 'use_idf': True}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "transformer = TfidfTransformer()   \n",
    "transformer.get_params()     #show default parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2321, 31164)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tdm_tfidf = transformer.fit_transform(tdm)   #transform the TDM\n",
    "tdm_tfidf.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a NB classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'I am sick' => sci.med\n",
      "'No more gun control' => talk.politics.guns\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultinomialNB().fit(tdm_tfidf, twentyTrain.target)    #build the classifier (data, classes)\n",
    "\n",
    "docs_test = ['I am sick', 'No more gun control']      #set up 2 test instances\n",
    "\n",
    "# transform the test data in the same way as the training through CountVector and TfidfTransformer\n",
    "test_counts = count_vect.transform(docs_test)       # don't fit as the vocab has been generated from the training data\n",
    "test_tfidf = transformer.transform(test_counts)\n",
    "\n",
    "predicted = clf.predict(test_tfidf)   #predict  \n",
    "\n",
    "for doc, category in zip(docs_test, predicted):\n",
    "    print('%r => %s' % (doc, twentyTrain.target_names[category]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Pipeline to do it all in one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('vect', CountVectorizer()), ('tfidf', TfidfTransformer()),\n",
       "                ('clf', MultinomialNB())])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "text_clf = Pipeline([('vect', CountVectorizer()),\n",
    "                      ('tfidf', TfidfTransformer()),\n",
    "                      ('clf', MultinomialNB()),])\n",
    "\n",
    "text_clf.fit(twentyTrain.data, twentyTrain.target)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the 20 NG test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9450194049159121"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twenty_test = fetch_20newsgroups(subset='test', \n",
    "                                 categories=categories, \n",
    "                                 shuffle=True, \n",
    "                                 random_state=42)  \n",
    "docs_test = twenty_test.data\n",
    "\n",
    "import numpy as np\n",
    "predicted = text_clf.predict(docs_test)   # predict\n",
    "np.mean(predicted == twenty_test.target)  #report accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using metrics package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    precision    recall  f1-score   support\n",
      "\n",
      "     comp.graphics       0.92      0.94      0.93       389\n",
      "rec.sport.baseball       0.99      0.94      0.96       397\n",
      "           sci.med       0.95      0.91      0.93       396\n",
      "talk.politics.guns       0.92      0.99      0.95       364\n",
      "\n",
      "          accuracy                           0.95      1546\n",
      "         macro avg       0.95      0.95      0.95      1546\n",
      "      weighted avg       0.95      0.95      0.95      1546\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "print(metrics.classification_report(twenty_test.target, predicted,\n",
    "    target_names=twenty_test.target_names))    #print classification results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[367,   2,  12,   8],\n",
       "       [ 10, 373,   4,  10],\n",
       "       [ 20,   1, 362,  13],\n",
       "       [  2,   1,   2, 359]], dtype=int64)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.confusion_matrix(twenty_test.target, predicted)  #print confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9451351758138555"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.f1_score(twenty_test.target, predicted, average='macro')   #print f-score\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using TfidfVectorizer \n",
    "TfidfVectorizer combines using CountVectorizer and TfidfTransformer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    precision    recall  f1-score   support\n",
      "\n",
      "       alt.atheism       0.82      0.86      0.84       319\n",
      "     comp.graphics       0.95      0.96      0.95       389\n",
      "           sci.med       0.95      0.94      0.94       396\n",
      "talk.religion.misc       0.82      0.77      0.79       251\n",
      "\n",
      "          accuracy                           0.89      1355\n",
      "         macro avg       0.88      0.88      0.88      1355\n",
      "      weighted avg       0.89      0.89      0.89      1355\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "categories = ['alt.atheism', 'talk.religion.misc',\n",
    "              'comp.graphics', 'sci.med']\n",
    "newsgroups_train = fetch_20newsgroups(subset='train',\n",
    "                                      categories=categories,\n",
    "                                    shuffle=True,\n",
    "                                     random_state=42)\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectors = vectorizer.fit_transform(newsgroups_train.data)\n",
    "\n",
    "newsgroups_test = fetch_20newsgroups(subset='test',\n",
    "                                     categories=categories,\n",
    "                                     shuffle=True,\n",
    "                                     random_state=42)\n",
    "vectors_test = vectorizer.transform(newsgroups_test.data)\n",
    "\n",
    "classifier = MultinomialNB(alpha=.01)\n",
    "classifier.fit(vectors, newsgroups_train.target)\n",
    "predicted = classifier.predict(vectors_test)\n",
    "print(metrics.classification_report(newsgroups_test.target, predicted,\n",
    "    target_names=newsgroups_train.target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
